# Replace or add these imports near the top of your file
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.base import Embeddings
# optional token counting to debug prompt sizes (install tiktoken if desired)
try:
    import tiktoken
    TOKENIZER_AVAILABLE = True
except Exception:
    TOKENIZER_AVAILABLE = False

# -------------------------
# Improved data loader that keeps JSON but stores as text fields
# -------------------------
def load_json_data(paths: dict[str, str]) -> dict[str, pd.DataFrame]:
    dfs: dict[str, pd.DataFrame] = {}
    for name, path in paths.items():
        if not path:
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            records = []
            # convert each logical item into a JSON string (one "record" per item)
            if isinstance(data, list):
                for idx, item in enumerate(data):
                    content = json.dumps(item, sort_keys=True, ensure_ascii=False)
                    records.append({"id": f"{name}__{idx}", "content": content})
            else:
                content = json.dumps(data, sort_keys=True, ensure_ascii=False)
                records.append({"id": f"{name}__0", "content": content})
            df = pd.DataFrame(records)
            dfs[name] = df
            logger.info(f"[Data] Loaded {len(df)} records for '{name}'")
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    return dfs

# -------------------------
# Build vectorstores with chunking & metadata
# -------------------------
def build_vectorstores(dfs: dict[str, pd.DataFrame]) -> list[ChildAgent]:
    agents: list[ChildAgent] = []

    # configure splitter: tune chunk_size & overlap as needed
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,    # characters; adjust down if needed (e.g. 800)
        chunk_overlap=200
    )

    for key, df in dfs.items():
        docs = []
        total_chunks = 0
        for _, row in df.iterrows():
            content = row["content"]
            doc_id = row.get("id", key)
            # split content into smaller passages
            chunks = splitter.split_text(content)
            for i, chunk in enumerate(chunks):
                metadata = {"source": key, "orig_id": doc_id, "chunk": i, "chunk_len": len(chunk)}
                docs.append(Document(page_content=chunk, metadata=metadata))
            total_chunks += len(chunks)

        if not docs:
            continue

        # Create FAISS store from documents
        store = FAISS.from_documents(docs, cached_embeddings)
        # reduce k so we only return a small number of short chunks
        retriever = store.as_retriever(search_kwargs={"k": 3})
        agents.append(ChildAgent(name=key, retriever=retriever))
        logger.info(f"[Vectorstore] Built store for '{key}' ({len(docs)} chunks total, {total_chunks} chunks added)")
    return agents

# -------------------------
# ChildAgent: use map_reduce/refine combiner and safer call
# -------------------------
class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        # use chain_type map_reduce (or "refine") instead of "stuff"
        # map_reduce will call LLM on each chunk and then combine â€” avoids stuffing everything
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="map_reduce",   # change from "stuff"
            retriever=retriever,
            return_source_documents=True,  # helpful for debugging / provenance
            # optionally provide chain_type_kwargs to tune reduce behavior
            chain_type_kwargs={
                "reduce_k": 3  # number of intermediate results to reduce; adjust if needed
            }
        )

    def ask(self, query: str) -> str:
        try:
            # run() returns a string answer; if you need structured output use self.chain(...)
            return self.chain.run(query)
        except Exception as e:
            logger.warning(f"Child agent {self.name} error in ask(): {e}")
            raise
