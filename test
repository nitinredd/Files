import os
import json
import pandas as pd
import streamlit as st
from langchain_openai import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
import torch
import clip
from PIL import Image
import numpy as np
import faiss
from PyPDF2 import PdfReader
import fitz  # PyMuPDF
import io
import base64
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer, LTFigure
import re
import tempfile

# Set Streamlit page config
st.set_page_config(page_title="Reaction Database", page_icon="ðŸ§ª", layout="wide")
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Configuration - Replace with your actual credentials
base_url = "YOUR_AZURE_ENDPOINT"
api_version = "2024-02-15-preview"
api_key = "YOUR_API_KEY"
deployment_name = "GPT4o"
model_name = "GPT4o"

embedded_folders = {
    'C-N_Bond_Formation': r"C:\Users\Desktop\C-N_Bond_Formation",
    # Add more reaction-specific folders as needed
}

# Initialize services
file_store = LocalFileStore('langchain-embeddings')
base_embeddings = AzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint=base_url,
    api_key=api_key,
    azure_deployment="Def_data_qa"
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base_embeddings, file_store, namespace=base_embeddings.model)

chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)

# Load CLIP model for image processing
@st.cache_resource
def load_clip_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)
    return model, preprocess, device

model, preprocess, device = load_clip_model()

# Initialize session state variables
if "faiss_image_index" not in st.session_state:
    st.session_state.faiss_image_index = None

if "image_metadata" not in st.session_state:
    st.session_state.image_metadata = []

if "synthetic_schemes" not in st.session_state:
    st.session_state.synthetic_schemes = {}

if "tables_data" not in st.session_state:
    st.session_state.tables_data = {}

# Improved text chunking with context preservation
def chunk_text(text, max_tokens=4000):
    """Chunk text intelligently while preserving context and structure"""
    chunks = []
    
    # First try splitting by major sections
    sections = re.split(r'\n\s*(?:RESULTS|DISCUSSION|EXPERIMENTAL|SYNTHESIS|SCHEME)\s*\n', text, flags=re.IGNORECASE)
    if len(sections) > 1:
        for section in sections:
            if len(section) > max_tokens:
                # Split large sections by paragraphs
                paragraphs = section.split('\n\n')
                current_chunk = ""
                for para in paragraphs:
                    if len(current_chunk) + len(para) < max_tokens:
                        current_chunk += para + "\n\n"
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = para + "\n\n"
                if current_chunk:
                    chunks.append(current_chunk.strip())
            else:
                chunks.append(section.strip())
    else:
        # Fallback to paragraph-based splitting
        paragraphs = text.split('\n\n')
        current_chunk = ""
        for para in paragraphs:
            if len(current_chunk) + len(para) < max_tokens:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"
        if current_chunk:
            chunks.append(current_chunk.strip())
            
    return chunks

# Improved synthetic scheme extraction
def extract_synthetic_schemes(pdf_path):
    """Extract synthetic schemes from PDFs using layout analysis"""
    synthetic_schemes = []
    try:
        doc = fitz.open(pdf_path)
        for page_num in range(len(doc)):
            page = doc[page_num]
            blocks = page.get_text("blocks")
            
            for block in blocks:
                x0, y0, x1, y1, text, block_no, block_type = block
                # Look for scheme indicators in text
                if "scheme" in text.lower() or "synthetic" in text.lower():
                    # Capture the scheme content - next 3 blocks
                    scheme_content = text + "\n"
                    next_blocks = blocks[blocks.index(block)+1:blocks.index(block)+4]
                    
                    for nb in next_blocks:
                        scheme_content += nb[4] + "\n"
                    
                    # Also extract images on the same page
                    image_list = page.get_images(full=True)
                    images = []
                    for img_index, img in enumerate(image_list):
                        base_image = doc.extract_image(img[0])
                        image_bytes = base_image["image"]
                        images.append({
                            "image_index": img_index,
                            "image_bytes": image_bytes,
                            "bbox": (x0, y0, x1, y1)
                        })
                    
                    synthetic_schemes.append({
                        "page": page_num + 1,
                        "text": scheme_content.strip(),
                        "source": pdf_path,
                        "images": images
                    })
                    
        doc.close()
    except Exception as e:
        st.error(f"Error extracting synthetic schemes from PDF '{pdf_path}': {e}")
    return synthetic_schemes

# Robust image extraction and processing
def extract_images_from_pdf(pdf_path):
    """Extract images from a PDF and create embeddings"""
    images = []
    try:
        pdf_document = fitz.open(pdf_path)
        
        for page_number in range(len(pdf_document)):
            image_list = pdf_document[page_number].get_images(full=True)
            
            for image_index, image_obj in enumerate(image_list):
                xref = image_obj[0]
                base_image = pdf_document.extract_image(xref)
                img_bytes = base_image.get("image", None)
                
                if img_bytes:
                    try:
                        img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
                        
                        # Resize large images to manage memory
                        if img.size[0] > 1000 or img.size[1] > 1000:
                            img.thumbnail((1000, 1000))
                        
                        # Create embedding
                        image_input = preprocess(img).unsqueeze(0).to(device)
                        with torch.no_grad():
                            embedding = model.encode_image(image_input).cpu().numpy().astype("float32")[0]
                        
                        metadata = {
                            "source": pdf_path,
                            "page": page_number + 1,
                            "image_index": image_index,
                            "image_bytes": img_bytes,
                            "width": img.width,
                            "height": img.height
                        }
                        images.append((embedding, metadata))
                    except Exception as e:
                        st.error(f"Error processing image on page {page_number + 1}: {e}")
        
        pdf_document.close()
    except Exception as e:
        st.error(f"Error extracting images from PDF '{pdf_path}': {e}")
    
    return images

# Tabular data extraction
def extract_tables(pdf_path):
    """Extract tabular data from PDFs"""
    tables = []
    try:
        pdf_document = fitz.open(pdf_path)
        
        for page_number in range(len(pdf_document)):
            page = pdf_document[page_number]
            tables_on_page = page.find_tables()
            
            if tables_on_page.tables:
                for table_index, table in enumerate(tables_on_page.tables):
                    df = table.to_pandas()
                    tables.append({
                        "page": page_number + 1,
                        "source": pdf_path,
                        "table_index": table_index,
                        "dataframe": df
                    })
        
        pdf_document.close()
    except Exception as e:
        st.error(f"Error extracting tables from PDF '{pdf_path}': {e}")
    
    return tables

# Main PDF processing function
def load_pdf_data(folder_path):
    """Load and process all PDFs in the specified folder"""
    text_docs = []
    image_embeddings = []
    synthetic_schemes = []
    tables_data = []
    
    for file_name in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file_name)
        if not file_path.lower().endswith('.pdf'):
            continue
        
        try:
            # Process textual content
            pdf_reader = PdfReader(file_path)
            text_content = ""
            for page in pdf_reader.pages:
                text_content += page.extract_text() + "\n"
            
            # Chunk the text intelligently
            chunks = chunk_text(text_content)
            for i, chunk in enumerate(chunks):
                text_docs.append(Document(
                    page_content=chunk,
                    metadata={
                        "source": file_name,
                        "chunk": i,
                        "type": "text"
                    }
                ))
            
            # Extract synthetic schemes
            schemes = extract_synthetic_schemes(file_path)
            for scheme in schemes:
                text_docs.append(Document(
                    page_content=scheme["text"],
                    metadata={
                        "source": file_name,
                        "page": scheme["page"],
                        "type": "scheme"
                    }
                ))
                synthetic_schemes.append(scheme)
            
            # Extract images
            images = extract_images_from_pdf(file_path)
            image_embeddings.extend(images)
            
            # Extract tables
            tables = extract_tables(file_path)
            for table in tables:
                # Convert table to markdown for storage
                table_md = table["dataframe"].to_markdown(index=False)
                text_docs.append(Document(
                    page_content=table_md,
                    metadata={
                        "source": file_name,
                        "page": table["page"],
                        "type": "table"
                    }
                ))
                tables_data.append(table)
                
        except Exception as e:
            st.error(f"Error processing PDF file '{file_name}': {e}")
    
    return text_docs, image_embeddings, synthetic_schemes, tables_data

# Build vector stores
def build_vectorstores_with_pdfs(folder_paths):
    agents = []
    
    # Initialize session state dictionaries
    if "image_metadata" not in st.session_state:
        st.session_state.image_metadata = {}
    if "faiss_image_index" not in st.session_state:
        st.session_state.faiss_image_index = {}
    if "synthetic_schemes" not in st.session_state:
        st.session_state.synthetic_schemes = {}
    if "tables_data" not in st.session_state:
        st.session_state.tables_data = {}
    
    for reaction_type, folder_path in folder_paths.items():
        with st.spinner(f"Processing {reaction_type} PDFs..."):
            text_docs, image_embeddings, synthetic_schemes, tables_data = load_pdf_data(folder_path)
            
            # Build agents for PDF textual data
            if text_docs:
                pdf_store = FAISS.from_documents(text_docs, cached_embeddings)
                pdf_retriever = pdf_store.as_retriever(search_kwargs={"k": 5})
                agents.append(ChildAgent(name=reaction_type, retriever=pdf_retriever))
            
            # Handle image embeddings
            if image_embeddings:
                try:
                    # Filter for valid embeddings
                    valid_embeddings = [e[0] for e in image_embeddings]
                    metadata_list = [e[1] for e in image_embeddings]
                    
                    if valid_embeddings:
                        # Create FAISS index
                        index = faiss.IndexFlatL2(valid_embeddings[0].shape[0])
                        embeddings_array = np.array(valid_embeddings).astype('float32')
                        index.add(embeddings_array)
                        
                        # Store in session state
                        st.session_state.image_metadata[reaction_type] = metadata_list
                        st.session_state.faiss_image_index[reaction_type] = index
            
                except Exception as e:
                    st.error(f"Error creating image index for {reaction_type}: {e}")
            
            # Store synthetic schemes and tables
            st.session_state.synthetic_schemes[reaction_type] = synthetic_schemes
            st.session_state.tables_data[reaction_type] = tables_data
    
    return agents

# Agent classes
class ChildAgent:
    def __init__(self, name, retriever):
        self.name = name
        self.retriever = retriever

    def ask(self, query):
        try:
            prompt = (
                "You are an expert chemistry assistant specialized in reaction data extraction. "
                "When answering user queries, follow these guidelines:\n"
                "1. **API Details**: Provide the API name if available\n"
                "2. **Reaction Chemistry**: Describe the reaction chemistry\n"
                "3. **Yield**: Report the yield\n"
                "4. **Procedure**: Present the complete experimental procedure EXACTLY as written. "
                "Preserve all details, measurements, and formatting. Do NOT summarize or modify.\n"
                "5. **Tabular Data**: For any tables, present them COMPLETELY in markdown format with ALL data. "
                "Do NOT omit any rows, columns, or values.\n\n"
                "Prioritize completeness and fidelity to the original source material. "
                "Include all relevant details while excluding unrelated content."
            )
            full_query = f"{prompt}\n\n{query}"

            result = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=self.retriever,
                return_source_documents=True
            ).invoke({"query": full_query})

            return {
                "result": result["result"],
                "source_documents": result["source_documents"]
            }

        except Exception as e:
            st.error(f"Error querying {self.name}: {e}")
            return None

class CoordinatorAgent:
    def __init__(self, child_agents):
        self.children = child_agents

    def coordinate(self, query):
        answers = []
        for child in self.children:
            try:
                resp = child.ask(query)
                if resp and resp["result"]:
                    answers.append((child.name, resp))
            except Exception as e:
                st.error(f"Error querying {child.name}: {e}")
                continue
        return answers

class AgentManager:
    def __init__(self, agents):
        self.coordinator = CoordinatorAgent(agents)
        self.reaction_types = list(embedded_folders.keys())

    def handle_query(self, query):
        raw_answers = self.coordinator.coordinate(query)
        if not raw_answers:
            return "Relevant information not found.", [], [], []
        
        # Find best answer
        best_answer = max(raw_answers, key=lambda x: len(x[1]["result"]))
        reaction_type = best_answer[0]
        answer_data = best_answer[1]
        
        # Extract source documents for display
        source_docs = answer_data.get("source_documents", [])
        
        # Find relevant schemes and tables
        schemes = st.session_state.synthetic_schemes.get(reaction_type, [])
        tables = st.session_state.tables_data.get(reaction_type, [])
        
        # Find relevant images
        relevant_images = []
        if query and st.session_state.faiss_image_index.get(reaction_type):
            try:
                # Embed the query text
                text_input = clip.tokenize([query]).to(device)
                with torch.no_grad():
                    text_embedding = model.encode_text(text_input).cpu().numpy().astype("float32")
                
                # Search the FAISS index
                index = st.session_state.faiss_image_index[reaction_type]
                D, I = index.search(text_embedding, 3)  # Get top 3 images
                
                # Retrieve the images
                metadata_list = st.session_state.image_metadata.get(reaction_type, [])
                for idx in I[0]:
                    if 0 <= idx < len(metadata_list):
                        relevant_images.append(metadata_list[idx])
            except Exception as e:
                st.error(f"Image search error: {e}")
        
        return answer_data["result"], source_docs, schemes, tables, relevant_images

# Streamlit UI
st.title("ðŸ§ª Reaction Database AI")
st.markdown("Search chemical reaction databases for API details, procedures, yields, and synthetic schemes")

# Build agents only once
if "agents_built" not in st.session_state:
    with st.spinner("Initializing database. This may take several minutes..."):
        child_agents = build_vectorstores_with_pdfs(embedded_folders)
        manager = AgentManager(child_agents)
        st.session_state.manager = manager
        st.session_state.agents_built = True
else:
    manager = st.session_state.manager

# Search interface
query = st.text_input("Search Reaction Database:", key="input_query", 
                      placeholder="Enter reaction type, API name, or chemical query")

if st.button("Search") or query:
    with st.spinner("Searching database..."):
        response, source_docs, schemes, tables, images = manager.handle_query(query)
        
        # Display main response
        st.subheader("Query Results")
        st.write(response)
        
        # Display source documents
        if source_docs:
            st.subheader("Source Documents")
            for i, doc in enumerate(source_docs):
                with st.expander(f"Source Document {i+1} - {doc.metadata['source']}"):
                    st.write(f"**Document Type:** {doc.metadata.get('type', 'text')}")
                    st.write(doc.page_content)
        
        # Display synthetic schemes
        if schemes:
            st.subheader("Synthetic Schemes")
            for scheme in schemes:
                with st.expander(f"Scheme from {scheme['source']} - Page {scheme['page']}"):
                    st.write(scheme["text"])
                    
                    # Display scheme images
                    for img_data in scheme["images"]:
                        img = Image.open(io.BytesIO(img_data["image_bytes"]))
                        st.image(img, caption=f"Scheme Image - Page {scheme['page']}", 
                                 use_column_width=True)
        
        # Display tables
        if tables:
            st.subheader("Tabular Data")
            for table in tables:
                with st.expander(f"Table from {table['source']} - Page {table['page']}"):
                    st.dataframe(table["dataframe"])
        
        # Display relevant images
        if images:
            st.subheader("Relevant Images")
            cols = st.columns(3)
            for i, img_meta in enumerate(images):
                img = Image.open(io.BytesIO(img_meta["image_bytes"]))
                with cols[i % 3]:
                    st.image(img, caption=f"Page {img_meta['page']} - {img_meta['source']}", 
                             use_column_width=True)

# Add footer
st.markdown("---")
st.caption("Reaction Database AI | Powered by Azure OpenAI and CLIP models")
