# main.py - Reaction Database AI (updated)
import os
import re
import glob
import base64
import io
import json
import time
import requests
import traceback
from typing import Union, List, Optional, Dict, Any, Tuple
from jose import jwt
import threading
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Depends
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBasic
from datetime import datetime, timedelta
from pydantic import BaseModel
import fitz  # PyMuPDF
import pandas as pd
from PIL import Image
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA, LLMChain
from langchain.prompts import PromptTemplate
import mimetypes
from difflib import SequenceMatcher
from pydantic import BaseModel
from pptx import Presentation
from pptx.util import Inches, Pt

# Optional libraries (safe-guarded)
try:
    import docx  # python-docx
except Exception:
    docx = None
try:
    import pptx as pptx_lib  # python-pptx used for extraction (optional)
except Exception:
    pptx_lib = None
try:
    import pytesseract
except Exception:
    pytesseract = None

# ---------------------------
# Config - adapt as needed
# ---------------------------
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Azure LLM config (kept as you requested)
base_url = ""  # set your azure endpoint if needed
api_version = "2025-01-01-preview"
api_key = ""
deployment_name = "api-ai4o"
model_name = "gpt-4o"

# Initialize Azure services
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-3-large",
    api_version="2025-01-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment=""
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# Bing / Web Search (optional - provide keys)
BING_SEARCH_ENDPOINT = os.environ.get("BING_SEARCH_ENDPOINT", "")  # e.g. https://api.bing.microsoft.com/v7.0/search or your azure bing endpoint
BING_SEARCH_KEY = os.environ.get("BING_SEARCH_KEY", "")
USE_BING_IF_NO_DOCS = True  # toggle this to disable web fallback

# JWT Config
SECRET_KEY = "myFAVsecretKEY"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

# Reaction types & file layout
REACTION_TYPES = [
    "C-C_Bond_Formation", "C-N_Bond_Formation", "Salt_Formation", "Hydrolysis",
    "Amidation", "Reduction", "Oxidation", "Cyclization", "Purification",
    "Metal_mediated_catalyzed", "C-halogen Bond Formation", "Miscellaneous"
]

BASE_DIR = r"C:\Users\Desktop\WORK\API\Reaction_Database\Datasets_O\Reaction_Database"
PRODUCTS_DIR = os.path.join(BASE_DIR, "Products")
SCHEMES_DIR = os.path.join(BASE_DIR, "Synthetic_Schemes")
UPLOADS_DIR = os.path.join(BASE_DIR, "User_Uploads")

os.makedirs(PRODUCTS_DIR, exist_ok=True)
os.makedirs(SCHEMES_DIR, exist_ok=True)
os.makedirs(UPLOADS_DIR, exist_ok=True)

# Prompts
EXTRACTION_PROMPT_TEMPLATE = """
You are a pharmaceutical chemistry expert specializing in reaction chemistry. Extract the following information from the document in a structured format Mandatorily:
1. **API Name**: The active pharmaceutical ingredient
2. **Reaction Chemistry**: Type and description
3. **Yield**: Exact yield percentages or values mentioned in the source
4. **Procedure**: Summarize the complete procedure into clear, concise numbered bullet points, preserving the key steps and important details. Do NOT omit any steps or essential content.
5. **Tabular Data**: Provide COMPLETE tabular data in markdown table format. Do NOT omit, summarize, or transform any content.

Structure your response as follows (literal headers must appear exactly like below):

### API Name
[API name here]
### Reaction Chemistry
[Reaction chemistry description here]
### Yield
[Yield value here]
### Procedure
[Complete procedure here]
### Tabular Data
[Markdown table here]

Document Content:
{context}
Question: {question}
Answer:
"""
EXTRACTION_PROMPT = PromptTemplate(template=EXTRACTION_PROMPT_TEMPLATE, input_variables=["context", "question"])

QA_PROMPT_TEMPLATE = """
You are a concise pharmaceutical chemistry expert. Use the provided document context to answer the user's question directly and concisely.

Rules (follow exactly):
- Use only the information present in the context. Do NOT hallucinate.
- Answer in one short paragraph (2-6 sentences) unless the user explicitly asks for step-by-step procedure.
- Do NOT reproduce the full document content. Do NOT provide unrequested long verbatim passages.
- If multiple documents were used, identify only those documents that were actually the sources of the answer.
- If the answer is not present in the context, say "I could not find an answer in the provided documents." (do not guess).

Context:
{context}

Question: {question}

Answer:
"""
QA_PROMPT = PromptTemplate(template=QA_PROMPT_TEMPLATE, input_variables=["context", "question"])

# ------------------------------
# Helpers: file listing, extraction, indexing, parsing
# ------------------------------
def find_scheme_image(reaction_type: str, product_name: str) -> Optional[str]:
    for ext in ['.jpeg', '.jpg', '.png', '.gif']:
        scheme_path = os.path.join(SCHEMES_DIR, reaction_type, f"{product_name}{ext}")
        if os.path.exists(scheme_path):
            return scheme_path
    return None

def list_products() -> List[Dict[str, Any]]:
    products = []
    for reaction_type in REACTION_TYPES:
        reaction_dir = os.path.join(PRODUCTS_DIR, reaction_type)
        if not os.path.exists(reaction_dir):
            continue
        files = glob.glob(os.path.join(reaction_dir, "*.*"))
        for path in files:
            filename = os.path.basename(path)
            product_name = os.path.splitext(filename)[0]
            scheme_image = find_scheme_image(reaction_type, product_name)
            scheme_cdx = os.path.join(SCHEMES_DIR, reaction_type, f"{product_name}.cdx")
            product_id = f"{reaction_type}_{product_name}"
            products.append({
                "id": product_id,
                "name": product_name,
                "reaction_type": reaction_type,
                "pdf_path": path,
                "scheme_image": scheme_image if scheme_image else None,
                "scheme_cdx": scheme_cdx if os.path.exists(scheme_cdx) else None
            })
    # user uploads (flat)
    uploads = glob.glob(os.path.join(UPLOADS_DIR, "*"))
    for path in uploads:
        if os.path.isdir(path):
            for fpath in glob.glob(os.path.join(path, "*.*")):
                filename = os.path.basename(fpath)
                product_name = os.path.splitext(filename)[0]
                product_id = f"UserUpload_{os.path.basename(path)}_{product_name}"
                products.append({
                    "id": product_id,
                    "name": product_name,
                    "reaction_type": "User_Uploads",
                    "pdf_path": fpath,
                    "scheme_image": None,
                    "scheme_cdx": None
                })
    return products

def extract_pdf_text(pdf_path: str) -> str:
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text() + "\n"
        return text
    except Exception as e:
        raise RuntimeError(f"Error reading PDF {pdf_path}: {e}")

# minimal extractors for other formats (safe guarded)
def extract_docx_text(path: str) -> str:
    if docx is None:
        raise RuntimeError("python-docx not installed")
    d = docx.Document(path)
    texts = [p.text for p in d.paragraphs if p.text and p.text.strip()]
    return "\n".join(texts)

def extract_pptx_text(path: str) -> str:
    if pptx_lib is None:
        raise RuntimeError("python-pptx not installed for extraction")
    prs = pptx_lib.Presentation(path)
    texts = []
    for slide in prs.slides:
        for shape in slide.shapes:
            try:
                if hasattr(shape, "text") and shape.text:
                    texts.append(shape.text)
            except Exception:
                continue
    return "\n".join(texts)

def extract_xls_text(path: str) -> str:
    try:
        xls = pd.read_excel(path, sheet_name=None)
        parts = []
        for sheet_name, df in xls.items():
            parts.append(f"### Sheet: {sheet_name}")
            parts.append(df.to_csv(index=False))
        return "\n\n".join(parts)
    except Exception as e:
        raise RuntimeError(f"Failed reading excel: {e}")

def extract_csv_text(path: str) -> str:
    try:
        df = pd.read_csv(path)
        return df.to_csv(index=False)
    except Exception as e:
        raise RuntimeError(f"Failed reading csv: {e}")

def extract_image_text(path: str) -> Tuple[str,bool]:
    if pytesseract is None:
        return ("", False)
    try:
        img = Image.open(path)
        txt = pytesseract.image_to_string(img)
        return (txt or "", True)
    except Exception as e:
        print(f"OCR error: {e}")
        return ("", False)

# ------------------------------
# Caches & locks
_vectorstore_cache: Dict[str, FAISS] = {}
_product_details_cache: Dict[str, Dict[str, Any]] = {}
_vectorstore_lock = threading.Lock()

def build_product_vector_store(product: Dict[str, Any]) -> Optional[FAISS]:
    pid = product["id"]
    with _vectorstore_lock:
        if pid in _vectorstore_cache:
            return _vectorstore_cache[pid]
        path = product.get("pdf_path")
        if not path or not os.path.exists(path):
            return None
        ext = os.path.splitext(path)[1].lower()
        text = ""
        try:
            if ext == ".pdf":
                text = extract_pdf_text(path)
            elif ext in [".docx", ".doc"]:
                text = extract_docx_text(path)
            elif ext == ".pptx":
                text = extract_pptx_text(path)
            elif ext in [".xlsx", ".xls"]:
                text = extract_xls_text(path)
            elif ext == ".csv":
                text = extract_csv_text(path)
            elif ext in [".png", ".jpg", ".jpeg", ".gif"]:
                txt, did = extract_image_text(path)
                if did and txt.strip():
                    text = txt
                else:
                    text = f"[image file: {os.path.basename(path)}]"
            else:
                # fallback try pdf extraction then placeholder
                try:
                    text = extract_pdf_text(path)
                except Exception:
                    text = f"[file: {os.path.basename(path)}]"
        except Exception as e:
            print(f"Extraction error for {path}: {e}")
            return None

        if not text or len(text.strip()) < 20:
            return None

        doc = Document(page_content=text, metadata={
            "product_id": pid,
            "product_name": product["name"],
            "reaction_type": product.get("reaction_type", "User_Uploads"),
            "source": path
        })
        try:
            vs = FAISS.from_documents([doc], cached_embeddings)
        except Exception as e:
            print(f"FAISS build error for {pid}: {e}")
            return None
        _vectorstore_cache[pid] = vs
        return vs

def parse_structured_response(text: str) -> Dict[str, Any]:
    result = {"raw": text, "api_name": None, "reaction_chemistry": None, "yield": None, "procedure": None, "tables": []}
    def grab(section):
        m = re.search(rf"###\s*{re.escape(section)}\s*(.*?)\s*(?=###\s*\w+|\Z)", text, re.DOTALL | re.IGNORECASE)
        return m.group(1).strip() if m else None
    result["api_name"] = grab("API Name")
    result["reaction_chemistry"] = grab("Reaction Chemistry")
    result["yield"] = grab("Yield")
    result["procedure"] = grab("Procedure")
    tab_raw = grab("Tabular Data")
    if tab_raw:
        table_patterns = re.findall(r"(\|[^\n]*\|\s*\n\|[-:\s|]*\|\s*\n(?:\|[^\n]*\|\s*\n?)*)", tab_raw, re.DOTALL)
        if table_patterns:
            for tbl_md in table_patterns:
                lines = [ln.strip().strip("|").strip() for ln in tbl_md.splitlines() if ln.strip()]
                if len(lines) >= 2:
                    header = [h.strip() for h in lines[0].split("|")]
                    rows = []
                    for rowline in lines[2:]:
                        cols = [c.strip() for c in rowline.split("|")]
                        rows.append(cols)
                    result["tables"].append({"headers": header, "rows": rows, "raw_md": tbl_md})
        else:
            result["tables"].append({"headers": [], "rows": [], "raw_md": tab_raw})
    return result

# ------------------------------
# FastAPI app
# ------------------------------
app = FastAPI(title="Reaction Database AI (FastAPI)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ------------------------------
# Models
# ------------------------------
class ProductOut(BaseModel):
    id: str
    name: str
    reaction_type: str
    has_scheme_image: bool
    has_scheme_cdx: bool

class QARequest(BaseModel):
    product_id: Optional[str] = None
    question: str = "Extract API Name, Reaction Chemistry, Yield, Procedure, and Tabular Data"

class QueryRequest(BaseModel):
    product_ids: List[str]
    question: str

# ------------------------------
# Sample prompts (frontend)
SAMPLE_PROMPTS = [
    {"id": "extract_all", "title": "Extract structured reaction data", "text": "Extract API Name, Reaction Chemistry, Yield, Procedure, and Tabular Data"},
    {"id": "yield_only", "title": "Yield details", "text": "What is the yield reported for the final step? Provide percent if available."},
    {"id": "procedure_steps", "title": "Procedure steps", "text": "List the procedure as numbered steps exactly as in the document."},
    {"id": "conditions", "title": "Reaction conditions", "text": "List solvents, catalysts, and temperatures used in the synthesis."}
]
# ------------------------------

# ------------------------------
# Utility: Bing / web search helper
# ------------------------------
def bing_search(query: str, top_k: int = 5) -> Dict[str, Any]:
    """
    Use Bing Web Search (Azure or classic) with endpoint and key.
    Returns { 'web_pages': [ { name, url, snippet } ], 'raw': <response dict> }
    """
    if not BING_SEARCH_ENDPOINT or not BING_SEARCH_KEY:
        return {"web_pages": [], "raw": None, "error": "Bing endpoint/key not configured."}
    headers = {"Ocp-Apim-Subscription-Key": BING_SEARCH_KEY}
    params = {"q": query, "count": top_k, "textDecorations": True, "textFormat": "Plain"}
    try:
        resp = requests.get(BING_SEARCH_ENDPOINT, headers=headers, params=params, timeout=20)
        resp.raise_for_status()
        j = resp.json()
        web_pages = []
        # Bing returns webPages.value typically
        for v in (j.get("webPages", {}).get("value", [])[:top_k]):
            web_pages.append({"name": v.get("name"), "url": v.get("url"), "snippet": v.get("snippet")})
        return {"web_pages": web_pages, "raw": j}
    except Exception as e:
        print(f"Bing search error: {e}")
        return {"web_pages": [], "raw": None, "error": str(e)}

# ------------------------------
# Endpoints
# ------------------------------
@app.get("/reactions", response_model=List[str])
def get_reactions():
    return REACTION_TYPES + ["User_Uploads"]

@app.get("/prompts")
def get_prompts():
    return SAMPLE_PROMPTS

@app.get("/products", response_model=List[ProductOut])
def get_products(reaction_type: Optional[str] = None):
    allp = list_products()
    if reaction_type:
        if reaction_type == "User_Uploads":
            allp = [p for p in allp if p["reaction_type"] == "User_Uploads"]
        else:
            allp = [p for p in allp if p["reaction_type"] == reaction_type]
    out = []
    for p in allp:
        out.append(ProductOut(
            id=p["id"],
            name=p["name"],
            reaction_type=p["reaction_type"],
            has_scheme_image=bool(p["scheme_image"]),
            has_scheme_cdx=bool(p["scheme_cdx"])
        ))
    return out

@app.get("/product/{product_id}/meta")
def product_meta(product_id: str):
    products = list_products()
    for p in products:
        if p["id"] == product_id:
            return {
                "id": p["id"],
                "name": p["name"],
                "reaction_type": p["reaction_type"],
                "pdf_path": p["pdf_path"],
                "scheme_image": p["scheme_image"],
                "scheme_cdx": p["scheme_cdx"]
            }
    raise HTTPException(status_code=404, detail="Product not found")

@app.get("/product/{product_id}/pdf")
def product_pdf(product_id: str):
    meta = product_meta(product_id)
    pdf_path = meta["pdf_path"]
    if os.path.exists(pdf_path):
        return FileResponse(pdf_path, media_type="application/pdf", filename=os.path.basename(pdf_path))
    raise HTTPException(status_code=404, detail="PDF not found")

@app.get("/product/{product_id}/scheme-image")
def product_scheme_image(product_id: str):
    meta = product_meta(product_id)
    path = meta.get("scheme_image")
    if path and os.path.exists(path):
        # let the browser handle caching / CORS (we allow_origins="*")
        return FileResponse(path, media_type="image/png", filename=os.path.basename(path))
    raise HTTPException(status_code=404, detail="Scheme image not found")

# ------------------------------
# Upload endpoint
@app.post("/upload")
async def upload_file(file: UploadFile = File(...), reaction_type: Optional[str] = Form("User_Uploads"), filename: Optional[str] = Form(None)):
    try:
        rt = re.sub(r'[^a-zA-Z0-9_\- ]', '', (reaction_type or "User_Uploads"))
    except Exception:
        rt = "User_Uploads"
    ts = int(time.time())
    folder_name = f"{rt}_{ts}"
    out_dir = os.path.join(UPLOADS_DIR, folder_name)
    os.makedirs(out_dir, exist_ok=True)

    orig_filename = filename or file.filename or f"uploaded_{ts}"
    save_path = os.path.join(out_dir, orig_filename)
    with open(save_path, "wb") as f:
        contents = await file.read()
        f.write(contents)

    ext = os.path.splitext(save_path)[1].lower()
    text = ""
    ocr_performed = False
    extract_error = None
    try:
        if ext == ".pdf":
            text = extract_pdf_text(save_path)
        elif ext in [".docx", ".doc"]:
            text = extract_docx_text(save_path)
        elif ext == ".pptx":
            text = extract_pptx_text(save_path)
        elif ext in [".xlsx", ".xls"]:
            text = extract_xls_text(save_path)
        elif ext == ".csv":
            text = extract_csv_text(save_path)
        elif ext in [".png", ".jpg", ".jpeg", ".gif"]:
            text, ocr_performed = extract_image_text(save_path)
        else:
            try:
                text = extract_pdf_text(save_path)
            except Exception:
                text = f"[uploaded file: {orig_filename}]"
    except Exception as e:
        extract_error = str(e)
        print("Upload extraction error:", e)

    product_name = os.path.splitext(orig_filename)[0]
    product_id = f"UserUpload_{folder_name}_{product_name}"
    meta = {
        "id": product_id,
        "name": product_name,
        "reaction_type": "User_Uploads",
        "pdf_path": save_path,
        "scheme_image": None,
        "scheme_cdx": None,
        "ocr_performed": ocr_performed,
        "extract_error": extract_error
    }

    # attempt immediate indexing
    try:
        doc = Document(page_content=(text or f"[uploaded file: {orig_filename}]"), metadata={
            "product_id": product_id,
            "product_name": product_name,
            "reaction_type": "User_Uploads",
            "source": save_path
        })
        vs = FAISS.from_documents([doc], cached_embeddings)
        with _vectorstore_lock:
            _vectorstore_cache[product_id] = vs
    except Exception as e:
        print(f"Warning: indexing uploaded file failed: {e}")

    return {"meta": meta, "message": "Uploaded and indexed (if supported).", "product_id": product_id}

# ------------------------------
# product/details (robust with detection, bing fallback, etc.)
@app.post("/product/details")
def product_details(req: QARequest):
    try:
        q_text = (req.question or "").strip()
        if not q_text:
            return JSONResponse(status_code=400, content={"error": "question is required"})

        CANONICAL_EXTRACTION = "extract api name, reaction chemistry, yield, procedure, and tabular data"

        def _normalize_alnum(s: str) -> str:
            return re.sub(r"[^a-z0-9]", "", (s or "").lower())

        def _tokens(s: str):
            return [t for t in re.split(r'[^a-z0-9]+', (s or "").lower()) if t]

        print("=== /product/details called ===")
        print("Question:", q_text)

        # detection by product name (3 strategies)
        def _detect_product_by_name(question: str) -> Optional[Dict[str, Any]]:
            products = list_products()
            if not products:
                return None
            q_norm_alnum = _normalize_alnum(question)
            q_tokens = set(_tokens(question))
            sorted_products = sorted(products, key=lambda p: len(p["name"]), reverse=True)

            # exact normalized substring
            for p in sorted_products:
                name_norm = _normalize_alnum(p["name"])
                if name_norm and name_norm in q_norm_alnum:
                    print(f"DEBUG: Exact normalized substring -> {p['name']}")
                    return p

            # token overlap
            token_matches = []
            for p in sorted_products:
                pname_tokens = set(_tokens(p["name"]))
                if not pname_tokens:
                    continue
                overlap = pname_tokens.intersection(q_tokens)
                if overlap:
                    token_matches.append((p, len(overlap), len(pname_tokens), overlap))
            if token_matches:
                token_matches.sort(key=lambda x: (-(x[1]/x[2]), -x[1]))
                p_best, match_count, token_count, overlap = token_matches[0]
                ratio = match_count / token_count
                print(f"DEBUG: token overlap candidate {p_best['name']} ratio={ratio:.2f}")
                if ratio >= 0.5:
                    return p_best

            # similarity fallback
            best = None
            best_ratio = 0.0
            for p in products:
                pname = (p["name"] or "").lower()
                if not pname.strip():
                    continue
                r1 = SequenceMatcher(None, pname, question.lower()).ratio()
                r2 = SequenceMatcher(None, _normalize_alnum(pname), _normalize_alnum(question)).ratio()
                ratio = (r1 + r2) / 2.0
                if ratio > best_ratio:
                    best_ratio = ratio
                    best = (p, ratio, r1, r2)
            if best:
                p_best, ratio, r1, r2 = best
                print(f"DEBUG: similarity best -> {p_best['name']} ratio={ratio:.3f}")
                if ratio >= 0.60:
                    return p_best
                else:
                    print("DEBUG: best similarity below threshold")
            return None

        # run retrieval for a given product
        def _run_retrieval_for_product(product: Dict[str, Any], question: str, k: int = 3) -> Dict[str, Any]:
            print(f"DEBUG: Running retrieval for product {product['id']}")
            pdf_path = product.get("pdf_path")
            if not pdf_path or not os.path.exists(pdf_path):
                raise HTTPException(status_code=404, detail="Product file not found")
            pid = product["id"]
            # try cached vectorstore, else build
            with _vectorstore_lock:
                vs = _vectorstore_cache.get(pid)
            if not vs:
                vs = build_product_vector_store(product)
                if not vs:
                    raise HTTPException(status_code=500, detail="Failed to build vectorstore for product")
            retriever = vs.as_retriever(search_kwargs={"k": k})
            qa_chain = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": QA_PROMPT},
                return_source_documents=True
            )
            out = qa_chain({"query": question})
            answer_text = out.get("result") or out.get("output_text") or ""
            source_docs = out.get("source_documents", []) or []
            # unique sources
            seen = set()
            sources = []
            for sd in source_docs:
                pid2 = sd.metadata.get("product_id")
                pname = sd.metadata.get("product_name")
                if pid2 and pid2 not in seen:
                    seen.add(pid2)
                    sources.append({"product_id": pid2, "product_name": pname})
            return {"answer": answer_text, "sources": sources}

        # 1) if explicit product_id provided
        if req.product_id:
            products = list_products()
            product = next((p for p in products if p["id"] == req.product_id), None)
            if not product:
                return JSONResponse(status_code=404, content={"error": "Product not found"})
            is_extraction = q_text.strip().lower() == CANONICAL_EXTRACTION
            if not is_extraction:
                return _run_retrieval_for_product(product, q_text, k=3)
            # extraction path (structured)
            if req.product_id in _product_details_cache:
                return _product_details_cache[req.product_id]
            vs = build_product_vector_store(product)
            if not vs:
                return JSONResponse(status_code=500, content={"error": "Failed to build vector store (empty/invalid file)"})
            retriever = vs.as_retriever(search_kwargs={"k": 1})
            qa_chain = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": EXTRACTION_PROMPT},
                return_source_documents=False,
            )
            raw_response = qa_chain.run(q_text)
            parsed = parse_structured_response(raw_response)
            _product_details_cache[req.product_id] = parsed
            return parsed

        # 2) no explicit product_id: attempt detect in free text
        detected = _detect_product_by_name(q_text)
        if detected:
            print(f"DEBUG: Detected product: {detected['name']} -> running retrieval")
            return _run_retrieval_for_product(detected, q_text, k=3)

        # 3) no product found -> do a multi-document retrieval across available docs (bounded)
        print("DEBUG: No product detected - attempting multi-doc retrieval")
        products_all = list_products()
        docs = []
        # limit number of docs to avoid heavy indexing (e.g., 50)
        MAX_DOCS = 50
        count = 0
        for p in products_all:
            if count >= MAX_DOCS:
                break
            path = p.get("pdf_path")
            if not path or not os.path.exists(path):
                continue
            # Build or reuse vector store but collect Document objects to build combined index
            try:
                # reuse extraction logic
                ext = os.path.splitext(path)[1].lower()
                text = ""
                if ext == ".pdf":
                    text = extract_pdf_text(path)
                elif ext in [".docx", ".doc"]:
                    text = extract_docx_text(path)
                elif ext == ".pptx":
                    text = extract_pptx_text(path)
                elif ext in [".xlsx", ".xls"]:
                    text = extract_xls_text(path)
                elif ext == ".csv":
                    text = extract_csv_text(path)
                else:
                    try:
                        text = extract_pdf_text(path)
                    except Exception:
                        text = f"[file: {os.path.basename(path)}]"
                if not text or len(text.strip()) < 50:
                    continue
                doc = Document(page_content=text, metadata={"product_id": p["id"], "product_name": p["name"], "reaction_type": p["reaction_type"]})
                docs.append(doc)
                count += 1
            except Exception:
                continue

        if docs:
            try:
                vector_store = FAISS.from_documents(docs, cached_embeddings)
                retriever = vector_store.as_retriever(search_kwargs={"k": 4})
                qa_chain = RetrievalQA.from_chain_type(
                    llm=chat_model,
                    chain_type="stuff",
                    retriever=retriever,
                    chain_type_kwargs={"prompt": QA_PROMPT},
                    return_source_documents=True
                )
                out = qa_chain({"query": q_text})
                answer_text = out.get("result") or out.get("output_text") or ""
                source_docs = out.get("source_documents", []) or []
                seen = set()
                sources = []
                for sd in source_docs:
                    pid2 = sd.metadata.get("product_id")
                    pname = sd.metadata.get("product_name")
                    if pid2 and pid2 not in seen:
                        seen.add(pid2)
                        sources.append({"product_id": pid2, "product_name": pname})
                return {"answer": answer_text, "sources": sources}
            except Exception as e:
                print("Multi-doc retrieval error:", e)
                # fall through to web search if enabled

        # 4) fallback to Bing web search (if enabled)
        if USE_BING_IF_NO_DOCS and BING_SEARCH_ENDPOINT and BING_SEARCH_KEY:
            print("DEBUG: Falling back to Bing web search for query")
            bing_result = bing_search(q_text, top_k=5)
            web_pages = bing_result.get("web_pages", [])
            # build a short context string (snippets) for LLM to synthesize an answer
            snippets = "\n\n".join([f"{i+1}. {wp['name']}\nURL: {wp['url']}\nSnippet: {wp['snippet']}" for i, wp in enumerate(web_pages)])
            combined_context = f"Search results for: {q_text}\n\n{snippets}"
            # create a simple LLMChain to synthesize a concise answer from snippets
            slide_prompt = PromptTemplate(template="""You are a concise helpful assistant. Use ONLY the following search result snippets to answer the question. If the answer cannot be found, say "I could not find an answer in the provided documents." Context: {context}\nQuestion: {question}\nAnswer:""", input_variables=["context", "question"])
            llm_chain = LLMChain(llm=chat_model, prompt=slide_prompt)
            try:
                ans = llm_chain.run({"context": combined_context, "question": q_text})
            except Exception as e:
                print("LLM synthesis from Bing snippets failed:", e)
                ans = "I could not synthesize an answer from web results."
            return {"answer": ans, "web_results": web_pages}
        else:
            # No docs and no bing -> respond politely
            return {"response": "I could not find any matching documents. Try selecting a product, uploading a relevant document, or enable web search."}

    except HTTPException as he:
        raise he
    except Exception as e:
        tb = traceback.format_exc()
        print("=== /product/details ERROR ===")
        print(tb)
        trace_lines = tb.splitlines()[-30:]
        return JSONResponse(status_code=500, content={
            "error": "Internal server error in /product/details",
            "message": str(e),
            "trace": trace_lines
        })

# ------------------------------
# Query multiple documents
@app.post("/query")
def query_documents(req: QueryRequest):
    if not req.product_ids:
        raise HTTPException(status_code=400, detail="product_ids must be a non-empty list")
    if not req.question or not req.question.strip():
        raise HTTPException(status_code=400, detail="question required")

    docs = []
    for pid in req.product_ids:
        try:
            meta = product_meta(pid)
        except HTTPException:
            continue
        pdf_path = meta.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            continue
        try:
            ext = os.path.splitext(pdf_path)[1].lower()
            if ext == ".pdf":
                text = extract_pdf_text(pdf_path)
            elif ext in [".docx", ".doc"]:
                text = extract_docx_text(pdf_path)
            elif ext == ".pptx":
                text = extract_pptx_text(pdf_path)
            elif ext in [".xlsx", ".xls"]:
                text = extract_xls_text(pdf_path)
            elif ext == ".csv":
                text = extract_csv_text(pdf_path)
            else:
                try:
                    text = extract_pdf_text(pdf_path)
                except:
                    text = f"[file: {os.path.basename(pdf_path)}]"
            if not text or len(text.strip()) < 50:
                continue
            doc = Document(page_content=text, metadata={
                "product_id": pid,
                "product_name": meta.get("name", ""),
                "reaction_type": meta.get("reaction_type", "")
            })
            docs.append(doc)
        except Exception:
            continue

    if len(docs) == 0:
        raise HTTPException(status_code=404, detail="No documents available for the provided product_ids")

    try:
        vector_store = FAISS.from_documents(docs, cached_embeddings)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed building vector index: {e}")

    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    qa_chain = RetrievalQA.from_chain_type(
        llm=chat_model,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": QA_PROMPT},
        return_source_documents=True
    )

    try:
        out = qa_chain({"query": req.question})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"QA execution failed: {e}")

    answer_text = out.get("result") or out.get("output_text") or str(out)
    source_docs = out.get("source_documents", [])
    seen = set()
    sources = []
    for sd in source_docs:
        pid = sd.metadata.get("product_id")
        pname = sd.metadata.get("product_name")
        if pid and pid not in seen:
            seen.add(pid)
            sources.append({"product_id": pid, "product_name": pname})
    return {"answer": answer_text, "sources": sources}

# ------------------------------
# Search endpoint (autocomplete)
@app.get("/products/search")
def search_products(q: str = "", limit: int = 10):
    if q is None:
        q = ""
    q_norm = re.sub(r'[^a-z0-9]', '', q.lower())
    if q_norm == "":
        prods = list_products()[:limit]
        return [{"id": p["id"], "name": p["name"], "reaction_type": p["reaction_type"]} for p in prods]
    products = list_products()
    prefix_matches = []
    substring_matches = []
    for p in products:
        name_norm = re.sub(r'[^a-z0-9]', '', p["name"].lower())
        if name_norm.startswith(q_norm):
            prefix_matches.append(p)
        elif q_norm in name_norm:
            substring_matches.append(p)
    prefix_matches = sorted(prefix_matches, key=lambda x: -len(x["name"]))
    substring_matches = sorted(substring_matches, key=lambda x: -len(x["name"]))
    combined = prefix_matches + substring_matches
    combined = combined[:limit]
    return [{"id": p["id"], "name": p["name"], "reaction_type": p["reaction_type"]} for p in combined]

# ------------------------------
# PPT generation endpoint
@app.post("/generate_ppt")
def generate_ppt(body: dict):
    """
    body: { "topic": "...", "n_slides": 5 }
    Returns a FileResponse with a generated PPTX.
    """
    topic = (body.get("topic") or "").strip()
    n_slides = int(body.get("n_slides") or 5)
    if not topic:
        raise HTTPException(status_code=400, detail="topic required")
    if n_slides < 1 or n_slides > 30:
        raise HTTPException(status_code=400, detail="n_slides must be between 1 and 30")

    prompt_template = PromptTemplate(
        template="""
You are an expert slide-writer. Produce a JSON array of exactly {n} slide objects for the topic: "{topic}".
Each slide object must have this structure:
{{"title": "Slide title", "bullets": ["first bullet", "second bullet", ...]}}
Bullets should be concise (3-6 items per slide maximum). The entire response must be valid JSON (an array of objects). No extra commentary.
""",
        input_variables=["topic", "n"]
    )
    llm_chain = LLMChain(llm=chat_model, prompt=prompt_template)
    try:
        raw = llm_chain.run({"topic": topic, "n": n_slides})
    except Exception as e:
        print("LLM error generating slide JSON:", e)
        raise HTTPException(status_code=500, detail="LLM error while generating slides")

    # Try to extract JSON array substring if the model added backticks or text
    json_text = None
    try:
        # look for first '[' and last ']'
        start = raw.find('[')
        end = raw.rfind(']')
        if start != -1 and end != -1 and end > start:
            json_text = raw[start:end+1]
            slides = json.loads(json_text)
        else:
            slides = json.loads(raw)
    except Exception as e:
        # try a looser extraction using regex for JSON array
        try:
            m = re.search(r'(\[.*\])', raw, re.DOTALL)
            if m:
                json_text = m.group(1)
                slides = json.loads(json_text)
            else:
                raise
        except Exception as e2:
            print("Failed to parse JSON from LLM output:", e2)
            raise HTTPException(status_code=500, detail="Failed to parse slide JSON from LLM output")

    # Build PPTX
    prs = Presentation()
    # set slide layout 1 (title + content)
    for s in slides:
        title = s.get("title", "")[:200]
        bullets = s.get("bullets", [])[:8]
        slide_layout = prs.slide_layouts[1] if len(prs.slide_layouts) > 1 else prs.slide_layouts[0]
        slide = prs.slides.add_slide(slide_layout)
        # set title
        try:
            slide.shapes.title.text = title
        except Exception:
            pass
        # body placeholder
        try:
            body = slide.shapes.placeholders[1].text_frame
            body.clear()
            for b in bullets:
                p = body.add_paragraph()
                p.text = b
                p.level = 0
                p.font.size = Pt(18)
        except Exception:
            # fallback: create a textbox
            txBox = slide.shapes.add_textbox(Inches(1), Inches(1.5), Inches(8), Inches(4))
            tf = txBox.text_frame
            tf.text = "\n".join(bullets)
    # Save to bytes
    fobj = io.BytesIO()
    prs.save(fobj)
    fobj.seek(0)
    filename = f"slides_{re.sub(r'[^0-9a-zA-Z]+','_', topic)[:60]}_{int(time.time())}.pptx"
    return StreamingResponse(fobj, media_type="application/vnd.openxmlformats-officedocument.presentationml.presentation", headers={
        "Content-Disposition": f'attachment; filename="{filename}"'
    })

# ------------------------------
# Transcribe endpoint (unchanged guidance)
GOOGLE_CREDENTIALS_JSON = ""

@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...), use_google: Optional[bool] = Form(False)):
    content = await file.read()
    if not use_google:
        return {"error": "Server-side transcription disabled. Use browser Web Speech API for demo, or set use_google=True and provide GOOGLE_CREDENTIALS_JSON in main.py."}
    if not GOOGLE_CREDENTIALS_JSON:
        return {"error": "GOOGLE_CREDENTIALS_JSON not configured in main.py. Paste credentials JSON string into file to enable."}
    try:
        from google.cloud import speech_v1p1beta1 as speech
        from google.oauth2 import service_account
    except Exception as e:
        return {"error": f"google-cloud-speech library not installed: {e}"}
    creds_info = json.loads(GOOGLE_CREDENTIALS_JSON)
    credentials = service_account.Credentials.from_service_account_info(creds_info)
    client = speech.SpeechClient(credentials=credentials)
    audio = speech.RecognitionAudio(content=content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="en-US",
        enable_automatic_punctuation=True,
    )
    response = client.recognize(config=config, audio=audio)
    transcripts = [r.alternatives[0].transcript for r in response.results]
    return {"transcript": " ".join(transcripts)}

# ------------------------------
# Health and auth
@app.get("/health")
def health():
    return {"status": "ok"}

def create_access_token(data: dict, expires_delta: Union[timedelta , None] = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

@app.post("/login")
async def login(user_info: dict = Depends(lambda: {"first_name":"demo","last_name":"user"})):
    user_info = {"first_name":"Demo","last_name":"User"}
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user_info["first_name"]},
        expires_delta=access_token_expires
    )
    return {
        "access_token": access_token,
        "token_type": "Bearer",
        "firstname": user_info["first_name"],
        "lastname": user_info["last_name"],
    }
