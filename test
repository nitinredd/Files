import os
import io
import json
import uuid
import pandas as pd
import logging
import speech_recognition as sr
import time
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Union, List, Dict, Any

from gtts import gTTS
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ─── CONFIG ──────────────────────────────────────────────────────

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure GPT configuration (fill these in)
base_url=""
api_version="2024-02-15-preview"
api_key=""
deployment_name="GPT4o"
model_name="GPT4o"

# Paths to embedded JSON files
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Prime_M+F_JSON.json",
}

# Sample‑prompt tiles (for frontend to fetch)
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X":["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# RATE LIMITING CONFIG
BATCH_SIZE = 50  # Process 50 documents at a time
BATCH_DELAY = 2  # Wait 2 seconds between batches
MAX_RETRIES = 3
RETRY_DELAY = 5

# ─── SETUP LLM + EMBEDDINGS ──────────────────────────────────────

# Embedding store & cache
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment="Def_data_qa",
    chunk_size=16  # Reduce concurrent requests
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url,
    temperature=0
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# ─── SMART CHUNKING UTILITIES ────────────────────────────────────

def flatten_json(data: Dict[str, Any], parent_key: str = '', sep: str = '.') -> Dict[str, Any]:
    """Flatten nested JSON to make it easier to chunk semantically."""
    items = []
    for k, v in data.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_json(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            if v and isinstance(v[0], dict):
                for i, item in enumerate(v):
                    items.extend(flatten_json(item, f"{new_key}[{i}]", sep=sep).items())
            else:
                items.append((new_key, str(v)))
        else:
            items.append((new_key, v))
    return dict(items)

def create_semantic_chunks(record: Dict[str, Any], record_id: str) -> List[Dict[str, Any]]:
    """
    Create semantic chunks with SMART strategy to prevent hallucinations.
    - Creates 1-2 chunks per record (not 4+)
    - Each chunk is COMPLETE and SELF-CONTAINED
    - Preserves full record context to prevent information loss
    """
    chunks = []
    
    flat_record = flatten_json(record)
    
    # Extract identifiers that should be in EVERY chunk
    identifier_fields = ['id', 'name', 'product', 'batch', 'formula', 'code', 'number']
    identifiers = {}
    identifier_content = []
    
    for key, value in flat_record.items():
        key_lower = key.lower()
        if any(id_field in key_lower for id_field in identifier_fields):
            identifiers[key] = value
            identifier_content.append(f"{key}: {value}")
    
    # Build complete content string
    all_fields = [f"{k}: {v}" for k, v in flat_record.items()]
    
    # STRATEGY: Create chunks that stay under token limits but remain complete
    # Token limit is ~8191 for embeddings, ~2000 tokens = safe chunk
    # Rough estimate: 1 token ≈ 4 characters
    MAX_CHUNK_CHARS = 7000  # ~1750 tokens, leaves room for safety
    
    full_content = " | ".join(all_fields)
    
    if len(full_content) <= MAX_CHUNK_CHARS:
        # Small record: One complete chunk with ALL information
        chunks.append({
            "content": f"RECORD {record_id}: {full_content}",
            "metadata": {
                "record_id": record_id,
                "full_record": json.dumps(record),
                "chunk_part": "complete",
                **identifiers
            }
        })
    else:
        # Large record: Split into 2 overlapping chunks
        # CRITICAL: Both chunks include identifiers for context continuity
        mid_point = len(all_fields) // 2
        
        # First half with identifiers
        first_half = identifier_content + all_fields[:mid_point]
        chunk1_content = " | ".join(first_half)
        
        chunks.append({
            "content": f"RECORD {record_id} (Part 1/2): {chunk1_content}",
            "metadata": {
                "record_id": record_id,
                "full_record": json.dumps(record),
                "chunk_part": "1_of_2",
                **identifiers
            }
        })
        
        # Second half with identifiers (overlap ensures context)
        second_half = identifier_content + all_fields[mid_point:]
        chunk2_content = " | ".join(second_half)
        
        chunks.append({
            "content": f"RECORD {record_id} (Part 2/2): {chunk2_content}",
            "metadata": {
                "record_id": record_id,
                "full_record": json.dumps(record),
                "chunk_part": "2_of_2",
                **identifiers
            }
        })
    
    return chunks

# ─── ENHANCED PROMPT TEMPLATE ────────────────────────────────────

ENHANCED_PROMPT = PromptTemplate(
    template="""You are a precise data assistant. Use ONLY the information provided in the context below to answer the question.

Context Information:
{context}

Question: {question}

Instructions:
1. Answer ONLY based on the provided context
2. If the information is in the context, provide specific details
3. If the context contains partial information, state what you know and what's missing
4. If the answer is NOT in the context, say "I don't have that information in the provided data"
5. Never make up or infer information not explicitly stated
6. When multiple records match, provide information from all relevant records

Answer:""",
    input_variables=["context", "question"]
)

# ─── AGENT CLASSES ───────────────────────────────────────────────

class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": ENHANCED_PROMPT}
        )
    
    def ask(self, query: str) -> tuple[str, List[Document]]:
        resp = self.chain.invoke({"query": query})
        answer = resp.get("result", "")
        sources = resp.get("source_documents", [])
        
        # ANTI-HALLUCINATION: Reconstruct full records from metadata
        # If we retrieved partial chunks, get the complete record
        enhanced_sources = []
        seen_records = set()
        
        for doc in sources:
            record_id = doc.metadata.get("record_id")
            if record_id and record_id not in seen_records:
                seen_records.add(record_id)
                # Get full record from metadata
                full_record = doc.metadata.get("full_record")
                if full_record:
                    enhanced_sources.append(Document(
                        page_content=f"COMPLETE RECORD: {full_record}",
                        metadata=doc.metadata
                    ))
                else:
                    enhanced_sources.append(doc)
            elif not record_id:
                enhanced_sources.append(doc)
        
        return answer, enhanced_sources

class CoordinatorAgent:
    def __init__(self, children: List[ChildAgent]):
        self.children = children

    def coordinate(self, query: str) -> Union[tuple[str, List[Document]], tuple[None, None]]:
        best_answer = None
        best_sources = []
        best_confidence = 0
        
        for child in self.children:
            try:
                ans, sources = child.ask(query)
                
                confidence = 0
                if ans and len(ans) > 20:
                    confidence += 30
                if "don't have" not in ans.lower() and "not found" not in ans.lower():
                    confidence += 40
                if sources:
                    confidence += 30
                
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_answer = ans
                    best_sources = sources
                    
                logger.info(f"Child {child.name} confidence: {confidence}")
                
            except Exception as e:
                logger.warning(f"Child agent {child.name} error: {str(e)}")
        
        if best_confidence > 50:
            return best_answer, best_sources
        return None, None

class OversightAgent:
    def validate(self, answer: str, sources: List[Document]) -> str:
        """
        Validate answer quality and check for hallucinations.
        """
        if not answer or "don't have" in answer.lower():
            return answer
        if len(answer.strip()) < 10:
            return "The answer appears incomplete. Please try rephrasing your question."
        
        # ANTI-HALLUCINATION CHECK: Verify key facts appear in sources
        # Extract specific values from answer (numbers, names, etc.)
        import re
        
        # Extract quoted values, numbers, and specific terms from answer
        answer_facts = set()
        
        # Get numbers (like batch sizes, temperatures, etc.)
        numbers = re.findall(r'\b\d+(?:\.\d+)?\b', answer)
        answer_facts.update(numbers)
        
        # Get capitalized words (likely names, products, etc.)
        words = re.findall(r'\b[A-Z][a-zA-Z0-9]+\b', answer)
        answer_facts.update(words)
        
        # Check if main facts appear in source documents
        if sources and answer_facts:
            source_text = " ".join([doc.page_content for doc in sources])
            
            # Count how many answer facts appear in sources
            facts_found = sum(1 for fact in answer_facts if fact in source_text)
            confidence_ratio = facts_found / len(answer_facts) if answer_facts else 0
            
            # If less than 50% of specific facts are in sources, it might be hallucinated
            if confidence_ratio < 0.5 and len(answer_facts) > 3:
                logger.warning(f"Low confidence answer detected: {confidence_ratio:.2%} facts verified")
                return "I found some information, but I'm not fully confident in the answer. Please verify or rephrase your question for better accuracy."
        
        return answer

class LearningAgent:
    def __init__(self):
        self.logs: List[Dict] = []
    
    def log(self, q: str, a: str, sources: List[Document] = None):
        self.logs.append({
            "query": q,
            "response": a,
            "source_count": len(sources) if sources else 0
        })

class AgentManager:
    def __init__(self, agents: List[ChildAgent]):
        self.coordinator = CoordinatorAgent(agents)
        self.oversight  = OversightAgent()
        self.learning   = LearningAgent()
    
    def handle_query(self, query: str) -> str:
        raw, sources = self.coordinator.coordinate(query)
        answer = raw if raw else "I couldn't find relevant information for your query. Please try rephrasing or ask about specific fields in the dataset."
        validated = self.oversight.validate(answer, sources or [])
        self.learning.log(query, validated, sources)
        return validated

# ─── DATA LOADING & VECTORSTORE BUILD WITH RATE LIMITING ─────────

def load_json_data_with_chunking(paths: Dict[str, str]) -> Dict[str, List[Dict[str, Any]]]:
    """Load JSON data and create semantic chunks."""
    all_chunks: Dict[str, List[Dict[str, Any]]] = {}
    
    for name, path in paths.items():
        if not path:
            continue
        try:
            logger.info(f"[Loading] Reading file: {name}")
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            chunks = []
            if isinstance(data, list):
                total_records = len(data)
                logger.info(f"[Chunking] Processing {total_records} records from '{name}'")
                
                for idx, item in enumerate(data):
                    record_id = f"{name}_{idx}"
                    record_chunks = create_semantic_chunks(item, record_id)
                    chunks.extend(record_chunks)
                    
                    if (idx + 1) % 1000 == 0:
                        logger.info(f"[Chunking] Processed {idx + 1}/{total_records} records")
            else:
                record_chunks = create_semantic_chunks(data, f"{name}_0")
                chunks.extend(record_chunks)
            
            all_chunks[name] = chunks
            logger.info(f"[Data] Created {len(chunks)} chunks from {name}")
            
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    
    return all_chunks

def build_vectorstores_with_rate_limiting(chunks_dict: Dict[str, List[Dict[str, Any]]]) -> List[ChildAgent]:
    """
    Build FAISS vectorstores with rate limiting to avoid 429 errors.
    Processes documents in batches with delays.
    """
    agents: List[ChildAgent] = []
    
    for key, chunks in chunks_dict.items():
        logger.info(f"[Vectorstore] Building store for '{key}' with {len(chunks)} chunks")
        
        docs = [
            Document(
                page_content=chunk["content"],
                metadata=chunk["metadata"]
            )
            for chunk in chunks
        ]
        
        if not docs:
            continue
        
        # Process in batches to avoid rate limits
        total_batches = (len(docs) + BATCH_SIZE - 1) // BATCH_SIZE
        logger.info(f"[Vectorstore] Processing {len(docs)} docs in {total_batches} batches")
        
        store = None
        for batch_idx in range(0, len(docs), BATCH_SIZE):
            batch_docs = docs[batch_idx:batch_idx + BATCH_SIZE]
            batch_num = (batch_idx // BATCH_SIZE) + 1
            
            retry_count = 0
            while retry_count < MAX_RETRIES:
                try:
                    logger.info(f"[Vectorstore] Processing batch {batch_num}/{total_batches} ({len(batch_docs)} docs)")
                    
                    if store is None:
                        # Create initial store
                        store = FAISS.from_documents(batch_docs, cached_embeddings)
                    else:
                        # Add to existing store
                        store.add_documents(batch_docs)
                    
                    logger.info(f"[Vectorstore] Batch {batch_num}/{total_batches} completed")
                    break  # Success, exit retry loop
                    
                except Exception as e:
                    if "429" in str(e) or "rate" in str(e).lower():
                        retry_count += 1
                        wait_time = RETRY_DELAY * (2 ** (retry_count - 1))  # Exponential backoff
                        logger.warning(f"[Vectorstore] Rate limit hit. Retry {retry_count}/{MAX_RETRIES} after {wait_time}s")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"[Vectorstore] Error processing batch: {e}")
                        raise
            
            # Delay between batches to respect rate limits
            if batch_idx + BATCH_SIZE < len(docs):
                logger.info(f"[Vectorstore] Waiting {BATCH_DELAY}s before next batch...")
                time.sleep(BATCH_DELAY)
        
        if store:
            # Create retriever with optimal settings
            retriever = store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 10}  # Retrieve more since we have fewer chunks per record
            )
            
            agents.append(ChildAgent(name=key, retriever=retriever))
            logger.info(f"[Vectorstore] Completed store for '{key}' with {len(docs)} total docs")
    
    return agents

# Initialize on startup
logger.info("=" * 60)
logger.info("[Startup] Initializing Multi-Agent Chatbot")
logger.info("=" * 60)
logger.info("[Startup] Loading and chunking data...")
CHUNKED_DATA = load_json_data_with_chunking(EMBEDDED_FILES)
logger.info("[Startup] Building vectorstores with rate limiting...")
AGENTS = build_vectorstores_with_rate_limiting(CHUNKED_DATA)
logger.info("[Startup] Initializing agent manager...")
MANAGER = AgentManager(AGENTS)
recognizer = sr.Recognizer()
logger.info("=" * 60)
logger.info("[Startup] System ready!")
logger.info("=" * 60)

# ─── FASTAPI APP ─────────────────────────────────────────────────

app = FastAPI(title="Multi-Agent JSON Chatbot with Rate Limiting")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

# ─── ENDPOINTS ────────────────────────────────────────────────────

@app.get("/sample-tiles")
def sample_tiles():
    """Return the sample-prompt tiles and questions."""
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
def chat(req: ChatRequest):
    q = req.message.strip()
    if not q:
        raise HTTPException(400, "Empty query")
    logger.info(f"[API] /chat query='{q}'")
    ans = MANAGER.handle_query(q)
    return {"response": ans}

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    """Accepts an uploaded audio file and returns transcribed text."""
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"[API] STT error: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    """Returns an MP3 audio stream of the given text."""
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"[API] TTS error: {e}")
        raise HTTPException(500, str(e))

@app.get("/health")
def health():
    total_chunks = sum(len(chunks) for chunks in CHUNKED_DATA.values())
    return {
        "status": "ok",
        "agents": [a.name for a in AGENTS],
        "total_chunks": total_chunks
    }

@app.get("/stats")
def stats():
    """Get statistics about the loaded data."""
    stats_info = {}
    for name, chunks in CHUNKED_DATA.items():
        stats_info[name] = {
            "total_chunks": len(chunks),
            "total_records": len(chunks)  # Since we now create 1 chunk per record
        }
    return stats_info
