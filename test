import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
from matplotlib.ticker import MaxNLocator
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')

# Initialize global array for bootstrap
arrayboot = []

# Set random seed for reproducibility
np.random.seed(306)

# --------------------------
# Data Processing Functions
# --------------------------
def load_data(uploaded_file):
    """Load reference and test data from uploaded Excel file"""
    reference_df = pd.read_excel(uploaded_file, sheet_name=0)
    test_df = pd.read_excel(uploaded_file, sheet_name=1)
    return reference_df, test_df

def prepare_data(reference_df, test_df):
    """Remove time zero if present and reset index"""
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.iloc[1:].reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.iloc[1:].reset_index(drop=True)
    return reference_df, test_df

# --------------------------
# Visualization Functions
# --------------------------
def dissolution_curve(reference_df, test_df):
    """Plot dissolution curves"""
    ref_means = reference_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    times = reference_df.iloc[:, 0]
    
    plt.figure(figsize=(10, 6))
    plt.plot(times, ref_means, 'bo-', label='Reference')
    plt.plot(times, test_means, 'g--', label='Test')
    
    plt.xlabel('Time (minutes)')
    plt.ylabel('Dissolution (%)')
    plt.title('Dissolution Profiles')
    plt.grid(True)
    plt.ylim(0, 100)
    plt.legend(loc='lower right')
    plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
    plt.gca().set_yticks(range(0, 101, 10))
    st.pyplot(plt)
    plt.close()

def dissolution_curve_interval(reference_df, test_df):
    """Plot dissolution curves with intervals"""
    ref_means = reference_df.iloc[:, 1:].mean(axis=1)
    ref_mins = reference_df.iloc[:, 1:].min(axis=1)
    ref_maxs = reference_df.iloc[:, 1:].max(axis=1)
    
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    test_mins = test_df.iloc[:, 1:].min(axis=1)
    test_maxs = test_df.iloc[:, 1:].max(axis=1)
    
    times = reference_df.iloc[:, 0]
    
    plt.figure(figsize=(10, 6))
    
    # Reference with error bars
    plt.errorbar(times, ref_means, 
                 yerr=[ref_means - ref_mins, ref_maxs - ref_means],
                 fmt='o-', color='blue', label='Reference')
    
    # Test with error bars
    plt.errorbar(times, test_means, 
                 yerr=[test_means - test_mins, test_maxs - test_means],
                 fmt='s--', color='green', label='Test')
    
    plt.xlabel('Time (minutes)')
    plt.ylabel('Dissolution (%)')
    plt.title('Dissolution Profiles with Variability Ranges')
    plt.grid(True)
    plt.ylim(0, 100)
    plt.legend(loc='lower right')
    plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
    plt.gca().set_yticks(range(0, 101, 10))
    st.pyplot(plt)
    plt.close()

# --------------------------
# Validation Checks
# --------------------------
def check_time_points(df):
    """Check minimum time points requirement"""
    if df.iloc[0, 0] == 0 or str(df.iloc[0, 0]).lower() == '0':
        return len(df) - 1 >= 3
    return len(df) >= 3

def two_time_points(df):
    """Check if first two time points exceed 85%"""
    means = df.iloc[:, 1:].mean(axis=1)
    if len(means) >= 2:
        return means.iloc[0] <= 85 and means.iloc[1] <= 85
    return True

def min15_check(df):
    """Check if dissolution exceeds 85% within 15 minutes"""
    time_col = df.columns[0]
    if "min" in str(time_col).lower() or "minutes" in str(time_col).lower():
        early_times = df[df.iloc[:, 0] <= 15]
        if not early_times.empty:
            means = early_times.iloc[:, 1:].mean(axis=1)
            return any(means > 85)
    return False

def check_cv(df):
    """Check coefficient of variation requirements"""
    if df.iloc[0, 0] == 0 or str(df.iloc[0, 0]).lower() == '0':
        df = df.iloc[1:]
    
    cv_values = []
    for i in range(len(df)):
        row = df.iloc[i, 1:]
        mean = row.mean()
        std = row.std()
        cv = (std / mean) * 100 if mean > 0 else 0
        cv_values.append(cv)
    
    if len(cv_values) > 0:
        first_cv = cv_values[0]
        other_cv = cv_values[1:] if len(cv_values) > 1 else []
        return first_cv < 20 and all(cv < 10 for cv in other_cv)
    return False

def check_same_time_points(df1, df2):
    """Check if time points are the same for both products"""
    return df1.iloc[:, 0].equals(df2.iloc[:, 0])

def check_sample_units(df):
    """Check minimum sample units requirement"""
    return df.shape[1] - 1 >= 12

# --------------------------
# f2 Calculation Functions
# --------------------------
def row_variance(df):
    """Calculate row-wise variance"""
    return df.iloc[:, 1:].var(axis=1, ddof=1)

def conventional_f2(ref_means, test_means):
    """Calculate conventional f2"""
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    p = len(ref_means)
    return 50 if p == 0 else 100 - 25 * np.log10(1 + (1/p) * sum_sq_diff)

def expected_f2(ref_df, test_df):
    """Calculate expected f2"""
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    
    # Conventional f2 component
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    
    # Variance components
    ref_var = row_variance(ref_df)
    test_var = row_variance(test_df)
    sum_var = (ref_var + test_var).sum()
    
    n = ref_df.shape[1] - 1  # Number of units per time point
    p = len(ref_means)
    
    adjustment = (1/n) * sum_var
    return 100 - 25 * np.log10(1 + (1/p) * (sum_sq_diff + adjustment))

def bias_corrected_f2(ref_df, test_df):
    """Calculate bias-corrected f2"""
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    
    # Conventional f2 component
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    
    # Variance components
    ref_var = row_variance(ref_df)
    test_var = row_variance(test_df)
    sum_var = (ref_var + test_var).sum()
    
    n = ref_df.shape[1] - 1  # Number of units per time point
    p = len(ref_means)
    
    adjustment = (1/n) * sum_var
    right_side = sum_sq_diff + p
    
    if adjustment >= right_side:
        return 100 - 25 * np.log10(1 + (1/p) * (sum_sq_diff - adjustment))
    return "Bias Corrected f2 cannot be calculated"

def calculate_all_f2(ref_df, test_df):
    """Calculate all f2 metrics"""
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    
    return {
        "Conventional": conventional_f2(ref_means, test_means),
        "Expected": expected_f2(ref_df, test_df),
        "Bias Corrected": bias_corrected_f2(ref_df, test_df)
    }

# --------------------------
# Bootstrap Functions
# --------------------------
def bootstrap_f2(ref_df, test_df, calc_func, n_iterations=1000):
    """Bootstrap f2 calculation with BCa confidence intervals"""
    arrayboot.clear()
    n_ref_units = ref_df.shape[1] - 1
    n_test_units = test_df.shape[1] - 1
    time_col = ref_df.iloc[:, 0]
    
    # Original calculation
    original_f2 = calc_func(ref_df, test_df)
    
    # Bootstrap iterations
    f2_values = []
    for _ in range(n_iterations):
        # Resample units with replacement
        ref_sample_idx = np.random.choice(range(1, ref_df.shape[1]), n_ref_units, replace=True)
        test_sample_idx = np.random.choice(range(1, test_df.shape[1]), n_test_units, replace=True)
        
        ref_sample = ref_df.iloc[:, [0] + list(ref_sample_idx)]
        test_sample = test_df.iloc[:, [0] + list(test_sample_idx)]
        
        # Calculate f2 for this sample
        f2_val = calc_func(ref_sample, test_sample)
        if isinstance(f2_val, (int, float)):
            f2_values.append(f2_val)
    
    if not f2_values:
        return original_f2, None, None, f2_values
    
    # BCa calculations
    f2_values = np.array(f2_values)
    mean_f2 = np.mean(f2_values)
    
    # Bias correction
    prop_less = np.mean(f2_values < original_f2)
    z0 = norm.ppf(prop_less)
    
    # Acceleration
    jk_estimates = []
    n = len(f2_values)
    for i in range(n):
        jack_sample = np.delete(f2_values, i)
        jk_estimates.append(np.mean(jack_sample))
    
    jk_mean = np.mean(jk_estimates)
    num = np.sum((jk_mean - jk_estimates) ** 3)
    den = 6 * (np.sum((jk_mean - jk_estimates) ** 2) ** 1.5
    a = num / den if den != 0 else 0
    
    # Confidence intervals
    alpha = 0.1  # 90% CI
    z_alpha = norm.ppf(alpha/2)
    z_1_alpha = norm.ppf(1 - alpha/2)
    
    def calc_bc_percentile(z):
        num = z0 + z
        den = 1 - a * num
        return norm.cdf(z0 + num/den) * 100
    
    lower_pct = calc_bc_percentile(z_alpha)
    upper_pct = calc_bc_percentile(z_1_alpha)
    
    lower_bound = np.percentile(f2_values, lower_pct)
    upper_bound = np.percentile(f2_values, upper_pct)
    
    return original_f2, lower_bound, upper_bound, f2_values

# --------------------------
# Streamlit App
# --------------------------
def main():
    st.set_page_config(page_title="Dissolution Similarity Analyzer", layout="wide")
    st.title("📊 Dissolution Profile Similarity Analyzer")
    st.markdown("""
    This tool calculates f2 similarity metrics for dissolution profiles according to regulatory guidelines.
    Upload your dissolution data to get started.
    """)
    
    # File upload section
    with st.expander("📤 Upload Dissolution Data", expanded=True):
        uploaded_file = st.file_uploader("Upload Excel file with two sheets:", 
                                         type=["xlsx"],
                                         help="First sheet: Reference product, Second sheet: Test product")
    
    if uploaded_file is not None:
        try:
            reference_df, test_df = load_data(uploaded_file)
            
            # Display data preview
            st.subheader("Data Preview")
            col1, col2 = st.columns(2)
            with col1:
                st.write("**Reference Product**")
                st.dataframe(reference_df.head())
            with col2:
                st.write("**Test Product**")
                st.dataframe(test_df.head())
            
            # Perform validation checks
            st.subheader("🔍 Validation Checks")
            checks = {
                "Minimum 3 time points (excl. zero)": 
                    [check_time_points(reference_df), check_time_points(test_df)],
                "First two points ≤85% dissolution": 
                    [two_time_points(reference_df), two_time_points(test_df)],
                "No >85% dissolution in first 15 min": 
                    [not min15_check(reference_df), not min15_check(test_df)],
                "CV requirements met": 
                    [check_cv(reference_df), check_cv(test_df)],
                "≥12 individual units": 
                    [check_sample_units(reference_df), check_sample_units(test_df)],
                "Same time points for both products": 
                    [check_same_time_points(reference_df, test_df)]
            }
            
            # Display check results
            check_df = pd.DataFrame(
                checks.values(),
                index=checks.keys(),
                columns=["Reference", "Test"]
            )
            st.dataframe(check_df.style.applymap(lambda x: "background-color: #90EE90" if x else "background-color: #FFCCCB"))
            
            # Show dissolution profiles
            st.subheader("📈 Dissolution Profiles")
            dissolution_curve(reference_df, test_df)
            dissolution_curve_interval(reference_df, test_df)
            
            # Agency selection
            st.subheader("🏢 Regulatory Agency Selection")
            agency = st.selectbox("Select regulatory agency:", 
                                 ["FDA", "EMA", "China", "ASEAN", "ANVISA", "Other"])
            
            # f2 calculation options
            st.subheader("🧮 f2 Calculation Method")
            
            # Agency recommendations
            recommendations = {
                "FDA": ["Conventional", "Conventional Bootstrap"],
                "EMA": ["Expected", "Expected Bootstrap"],
                "China": ["Conventional", "Expected", "Bias Corrected"],
                "ASEAN": ["Conventional", "Expected", "Bias Corrected"],
                "ANVISA": ["Conventional", "Expected", "Bias Corrected"],
                "Other": ["All Methods"]
            }
            
            # Show recommendation
            rec = recommendations[agency]
            st.info(f"**{agency} recommends:** {', '.join(rec)}")
            
            # Method selection
            options = ["Conventional", "Expected", "Bias Corrected",
                      "Conventional Bootstrap", "Expected Bootstrap", "Bias Corrected Bootstrap"]
            
            # Set defaults based on agency
            if agency == "FDA":
                default = ["Conventional", "Conventional Bootstrap"]
            elif agency == "EMA":
                default = ["Expected", "Expected Bootstrap"]
            else:
                default = ["Conventional", "Expected", "Bias Corrected"]
            
            selected_methods = st.multiselect("Select calculation methods:", 
                                             options, 
                                             default=default)
            
            # Prepare data - remove time zero
            ref_clean, test_clean = prepare_data(reference_df, test_df)
            
            # Perform calculations
            if st.button("Calculate Similarity Metrics", type="primary"):
                with st.spinner("Calculating..."):
                    results = {}
                    
                    # Point estimates
                    if any(m in selected_methods for m in ["Conventional", "Expected", "Bias Corrected"]):
                        f2_estimates = calculate_all_f2(ref_clean, test_clean)
                        
                        if "Conventional" in selected_methods:
                            results["Conventional"] = {
                                "value": f2_estimates["Conventional"],
                                "ci": None
                            }
                        
                        if "Expected" in selected_methods:
                            results["Expected"] = {
                                "value": f2_estimates["Expected"],
                                "ci": None
                            }
                        
                        if "Bias Corrected" in selected_methods:
                            results["Bias Corrected"] = {
                                "value": f2_estimates["Bias Corrected"],
                                "ci": None
                            }
                    
                    # Bootstrap calculations
                    bootstrap_results = {}
                    n_iter = 1000  # Reduced for demo, use 10000 for production
                    
                    if "Conventional Bootstrap" in selected_methods:
                        def conv_func(r, t): 
                            return conventional_f2(r.iloc[:, 1:].mean(axis=1), t.iloc[:, 1:].mean(axis=1))
                        orig, lower, upper, vals = bootstrap_f2(
                            ref_clean, test_clean, conv_func, n_iter)
                        bootstrap_results["Conventional"] = (orig, lower, upper, vals)
                    
                    if "Expected Bootstrap" in selected_methods:
                        orig, lower, upper, vals = bootstrap_f2(
                            ref_clean, test_clean, expected_f2, n_iter)
                        bootstrap_results["Expected"] = (orig, lower, upper, vals)
                    
                    if "Bias Corrected Bootstrap" in selected_methods:
                        def bc_func(r, t): 
                            bc = bias_corrected_f2(r, t)
                            return bc if isinstance(bc, float) else None
                        orig, lower, upper, vals = bootstrap_f2(
                            ref_clean, test_clean, bc_func, n_iter)
                        bootstrap_results["Bias Corrected"] = (orig, lower, upper, vals)
                    
                    # Merge bootstrap results
                    for method, (orig, lower, upper, vals) in bootstrap_results.items():
                        if method in results:
                            results[method]["ci"] = (lower, upper)
                        else:
                            results[method] = {
                                "value": orig,
                                "ci": (lower, upper)
                            }
                    
                    # Display results
                    st.subheader("📊 Results")
                    
                    for method, data in results.items():
                        st.markdown(f"##### {method} f2")
                        col1, col2 = st.columns([1, 3])
                        
                        with col1:
                            if isinstance(data["value"], str):
                                st.metric("Value", data["value"])
                            else:
                                st.metric("Value", f"{data['value']:.2f}")
                            
                            if data["ci"] and all(x is not None for x in data["ci"]):
                                st.metric("90% Confidence Interval", 
                                         f"{data['ci'][0]:.2f} - {data['ci'][1]:.2f}")
                        
                        with col2:
                            if data.get("ci") and all(x is not None for x in data["ci"]):
                                plt.figure(figsize=(8, 4))
                                sns.histplot(vals, kde=True, bins=20)
                                plt.axvline(data['value'], color='r', linestyle='--', label='Original')
                                plt.axvline(data['ci'][0], color='g', linestyle=':', label='CI Lower')
                                plt.axvline(data['ci'][1], color='g', linestyle=':', label='CI Upper')
                                plt.title(f"{method} f2 Bootstrap Distribution")
                                plt.xlabel("f2 Value")
                                plt.legend()
                                st.pyplot(plt)
                                plt.close()
                    
                    # Similarity assessment
                    st.subheader("📝 Similarity Assessment")
                    conv_f2 = results.get("Conventional", {}).get("value", 0)
                    
                    if isinstance(conv_f2, (int, float)):
                        if conv_f2 >= 50:
                            st.success("✅ Profiles are similar (f2 ≥ 50)")
                        else:
                            st.error("❌ Profiles are NOT similar (f2 < 50)")
                        st.info("**Note:** Similarity assessment based on conventional f2 ≥ 50")
                    else:
                        st.warning("⚠️ Conventional f2 not available for similarity assessment")
            
        except Exception as e:
            st.error(f"Error processing data: {str(e)}")
    else:
        st.info("👆 Please upload an Excel file to begin analysis")

if __name__ == "__main__":
    main()
