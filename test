# batch_similarity_analyzer_with_market_rules.py
import os
import math
import streamlit as st
import pandas as pd
import numpy as np
from zipfile import ZipFile
from scipy.stats import skew, kurtosis, norm, probplot
from scipy import stats
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# ----------------------------------------
# Required Helper Functions
# ----------------------------------------

def prepare_data(reference_df, test_df):
    """Remove time zero if present and reset index"""
    if reference_df.shape[0] > 0 and (reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0'):
        reference_df = reference_df.iloc[1:].reset_index(drop=True)
    if test_df.shape[0] > 0 and (test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0'):
        test_df = test_df.iloc[1:].reset_index(drop=True)
    return reference_df, test_df

def conventional_f2(ref_means, test_means):
    """Calculate conventional f2 from 1-D arrays (means per timepoint)."""
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    p = len(ref_means)
    if p == 0:
        return None
    return 100 - 25 * np.log10(1 + (1/p) * sum_sq_diff)

def expected_f2(ref_df, test_df):
    """Calculate expected f2 from full dataframes (first col time, rest units)"""
    # Means per timepoint
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    
    # Conventional f2 component
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    
    # Variance components
    ref_var = row_variance(ref_df)
    test_var = row_variance(test_df)
    sum_var = (ref_var + test_var).sum()
    
    n = ref_df.shape[1] - 1  # Number of units per time point
    p = len(ref_means)
    
    if n == 0 or p == 0:
        return None
    
    adjustment = (1/n) * sum_var
    return 100 - 25 * np.log10(1 + (1/p) * (sum_sq_diff + adjustment))

def bias_corrected_f2(ref_df, test_df):
    """Calculate bias-corrected f2"""
    try:
        ref_means = ref_df.iloc[:, 1:].mean(axis=1)
        test_means = test_df.iloc[:, 1:].mean(axis=1)

        diff = test_means - ref_means
        sum_sq_diff = (diff ** 2).sum()

        ref_var = row_variance(ref_df)
        test_var = row_variance(test_df)
        sum_var = (ref_var + test_var).sum()

        n = ref_df.shape[1] - 1
        p = len(ref_means)

        if n == 0 or p == 0:
            return None

        adjustment = (1 / n) * sum_var
        right_side = sum_sq_diff + p

        if adjustment < right_side:
            adjusted_diff = sum_sq_diff - adjustment
            if adjusted_diff > 0:
                return 100 - 25 * np.log10(1 + (1 / p) * adjusted_diff)
            else:
                return None
        else:
            return None

    except Exception:
        return None

def row_variance(df):
    """Calculate row-wise variance across units (columns 1:)."""
    if df.shape[1] <= 1:
        return pd.Series([0]*df.shape[0])
    return df.iloc[:, 1:].var(axis=1, ddof=1)

def bootstrap_f2(ref_df, test_df, calc_func, n_iterations=10000):
    """Bootstrap f2 calculation (returns mean, median, skewness, and kurtosis)."""
    # Number of units (columns excluding time)
    n_ref_units = max(0, ref_df.shape[1] - 1)
    n_test_units = max(0, test_df.shape[1] - 1)

    original_f2 = calc_func(ref_df, test_df)
    f2_values = []
    
    if n_ref_units == 0 or n_test_units == 0:
        return original_f2, None, None, None, None, None, None, []
    
    for _ in range(n_iterations):
        ref_sample_idx = np.random.choice(range(1, ref_df.shape[1]), n_ref_units, replace=True)
        test_sample_idx = np.random.choice(range(1, test_df.shape[1]), n_test_units, replace=True)

        ref_sample = ref_df.iloc[:, [0] + list(ref_sample_idx)].reset_index(drop=True)
        test_sample = test_df.iloc[:, [0] + list(test_sample_idx)].reset_index(drop=True)

        f2_val = calc_func(ref_sample, test_sample)

        if f2_val is not None and isinstance(f2_val, (int, float, np.floating, np.integer)):
            f2_values.append(float(f2_val))
    
    if not f2_values:
        return original_f2, None, None, None, None, None, None, []
    
    f2_values = np.array(f2_values)
    mean_f2 = np.mean(f2_values)
    median_f2 = np.median(f2_values)
    skewness_f2 = skew(f2_values)
    kurtosis_f2 = kurtosis(f2_values)
    lower_bound = np.percentile(f2_values, 5)
    upper_bound = np.percentile(f2_values, 95)

    return original_f2, lower_bound, upper_bound, mean_f2, median_f2, skewness_f2, kurtosis_f2, f2_values

def simulate_lilliefors_critical_value(n, alpha=0.05, num_simulations=1000):
    """Simulate Lilliefors critical value for confidence bands"""
    D_values = []
    for _ in range(num_simulations):
        sample = np.random.normal(loc=0, scale=1, size=n)
        sample_mean = np.mean(sample)
        sample_std = np.std(sample, ddof=1)
        z_scores = (sample - sample_mean) / sample_std
        z_scores.sort()
        ecdf_sim = np.arange(1, n + 1) / (n + 1)
        cdf = norm.cdf(z_scores)
        D = np.max(np.abs(ecdf_sim - cdf))
        D_values.append(D)
    return np.percentile(D_values, 100 * (1 - alpha))

def create_jmp_style_qq_plot(data, title, method_name, file_name):
    """Create JMP-like QQ plot with Lilliefors style bands."""
    if len(data) == 0:
        return None
    
    n = len(data)
    
    # Sort data and calculate plotting positions
    sorted_data = np.sort(data)
    plotting_positions = norm.ppf((np.arange(1, n + 1)) / (n + 2))
    
    # Fit line for normal distribution
    slope, intercept, r, _, _ = stats.linregress(plotting_positions, sorted_data)
    fit_line = intercept + slope * plotting_positions
    
    # Calculate empirical CDF
    ecdf = (np.arange(1, n + 1)) / (n + 1)
    
    # Get Lilliefors critical value for confidence bands
    D_critical = simulate_lilliefors_critical_value(n=n, alpha=0.05)
    
    # Calculate confidence bands using ECDF approach
    lower_quantile = norm.ppf(np.clip(ecdf - D_critical, 1e-10, 1 - 1e-10))
    upper_quantile = norm.ppf(np.clip(ecdf + D_critical, 1e-10, 1 - 1e-10))
    
    # Transform quantile bands to data space using the fitted line
    lower_band = intercept + slope * lower_quantile
    upper_band = intercept + slope * upper_quantile
    
    # Create the plot
    fig = go.Figure()
    
    # Add curved confidence bands first (so they appear behind)
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=upper_band,
        mode='lines',
        name='Upper Confidence Band',
        line=dict(color='red', width=1, dash='dot'),
        showlegend=False,
        hoverinfo='skip'
    ))
    
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=lower_band,
        mode='lines',
        name='Lower Confidence Band', 
        line=dict(color='red', width=1, dash='dot'),
        showlegend=False,
        hoverinfo='skip'
    ))
    
    # Add the main reference line (solid red)
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=fit_line,
        mode='lines',
        name='Normal Reference Line',
        line=dict(color='red', width=2, dash='solid'),
        showlegend=False,
        hovertemplate='Normal Reference Line<extra></extra>'
    ))
    
    # Add the data points (black circles)
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=sorted_data,
        mode='markers',
        name='Data Points',
        marker=dict(
            color='black',
            size=6,
            symbol='circle',
            line=dict(width=0)
        ),
        showlegend=False,
        hovertemplate=
        '<b>Normal Quantile:</b> %{x:.3f}<br>' +
        '<b>Sample Quantile:</b> %{y:.3f}<br>' +
        '<extra></extra>'
    ))
    
    # Calculate nice axis ranges
    x_range = max(plotting_positions) - min(plotting_positions)
    y_range = max(sorted_data) - min(sorted_data)
    x_margin = x_range * 0.1
    y_margin = y_range * 0.1
    
    # Set up axis ticks similar to JMP
    x_ticks = [-2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]
    x_ticks = [tick for tick in x_ticks if min(plotting_positions)-x_margin <= tick <= max(plotting_positions)+x_margin]
    
    # Calculate statistics for annotation
    mean_val = np.mean(data)
    std_val = np.std(data)
    skewness_val = skew(data)
    kurtosis_val = kurtosis(data)
    
    
    fig.update_layout(
        title=dict(
            text=f'<b>Normal Quantile Plot</b><br><span style="font-size: 12px;">{method_name} - {file_name.replace(".xlsx", "")}</span>',
            x=0.5,
            y=0.95,
            font=dict(size=14, color='black', family='Arial')
        ),
        xaxis=dict(
            title=dict(
                text='<b>Normal Quantile</b>',
                font=dict(size=12, color='black', family='Arial')
            ),
            range=[min(plotting_positions)-x_margin, max(plotting_positions)+x_margin],
            tickvals=x_ticks,
            tickmode='array',
            gridcolor='lightgray',
            gridwidth=0.5,
            showgrid=True,
            zeroline=True,
            zerolinecolor='lightgray',
            zerolinewidth=0.8,
            tickfont=dict(size=10, color='black', family='Arial'),
            linecolor='black',
            linewidth=1,
            mirror=True
        ),
        yaxis=dict(
            title=dict(
                text='<b>Sample Quantile</b>',
                font=dict(size=12, color='black', family='Arial')
            ),
            range=[min(sorted_data)-y_margin, max(sorted_data)+y_margin],
            gridcolor='lightgray',
            gridwidth=0.5,
            showgrid=True,
            zeroline=True,
            zerolinecolor='lightgray',
            zerolinewidth=0.8,
            tickfont=dict(size=10, color='black', family='Arial'),
            linecolor='black',
            linewidth=1,
            mirror=True
        ),
        plot_bgcolor='white',
        paper_bgcolor='white',
        font=dict(family="Arial, sans-serif"),
        hovermode='closest',
        width=600,
        height=500,
        margin=dict(l=80, r=100, t=80, b=60),
        showlegend=False
    )
    
    
    stats_text = (
        f'<b>Statistics:</b><br>'
        f'Mean: {mean_val:.4f}<br>'
        f'Std Dev: {std_val:.4f}<br>'
        f'Skewness: {skewness_val:.4f}<br>'
        f'Kurtosis: {kurtosis_val:.4f}<br>'
        f'R²: {r**2:.4f}<br>'
        f'N: {n:,}'
    )
    
    fig.add_annotation(
        x=0.98,
        y=0.98,
        xref="paper",
        yref="paper",
        text=stats_text,
        showarrow=False,
        align="left",
        bgcolor="white",
        bordercolor="black",
        borderwidth=1,
        borderpad=6,
        font=dict(size=9, color='black', family='Arial')
    )
    
    return fig

def create_combined_qq_plots(bootstrap_results, file_name):
    """Create a combined view of all QQ plots for a file"""
    methods_with_data = [(method, data) for method, data in bootstrap_results.items() if len(data) > 0]
    
    if not methods_with_data:
        return None
    
    n_methods = len(methods_with_data)
    cols = min(3, n_methods)
    rows = (n_methods + cols - 1) // cols
    
    subplot_titles = [method for method, _ in methods_with_data]
    
    fig = make_subplots(
        rows=rows,
        cols=cols,
        subplot_titles=subplot_titles,
        vertical_spacing=0.12,
        horizontal_spacing=0.1
    )
    
    colors = ['#3498DB', '#E74C3C', '#2ECC71', '#F39C12', '#9B59B6', '#1ABC9C']
    
    for idx, (method, data) in enumerate(methods_with_data):
        row = idx // cols + 1
        col = idx % cols + 1
        color = colors[idx % len(colors)]
        
        # Calculate QQ plot data
        (osm, osr), (slope, intercept, r) = probplot(data, dist="norm", plot=None)
        line_x = np.linspace(osm.min(), osm.max(), 100)
        line_y = slope * line_x + intercept
        
        # Add reference line
        fig.add_trace(
            go.Scatter(
                x=line_x,
                y=line_y,
                mode='lines',
                line=dict(color='#E74C3C', width=2),
                showlegend=False,
                hoverinfo='skip'
            ),
            row=row, col=col
        )
        
        # Add data points
        fig.add_trace(
            go.Scatter(
                x=osm,
                y=osr,
                mode='markers',
                marker=dict(
                    color=color,
                    size=6,
                    opacity=0.7,
                    line=dict(width=1, color=color)
                ),
                showlegend=False,
                hovertemplate=f'<b>{method}</b><br>Theoretical: %{{x:.3f}}<br>Sample: %{{y:.3f}}<extra></extra>'
            ),
            row=row, col=col
        )
    
    fig.update_layout(
        title=dict(
            text=f'<b>Normal Quantile Plots Comparison</b><br><sub>{file_name}</sub>',
            x=0.5,
            font=dict(size=16, color='#2C3E50')
        ),
        plot_bgcolor='white',
        paper_bgcolor='white',
        font=dict(family="Arial, sans-serif"),
        height=300 * rows,
        margin=dict(l=60, r=40, t=80, b=40)
    )
    
    fig.update_xaxes(
        title_text="Normal Quantile",
        gridcolor='#ECF0F1',
        showgrid=True,
        tickfont=dict(size=9)
    )
    
    fig.update_yaxes(
        title_text="f2 Value",
        gridcolor='#ECF0F1',
        showgrid=True,
        tickfont=dict(size=9)
    )
    
    return fig

# ----------------------------------------
# New: Market-specific CV & trimming rules
# ----------------------------------------

def compute_cv_per_timepoint(df):
    """Compute CV (%) per timepoint for a dataframe (columns: time, unit1, unit2, ...)."""
    if df.shape[1] <= 1:
        return pd.Series([np.nan]*df.shape[0])
    means = df.iloc[:, 1:].mean(axis=1)
    stds = df.iloc[:, 1:].std(axis=1, ddof=1)
    # avoid division by zero
    with np.errstate(divide='ignore', invalid='ignore'):
        cvs = (stds / means) * 100.0
    cvs = cvs.replace([np.inf, -np.inf], np.nan)
    return cvs.fillna(999.0)  # very large CV if mean is 0 (signals failure)

def check_cv_criteria(ref_df, test_df, market):
    """
    Return True if CV criteria PASS for given market.
    Rules implemented from user's specification:
    - EMA/CHINA/ASEAN/CANADA/WHO: CV <=20% for timepoints up to 10 min, <=10% for rest.
    - USFDA/FDA: CV <=20% for early timepoints up to 15 min, <=10% for rest.
    - ANVISA: CV <=20% for first 40% of points, <=10% for rest.
    We check both reference and test CVs and require both to meet criteria.
    """
    # Normalize market string
    m = (market or "").strip().upper()
    # get time vector
    if ref_df.shape[0] == 0:
        return False
    times = pd.to_numeric(ref_df.iloc[:, 0], errors='coerce')
    if times.isnull().all():
        # cannot evaluate time-based rules -> conservative fail
        return False

    ref_cvs = compute_cv_per_timepoint(ref_df)
    test_cvs = compute_cv_per_timepoint(test_df)

    # default pass True until failure
    pass_criteria = True

    if m in ['EMA', 'CHINA', 'ASEAN', 'CANADA', 'WHO']:
        # early if time <= 10
        early_mask = times <= 10
        # for early: <=20, for rest <=10
        for i in range(len(times)):
            if early_mask.iloc[i]:
                if ref_cvs.iloc[i] > 20 or test_cvs.iloc[i] > 20:
                    pass_criteria = False
                    break
            else:
                if ref_cvs.iloc[i] > 10 or test_cvs.iloc[i] > 10:
                    pass_criteria = False
                    break

    elif m in ['USFDA', 'FDA']:
        early_mask = times <= 15
        for i in range(len(times)):
            if early_mask.iloc[i]:
                if ref_cvs.iloc[i] > 20 or test_cvs.iloc[i] > 20:
                    pass_criteria = False
                    break
            else:
                if ref_cvs.iloc[i] > 10 or test_cvs.iloc[i] > 10:
                    pass_criteria = False
                    break

    elif m == 'ANVISA':
        # first 40% of the total collected points are early
        p = len(times)
        num_early = max(1, int(math.ceil(0.4 * p)))
        for i in range(len(times)):
            if i < num_early:
                if ref_cvs.iloc[i] > 20 or test_cvs.iloc[i] > 20:
                    pass_criteria = False
                    break
            else:
                if ref_cvs.iloc[i] > 10 or test_cvs.iloc[i] > 10:
                    pass_criteria = False
                    break
    else:
        # Unknown market: be conservative (fail)
        pass_criteria = False

    return pass_criteria

def trim_by_85_rule(ref_df, test_df, market):
    """
    Trim timepoints to the appropriate stopping time according to market rules for >85% truncation:
    - FDA & ANVISA: f2 calculated up to the first timepoint where BOTH test and ref mean >=85% (inclusive).
      For FDA additionally, maximum cutting timepoint = 15 min (so we take min(index_of_15min, index_of_both85) if both exist).
    - EMA/CHINA/ASEAN: f2 calculated up to the earliest timepoint where EITHER test or ref mean >=85% (inclusive).
    - Otherwise: no trimming.
    Returns trimmed (ref_df_trimmed, test_df_trimmed)
    """
    # Ensure numeric times
    times = pd.to_numeric(ref_df.iloc[:, 0], errors='coerce')
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)

    # find first index where both >=85
    both_mask = (ref_means >= 85) & (test_means >= 85)
    either_mask = (ref_means >= 85) | (test_means >= 85)

    idx_both = np.where(both_mask)[0]
    idx_either = np.where(either_mask)[0]

    trim_idx = None

    m = (market or "").strip().upper()

    if m in ['USFDA', 'FDA']:
        # index of last time <= 15 min
        idx_time15 = np.where(times <= 15)[0]
        last_idx_time15 = idx_time15[-1] if len(idx_time15) > 0 else None
        if len(idx_both) > 0:
            first_both = int(idx_both[0])
            if last_idx_time15 is not None:
                # cutting point is the earlier of the two (min index)
                trim_idx = min(first_both, last_idx_time15)
            else:
                trim_idx = first_both
        else:
            # no both85, cut at 15 min if present else keep all
            trim_idx = last_idx_time15

    elif m == 'ANVISA':
        if len(idx_both) > 0:
            trim_idx = int(idx_both[0])
        else:
            trim_idx = None  # keep all if no both85

    elif m in ['EMA', 'CHINA', 'ASEAN']:
        if len(idx_either) > 0:
            trim_idx = int(idx_either[0])
        else:
            trim_idx = None

    else:
        # other markets: keep all
        trim_idx = None

    if trim_idx is None:
        return ref_df.copy().reset_index(drop=True), test_df.copy().reset_index(drop=True)
    else:
        # keep rows up to and including trim_idx
        ref_trim = ref_df.iloc[: trim_idx + 1].reset_index(drop=True)
        test_trim = test_df.iloc[: trim_idx + 1].reset_index(drop=True)
        return ref_trim, test_trim

# Wrappers that trim based on market then call the actual calculation

def calc_conventional_f2_for_market(ref_df, test_df, market):
    r_trim, t_trim = trim_by_85_rule(ref_df, test_df, market)
    if r_trim.shape[0] == 0:
        return None
    ref_means = r_trim.iloc[:, 1:].mean(axis=1)
    test_means = t_trim.iloc[:, 1:].mean(axis=1)
    return conventional_f2(ref_means, test_means)

def calc_expected_f2_for_market(ref_df, test_df, market):
    r_trim, t_trim = trim_by_85_rule(ref_df, test_df, market)
    return expected_f2(r_trim, t_trim)

def calc_bias_corrected_f2_for_market(ref_df, test_df, market):
    r_trim, t_trim = trim_by_85_rule(ref_df, test_df, market)
    return bias_corrected_f2(r_trim, t_trim)

# ----------------------------------------
# Batch Processing Functions (updated)
# ----------------------------------------

def load_batch_data(folder_path):
    """Load test and reference data from multiple Excel files in a folder."""
    workbook_data = {}
    
    for file_name in os.listdir(folder_path):
        if file_name.endswith(".xlsx") or file_name.endswith(".xls"):
            file_path = os.path.join(folder_path, file_name)
            try:
                # Expect sheet 0 = reference, sheet 1 = test
                reference_df = pd.read_excel(file_path, sheet_name=0)
                test_df = pd.read_excel(file_path, sheet_name=1)
                workbook_data[file_name] = (reference_df, test_df)
            except Exception as e:
                st.warning(f"Skipping file `{file_name}` due to error: {e}")
    
    return workbook_data

def choose_methods_for_file(market, cv_pass):
    """
    Map to recommended methods depending on market and whether CV criteria passed.
    Returns a list of method keys to compute for this file.
    Method keys: 'Conventional', 'Expected', 'Bias Corrected',
                 'Conventional Bootstrap', 'Expected Bootstrap', 'Bias Corrected Bootstrap'
    """
    m = (market or "").strip().upper()

    # If CV pass:
    # - EMA/CHINA/ASEAN/CANADA/WHO -> Expected (non-bootstrap)
    # - USFDA/FDA/ANVISA -> Conventional (non-bootstrap)
    # If CV fail:
    # - EMA/CHINA/ASEAN/CANADA/WHO -> Expected Bootstrap
    # - USFDA/FDA/ANVISA -> Conventional Bootstrap
    if cv_pass:
        if m in ['EMA', 'CHINA', 'ASEAN', 'CANADA', 'WHO']:
            return ['Expected']
        elif m in ['USFDA', 'FDA', 'ANVISA']:
            return ['Conventional']
        else:
            # conservative fallback: run both Conventional and Expected
            return ['Conventional', 'Expected']
    else:
        if m in ['EMA', 'CHINA', 'ASEAN', 'CANADA', 'WHO']:
            return ['Expected Bootstrap']
        elif m in ['USFDA', 'FDA', 'ANVISA']:
            return ['Conventional Bootstrap']
        else:
            return ['Conventional Bootstrap', 'Expected Bootstrap']

def process_batch(workbook_data, selected_methods, market, use_recommended=True, n_iterations=10000):
    """
    Process multiple workbooks and calculate f2 metrics.
    If use_recommended is True, selected_methods is ignored and mapping per-file is used.
    """
    all_results = []
    bootstrap_data = {}  # Store bootstrap data for QQ plots
    
    for file_name, (reference_df, test_df) in workbook_data.items():
        try:
            ref_clean, test_clean = prepare_data(reference_df.copy(), test_df.copy())
            results = {"File Name": file_name}
            file_bootstrap_data = {}

            # Decide which methods to run for this file
            if use_recommended:
                cv_pass = check_cv_criteria(ref_clean, test_clean, market)
                methods_to_run = choose_methods_for_file(market, cv_pass)
                results['Market'] = market
                results['CV Criteria Pass'] = cv_pass
                results['Recommended Methods'] = ", ".join(methods_to_run)
            else:
                methods_to_run = selected_methods

            # For each method, compute f2 (and bootstrap if requested)
            for method in methods_to_run:
                # Non-bootstrap Conventional
                if method == "Conventional":
                    val = calc_conventional_f2_for_market(ref_clean, test_clean, market)
                    results["Conventional f2"] = val
                elif method == "Expected":
                    val = calc_expected_f2_for_market(ref_clean, test_clean, market)
                    results["Expected f2"] = val
                elif method == "Bias Corrected":
                    val = calc_bias_corrected_f2_for_market(ref_clean, test_clean, market)
                    results["Bias Corrected f2"] = val

                # Bootstrap variants
                elif method == "Conventional Bootstrap":
                    # define calc_func that accepts dfs and applies conventional wrapper
                    def conv_func(r, t):
                        return calc_conventional_f2_for_market(r.reset_index(drop=True), t.reset_index(drop=True), market)
                    orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                        ref_clean, test_clean, conv_func, n_iterations)
                    results["Conventional Bootstrap f2"] = orig
                    results["Conventional Bootstrap CI"] = f"{lower:.2f} - {upper:.2f}" if lower is not None and upper is not None else None
                    results["Conventional Bootstrap Mean"] = mean
                    results["Conventional Bootstrap Median"] = median
                    results["Conventional Bootstrap Skewness"] = skewness
                    results["Conventional Bootstrap Kurtosis"] = kurt
                    file_bootstrap_data["Conventional Bootstrap"] = list(vals)

                elif method == "Expected Bootstrap":
                    def exp_func(r, t):
                        return calc_expected_f2_for_market(r.reset_index(drop=True), t.reset_index(drop=True), market)
                    orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                        ref_clean, test_clean, exp_func, n_iterations)
                    results["Expected Bootstrap f2"] = orig
                    results["Expected Bootstrap CI"] = f"{lower:.2f} - {upper:.2f}" if lower is not None and upper is not None else None
                    results["Expected Bootstrap Mean"] = mean
                    results["Expected Bootstrap Median"] = median
                    results["Expected Bootstrap Skewness"] = skewness
                    results["Expected Bootstrap Kurtosis"] = kurt
                    file_bootstrap_data["Expected Bootstrap"] = list(vals)

                elif method == "Bias Corrected Bootstrap":
                    def bc_func(r, t):
                        return calc_bias_corrected_f2_for_market(r.reset_index(drop=True), t.reset_index(drop=True), market)
                    orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                        ref_clean, test_clean, bc_func, n_iterations)
                    results["Bias Corrected Bootstrap f2"] = orig
                    results["Bias Corrected Bootstrap CI"] = f"{lower:.2f} - {upper:.2f}" if lower is not None and upper is not None else None
                    results["Bias Corrected Bootstrap Mean"] = mean
                    results["Bias Corrected Bootstrap Median"] = median
                    results["Bias Corrected Bootstrap Skewness"] = skewness
                    results["Bias Corrected Bootstrap Kurtosis"] = kurt
                    file_bootstrap_data["Bias Corrected Bootstrap"] = list(vals)

                else:
                    # Unknown method requested (allow original selected strings to pass)
                    # Try to handle if the user passed "Conventional Bootstrap" aliases etc.
                    pass

            all_results.append(results)
            bootstrap_data[file_name] = file_bootstrap_data

        except Exception as e:
            st.warning(f"Error processing file `{file_name}`: {e}")
    
    # Convert results to DataFrame (fill missing keys)
    if all_results:
        df = pd.DataFrame(all_results)
    else:
        df = pd.DataFrame(columns=["File Name"])
    return df, bootstrap_data

def create_zip_report(report_df, qq_plots_data=None):
    """Create a ZIP file containing the report CSV and QQ plots."""
    report_file = "f2_similarities_report.csv"
    zip_file = "f2_similarities_report.zip"
    
    report_df.to_csv(report_file, index=False)
    
    with ZipFile(zip_file, "w") as zipf:
        zipf.write(report_file)
        
        # Add QQ plots if available
        if qq_plots_data:
            for file_name, plots in qq_plots_data.items():
                # Create individual plots
                for method, fig in plots['individual'].items():
                    if fig:
                        plot_filename = f"QQ_Plot_{file_name.replace('.xlsx', '')}_{method.replace(' ', '_')}.html"
                        fig.write_html(plot_filename)
                        zipf.write(plot_filename)
                        os.remove(plot_filename)  # Clean up temp file
                
                # Add combined plot
                if plots['combined']:
                    combined_filename = f"QQ_Plot_Combined_{file_name.replace('.xlsx', '')}.html"
                    plots['combined'].write_html(combined_filename)
                    zipf.write(combined_filename)
                    os.remove(combined_filename)  # Clean up temp file
    
    # Clean up temp CSV file
    if os.path.exists(report_file):
        os.remove(report_file)
    
    return zip_file

def generate_qq_plots(bootstrap_data):
    """Generate all QQ plots and return them as a dictionary."""
    qq_plots_data = {}
    
    for file_name, file_bootstrap_data in bootstrap_data.items():
        plots = {
            'individual': {},
            'combined': None
        }
        
        # Generate individual plots
        for method, data in file_bootstrap_data.items():
            if data and len(data) > 0:
                fig = create_jmp_style_qq_plot(np.array(data), f"QQ Plot", method, file_name)
                plots['individual'][method] = fig
        
        # Generate combined plot
        combined_fig = create_combined_qq_plots(file_bootstrap_data, file_name)
        plots['combined'] = combined_fig
        
        qq_plots_data[file_name] = plots
    
    return qq_plots_data

# ----------------------------------------
# Streamlit App Code
# ----------------------------------------

def main():
    st.set_page_config(page_title="Batch Similarity Analyzer (Market Rules)", layout="wide")
    st.title("Batch Similarity Analyzer — Market-aware Recommendations")
    st.markdown("""
    This tool calculates f2 similarity for multiple Excel workbooks at once.
    It includes market-specific CV rules and recommended calculation methods per market.
    """)

    folder_path = st.text_input("Enter path to folder containing Excel files:", "")
    
    # Market selection (single market to apply rules)
    market_options = ['EMA', 'CHINA', 'ASEAN', 'USFDA', 'ANVISA', 'WHO', 'CANADA', 'FDA']
    market = st.selectbox("Select target market (affects CV rules & recommended methods):", market_options, index=0)
    
    # Let the user still pick methods if they don't want recommendations
    options = ["Conventional", "Expected", "Bias Corrected",
               "Conventional Bootstrap", "Expected Bootstrap", "Bias Corrected Bootstrap"]
    selected_methods = st.multiselect("(Optional) Select calculation methods to run (ignored if using recommendations):", options, 
                                    default=["Conventional Bootstrap", "Expected Bootstrap", "Bias Corrected Bootstrap"])
    
    use_recommended = st.checkbox("Use recommended methods for chosen market (per-file, based on CV pass/fail)", value=True)
    
    n_iterations = st.slider("Number of bootstrap iterations:", 1000, 50000, 10000, 1000)
    
    if st.button("Calculate and Generate Report"):
        if os.path.isdir(folder_path):
            with st.spinner("Processing files and generating QQ plots..."):
                workbook_data = load_batch_data(folder_path)
                if workbook_data:
                    report_df, bootstrap_data = process_batch(workbook_data, selected_methods, market, use_recommended, n_iterations)
                    
                    st.subheader("📊 Results Preview")
                    st.dataframe(report_df.head(50))
                    
                    # Generate QQ Plots for bootstrap methods
                    qq_plots_data = None
                    # If any bootstrap data exists, create QQ plots
                    any_bootstrap = any(len(v) > 0 for file_v in bootstrap_data.values() for v in file_v.values())
                    if any_bootstrap:
                        with st.spinner("Generating QQ plots for download..."):
                            qq_plots_data = generate_qq_plots(bootstrap_data)
                            
                            # Show summary of generated plots
                            st.subheader("📈 QQ Plot Generation Summary")
                            st.success("✅ QQ plots have been generated successfully!")
                            
                            total_plots = 0
                            for file_name, plots in qq_plots_data.items():
                                individual_count = len([fig for fig in plots['individual'].values() if fig is not None])
                                combined_count = 1 if plots['combined'] is not None else 0
                                file_total = individual_count + combined_count
                                total_plots += file_total
                                
                                st.write(f"**{file_name}:** {individual_count} individual plots + {combined_count} combined plot = {file_total} plots")
                            
                            st.info(f"📊 **Total QQ plots generated:** {total_plots}")
                            st.markdown("""
                            **QQ Plot Files Included in Download:**
                            - Individual method plots: `QQ_Plot_[FileName]_[Method].html`
                            - Combined comparison plots: `QQ_Plot_Combined_[FileName].html`
                
                            """)
                    
                    # Download report with QQ plots
                    with st.spinner("Creating downloadable ZIP file..."):
                        zip_file_path = create_zip_report(report_df, qq_plots_data)
                    
                    with open(zip_file_path, "rb") as f:
                        file_content = f.read()
                    
                    # Clean up the zip file
                    if os.path.exists(zip_file_path):
                        os.remove(zip_file_path)
                    
                    st.download_button(
                        label="📥 Download Complete Report with QQ Plots (ZIP)",
                        data=file_content,
                        file_name="f2_similarities_report_with_plots.zip",
                        mime="application/zip",
                        help="Download includes CSV report and all QQ plot HTML files"
                    )
                    
                    st.success("Report and QQ plots are ready for download!")
                    
                else:
                    st.warning("No valid Excel files found in selected folder.")
        else:
            st.error("Invalid folder path. Please check the path and try again.")
    
    # Add information section
    with st.expander("ℹ️ About Similarity Analyzer (market & CV behavior)"):
        st.markdown("""
        **CV Rules implemented (from provided spec):**
        - EMA / CHINA / ASEAN / CANADA / WHO: CV ≤ 20% for timepoints up to 10 min; CV ≤ 10% for other timepoints.
        - USFDA / FDA: CV ≤ 20% for early timepoints up to 15 min; CV ≤ 10% for others.
        - ANVISA: CV ≤ 20% for the first 40% of collected points; CV ≤ 10% for the rest.
        
        **Recommended behavior implemented:**
        - If CV criteria PASS:
            - EMA/CHINA/ASEAN/CANADA/WHO → **Expected f2**
            - USFDA / ANVISA → **Conventional f2**
        - If CV criteria FAIL:
            - EMA/CHINA/ASEAN/CANADA/WHO → **Expected Bootstrap**
            - USFDA / ANVISA → **Conventional Bootstrap**
        
        **f2 truncation by 85% rule (implemented):**
        - For FDA & ANVISA: f2 is calculated up to the first timepoint where **both** test and reference mean ≥ 85% (inclusive). For FDA additionally the maximum timepoint used is 15 min (whichever is earlier).
        - For EMA/CHINA/ASEAN: f2 is calculated up to the earliest timepoint where **either** test or reference mean ≥ 85% (inclusive).
        - Otherwise, all timepoints are considered.
        
        These rules are applied per-file (so different files in the folder can get different recommended methods based on their CV pass/fail).
        """)
    
if __name__ == "__main__":
    main()
