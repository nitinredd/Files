import pandas as pd
import numpy as np
from summit.domain import Domain, ContinuousVariable
from typing import List, Dict , Any
from summit.strategies import TSEMO,Random, SNOBFIT
from summit.utils.dataset import DataSet

def HPLC_data_read_csv(file_path: str) -> np.ndarray:
    """
    Reads HPLC data from a CSV file.
    Assumes the CSV has 'Area' and 'RT' columns.
    Returns a NumPy array with [Area, RT].
    """
    try:
        data = pd.read_csv(file_path)
        data_final = pd.DataFrame()
        data_final['Peak_Area'] = data['Area']
        data_final['RT'] = data['RT']
        return data_final.to_numpy()
    except Exception as e:
        print(f"Error reading HPLC CSV file {file_path}: {e}")
        return np.array([]) # Return empty array on error

def impurity_response_csv(data_np: np.ndarray, IminRT: float, ImaxRT: float, areaISO: float) -> float:
    """
    Calculates impurity response based on peak area within a given RT range.
    data_np: NumPy array with [Area, RT]
    IminRT, ImaxRT: Retention time range for the impurity
    areaISO: Isocratic peak area for normalization
    """
    areaB = 0
    for i in range(data_np.shape[0]):
        if IminRT <= data_np[i, 1] <= ImaxRT:
            areaB += data_np[i, 0]
    return areaB / areaISO if areaISO != 0 else 0.0

def response_HPLC_csv(
    data_np: np.ndarray,
    YminRT: float, YmaxRT: float,
    IminRT_list: List[float], ImaxRT_list: List[float],
    minRTISO: float, maxRTISO: float,
    nobj: int
) -> List[float]:
    """
    Computes yield and impurity responses from HPLC data.
    data_np: NumPy array with [Area, RT]
    YminRT, YmaxRT: RT range for the main product (yield)
    IminRT_list, ImaxRT_list: Lists of RT ranges for multiple impurities
    minRTISO, maxRTISO: RT range for the internal standard (isocratic)
    nobj: Number of objectives (including yield and impurities)
    """
    if data_np.size == 0:
        # Return default values if data is empty
        return [float('inf')] * nobj

    areaA = 0
    for i in range(data_np.shape[0]):
        if YminRT <= data_np[i, 1] <= YmaxRT:
            areaA += data_np[i, 0]

    areaISO = 0
    for i in range(data_np.shape[0]):
        if minRTISO <= data_np[i, 1] <= maxRTISO:
            areaISO += data_np[i, 0]

    response = []
    # Yield calculation (negative log for minimization)
    yield_result = areaA / areaISO if areaISO != 0 else 0.0
    response.append(-np.log(yield_result) if yield_result > 0 else float('inf')) # Handle log(0)

    # Impurities calculation
    # nobj includes yield, so iterate for nobj-1 impurities
    for i in range(nobj - 1):
        if i < len(IminRT_list) and i < len(ImaxRT_list):
            impurities_result = impurity_response_csv(data_np, IminRT_list[i], ImaxRT_list[i], areaISO)
            response.append(impurities_result)
        else:
            response.append(0.0) # Default if impurity range is missing

    return response

def monitor_folder_creation1_csv(
    file_path: str,
    nobj: List,
    YminRT: float, YmaxRT: float,
    IminRT: float, ImaxRT: float,
    minRTISO: float, maxRTISO: float
) -> Dict[str, Dict[str, List[float]]]:
    """
    Processes a single HPLC CSV file and returns the calculated responses.
    This function wraps the HPLC data reading and response calculation.
    filepath = new file path (excel file path)
    nobj = Objective data objectives names 
    """
    data = HPLC_data_read_csv(file_path)
    response = response_HPLC_csv(data,YminRT,YmaxRT,IminRT,ImaxRT,minRTISO,maxRTISO,nobj)
    return response

def process_hplc_and_fill(filename: str, data_np: np.ndarray,lhs: pd.DataFrame,objectives: list,nobj,hplc_params: dict):
    """Process HPLC data and update results."""
    nobj = max(1, len(objectives))
    # Calculate response
    resp = response_HPLC_csv(
        data_np,
        hplc_params.get("YminRT", 2.0),
        hplc_params.get("YmaxRT", 4.0),
        hplc_params.get("IminRT_list", [0.5]),
        hplc_params.get("ImaxRT_list", [1.0]),
        hplc_params.get("minRTISO", 10.0),
        hplc_params.get("maxRTISO", 12.0),
        nobj
    )

    return resp

def process_uploaded_csv_file(full_path_uploaded_csv,
                              df,
                              minRTISO,
                              maxRTISO,
                              YminRT,
                              YmaxRT,
                              IminRT_list,
                              ImaxRT_list,
                              transformed_objectives,
                              lhs ):
    if 'Area' in df.columns and 'RT' in df.columns:
            data_np = df[['Area', 'RT']].to_numpy()
    else:
        cols_lower = {c.lower(): c for c in df.columns}
        if 'area' in cols_lower and 'rt' in cols_lower:
            data_np = df[[cols_lower['area'], cols_lower['rt']]].to_numpy()
    hplc_params = {}
    if YminRT is not None: hplc_params['YminRT'] = YminRT
    if YmaxRT is not None: hplc_params['YmaxRT'] = YmaxRT
    if IminRT_list is not None:
        hplc_params['IminRT_list'] = [float(x) for x in IminRT_list if str(x).strip() != '']

    if ImaxRT_list is not None:
        hplc_params['ImaxRT_list'] = [float(x) for x in ImaxRT_list if str(x).strip() != '']

    if minRTISO is not None: hplc_params['minRTISO'] = float(minRTISO)
    if maxRTISO is not None: hplc_params['maxRTISO'] = float(maxRTISO)

    resp = process_hplc_and_fill(filename = full_path_uploaded_csv, 
                                 data_np = data_np,
                                 lhs = lhs,
                                 objectives = transformed_objectives,
                                 nobj = len(transformed_objectives),
                                 hplc_params = hplc_params)
    return resp

def build_domain_from_df(df: pd.DataFrame, objectives: List[Dict[str, Any]]):
    """Build optimization domain from DataFrame."""
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    domain = Domain()
    if len(numeric_cols) == 0:
        raise ValueError("No numeric columns to build domain.")

    # Get objective column names to exclude them from input variables
    objective_names = [obj.get("name", "").replace(" ", "_") for obj in objectives]
    
    # Add ALL numeric columns as input variables (except objectives)
    for col in numeric_cols:
        sanitized_name = str(col).replace(" ", "_")  # Replace spaces with underscores
        
        # Skip if this column is an objective
        if sanitized_name in objective_names:
            continue
            
        # Calculate bounds from existing data
        col_data = df[col].dropna()  # Remove NaN values for bound calculation
        if len(col_data) > 0:
            lb = float(col_data.min())
            ub = float(col_data.max())
            if lb == ub:
                lb -= 1e-6
                ub += 1e-6
        else:
            # Default bounds if no data available
            lb, ub = 0.0, 1.0
            
        domain += ContinuousVariable(name=sanitized_name, description=str(col), bounds=[lb, ub])

    # Add objectives
    for obj in objectives:
        obj_name = obj.get("name", "obj").replace(" ", "_")  # Sanitize objective name
        maximize = bool(obj.get("maximize", False))
        domain += ContinuousVariable(
            name=obj_name,
            description=obj_name,
            bounds=[0, 100],
            is_objective=True,
            maximize=maximize
        )
    return domain

def clean_dataframe_for_summit(df: pd.DataFrame) -> pd.DataFrame:
    """Clean DataFrame to avoid Summit/TSEMO column overlap issues."""
    df_clean = df.copy()
    
    # Handle MultiIndex columns
    if isinstance(df_clean.columns, pd.MultiIndex):
        new_columns = []
        for col in df_clean.columns:
            if isinstance(col, tuple):
                # Join tuple elements, filter out 'DATA' and empty strings
                parts = [str(x) for x in col if str(x).upper() != 'DATA' and str(x).strip()]
                if parts:
                    col_name = "_".join(parts)
                else:
                    col_name = f"col_{len(new_columns)}"
            else:
                col_name = str(col)
            
            # Remove special characters that might cause issues
            col_name = col_name.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')
            
            # Ensure uniqueness
            counter = 1
            original_name = col_name
            while col_name in new_columns:
                col_name = f"{original_name}_{counter}"
                counter += 1
            new_columns.append(col_name)
        
        df_clean.columns = new_columns
    else:
        # Clean regular column names
        new_columns = []
        for col in df_clean.columns:
            col_name = str(col).replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')
            # Ensure uniqueness
            counter = 1
            original_name = col_name
            while col_name in new_columns:
                col_name = f"{original_name}_{counter}"
                counter += 1
            new_columns.append(col_name)
        df_clean.columns = new_columns
    
    return df_clean

def run_summit_optimization(domain, lhs_df: pd.DataFrame, nobj: int):
    """Run Summit optimization."""
    
    # Clean the DataFrame first
    lhs_df_clean = clean_dataframe_for_summit(lhs_df)
    
    if nobj > 1:
        try:
            strat = TSEMO(domain, random_rate=0.00, n_spectral_points=4000)
            lhs_ds = DataSet.from_df(lhs_df_clean)
            out = strat.suggest_experiments(1, lhs_ds, use_spectral_sample=True, pop_size=100, iterations=100)
        except Exception as e:
            print(f"TSEMO failed with error: {e}")
            print("Falling back to SNOBFIT...")
            # Fallback to SNOBFIT if TSEMO fails
            strat = SNOBFIT(domain)
            lhs_ds = DataSet.from_df(lhs_df_clean)
            out = strat.suggest_experiments(1, lhs_ds)
    else:
        strat = SNOBFIT(domain)
        lhs_ds = DataSet.from_df(lhs_df_clean)
        out = strat.suggest_experiments(1, lhs_ds)
    
    # Clean up output column names
    try:
        if isinstance(out.columns, pd.MultiIndex):
            out.columns = [col[0] if isinstance(col, tuple) else str(col) for col in out.columns]
        else:
            out.columns = [str(col) for col in out.columns]
    except Exception:
        pass
    
    # Remove strategy column if present
    if "strategy" in out.columns:
        out = out.drop(columns=["strategy"])
    
    # Map back to original column structure
    original_columns = lhs_df.columns
    cleaned_columns = lhs_df_clean.columns
    
    # Create mapping from cleaned names back to original names
    if len(original_columns) == len(cleaned_columns):
        column_mapping = dict(zip(cleaned_columns, original_columns))
        
        # Get all input variable columns (non-objective columns from output)
        output_cols = [c for c in out.columns if c != "strategy"]
        mapped_cols = []
        
        for col in output_cols:
            if col in column_mapping:
                mapped_cols.append(column_mapping[col])
            elif col in original_columns:
                mapped_cols.append(col)
        
        if mapped_cols:
            # Reindex output to match original DataFrame column structure
            out_reindexed = pd.DataFrame(index=out.index)
            for orig_col in original_columns:
                if orig_col in mapped_cols:
                    # Find the corresponding output column
                    reverse_mapping = {v: k for k, v in column_mapping.items()}
                    clean_col = reverse_mapping.get(orig_col, orig_col)
                    if clean_col in out.columns:
                        out_reindexed[orig_col] = out[clean_col]
                    else:
                        out_reindexed[orig_col] = np.nan
                else:
                    out_reindexed[orig_col] = np.nan
            out = out_reindexed
    else:
        # Fallback: create DataFrame with original column structure
        out_reindexed = pd.DataFrame(index=out.index)
        for orig_col in original_columns:
            if isinstance(orig_col, tuple):
                # Handle MultiIndex columns
                clean_col = "_".join(str(x) for x in orig_col if str(x).upper() != 'DATA' and str(x).strip())
                clean_col = clean_col.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')
            else:
                clean_col = str(orig_col).replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')
            
            if clean_col in out.columns:
                out_reindexed[orig_col] = out[clean_col]
            else:
                out_reindexed[orig_col] = np.nan
        out = out_reindexed
    
    return out

def suggest_experiments_and_append(num_suggestions: int, objectives: List[Dict[str, Any]],lhs):
    try:
        print("In Domain")
        domain = build_domain_from_df(lhs, objectives)
        print("Out Domain")
    except Exception as e:
        raise RuntimeError(f"Domain build failed: {e}")
    suggestions = []
    for _ in range(num_suggestions):
        print("in run optimization")
        out = run_summit_optimization(domain, lhs, len(objectives) or 1)
        print("out run optimization")
        if isinstance(out, pd.DataFrame) and out.shape[0] >= 1:
            suggested = out.iloc[0].to_dict()
        else:
            suggested = {}

        new_row = {c: (np.nan if c not in suggested else suggested[c]) for c in lhs.columns}
        suggestions.append(new_row)
        lhs = pd.concat([lhs, pd.DataFrame([new_row])], ignore_index=True)
    return lhs
