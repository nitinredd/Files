# reaction_database_app.py

import os
import io
import json
import faiss
import torch
import clip
import camelot
import tabula
import numpy as np
import pandas as pd
import streamlit as st
from PIL import Image
from PyPDF2 import PdfReader
import fitz  # PyMuPDF

from langchain_openai import AzureChatOpenAI
from langchain_openai import AzureOpenAIEmbeddings
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain.schema import Document, HumanMessage, SystemMessage

# --- STREAMLIT CONFIG -------------------------------------------------------

st.set_page_config(
    page_title="ðŸ”¬ Reaction Database AI", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- AZURE & MODEL SETTINGS ------------------------------------------------

AZURE_ENDPOINT      = os.getenv("AZURE_ENDPOINT", "<YOUR_AZURE_ENDPOINT>")
AZURE_API_KEY       = os.getenv("AZURE_API_KEY", "<YOUR_API_KEY>")
CHAT_DEPLOYMENT     = "GPT4o"
EMBED_DEPLOYMENT    = "Def_data_qa"
EMBED_MODEL         = "text-embedding-ada-002"
PDF_TEXT_CHUNK_SIZE = 5000     # chars per embedding
TOP_K_TEXT          = 5
TOP_K_IMAGES        = 3

# --- YOUR REACTION FOLDERS --------------------------------------------------

REACTION_FOLDERS = {
    "C-N Bond Formation": r"/absolute/path/to/C-N_Bond_Formation",
    # Add other classes here, e.g.:
    # "Oxidation Reactions": r"/path/to/Oxidation",
}

# --- LOAD & CACHE MODELS ----------------------------------------------------

@st.cache_resource(show_spinner=False)
def load_clip_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)
    return model, preprocess, device

clip_model, clip_preprocess, clip_device = load_clip_model()

@st.cache_resource(show_spinner=False)
def get_text_embedder():
    store = LocalFileStore("langchain-embeddings")
    base  = AzureOpenAIEmbeddings(
        model=EMBED_MODEL,
        api_version="2023-07-01-preview",
        azure_endpoint=AZURE_ENDPOINT,
        api_key=AZURE_API_KEY,
        azure_deployment=EMBED_DEPLOYMENT
    )
    return CacheBackedEmbeddings.from_bytes_store(base, store, namespace=base.model)

text_embedder = get_text_embedder()

@st.cache_resource(show_spinner=False)
def get_chat_model():
    return AzureChatOpenAI(
        azure_deployment=CHAT_DEPLOYMENT,
        model="GPT4o",
        api_version="2024-02-15-preview",
        api_key=AZURE_API_KEY,
        azure_endpoint=AZURE_ENDPOINT
    )

chat_model = get_chat_model()

# --- UTILITIES --------------------------------------------------------------

def chunk_text(text: str, max_chars: int):
    """Split a long string into chunks of â‰¤ max_chars."""
    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]

def extract_tables_from_pdf(pdf_path: str):
    """Extract tables via Camelot and Tabula, return list of DataFrames."""
    tables = []
    try:
        tables += [t.df for t in camelot.read_pdf(pdf_path, pages="all", flavor="stream")]
    except Exception:
        pass
    try:
        tables += tabula.read_pdf(pdf_path, pages="all", multiple_tables=True)
    except Exception:
        pass
    return tables

def extract_images_and_schemes(pdf_path: str):
    """
    Return list of dicts:
      - {"type":"image", "page":int, "img":PIL.Image, "emb":np.ndarray}
      - {"type":"caption", "page":int}
    """
    out = []
    doc = fitz.open(pdf_path)
    for p in range(len(doc)):
        blocks = doc[p].get_text("blocks")
        # detect any "Scheme" captions
        for b in blocks:
            if "scheme" in b[4].lower():
                out.append({"type":"caption", "page": p+1})
                break
        # extract all images
        for img_idx, img_obj in enumerate(doc[p].get_images(full=True)):
            xref = img_obj[0]
            img_bytes = doc.extract_image(xref)["image"]
            img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
            inp = clip_preprocess(img).unsqueeze(0).to(clip_device)
            with torch.no_grad():
                emb = clip_model.encode_image(inp).cpu().numpy().reshape(-1)
            out.append({"type":"image", "page":p+1, "img":img, "emb":emb})
    return out

def build_faiss_index(embs: np.ndarray):
    """Create an L2 FAISS index from (N, D) numpy array."""
    dim = embs.shape[1]
    idx = faiss.IndexFlatL2(dim)
    idx.add(embs.astype("float32"))
    return idx

# --- BUILD INDICES (CACHED) -----------------------------------------------

@st.cache_data(show_spinner=True)
def initialize_indices():
    text_indices  = {}
    text_docs     = {}
    image_indices = {}
    image_meta    = {}

    for cls, folder in REACTION_FOLDERS.items():
        txt_embs, docs = [], []
        img_embs, meta  = [], []

        for fname in sorted(os.listdir(folder)):
            if not fname.lower().endswith(".pdf"):
                continue
            path = os.path.join(folder, fname)

            # --- TEXT & TABLE EXTRACTION ---
            reader = PdfReader(path)
            raw = "\n".join(p.extract_text() or "" for p in reader.pages)
            chunks = chunk_text(raw, PDF_TEXT_CHUNK_SIZE)
            docs.extend([Document(page_content=c, metadata={"source":fname, "chunk":i})
                         for i,c in enumerate(chunks)])
            txt_embs.extend(text_embedder.embed_documents(chunks))

            # --- TABLES ---
            for tbl in extract_tables_from_pdf(path):
                docs.append(Document(
                    page_content=tbl.to_csv(index=False),
                    metadata={"source":fname, "chunk":"table"}
                ))
                txt_embs.append(text_embedder.embed_documents([tbl.to_csv(index=False)])[0])

            # --- IMAGES & SCHEMES ---
            for item in extract_images_and_schemes(path):
                if item["type"] == "image":
                    img_embs.append(item["emb"])
                    meta.append({"source":fname, "page":item["page"], "type":"image"})
                else:
                    # caption placeholder: zero vector
                    emb_dim = img_embs[0].shape[0] if img_embs else 512
                    img_embs.append(np.zeros(emb_dim))
                    meta.append({"source":fname, "page":item["page"], "type":"caption"})

        # build and store
        text_indices[cls]  = build_faiss_index(np.vstack(txt_embs))
        text_docs[cls]     = docs
        image_indices[cls] = build_faiss_index(np.vstack(img_embs))
        image_meta[cls]    = meta

    return text_indices, text_docs, image_indices, image_meta

text_idxs, text_ds, img_idxs, img_meta = initialize_indices()

# --- QUERY FUNCTIONS --------------------------------------------------------

def retrieve_text_chunks(cls: str, q: str, k: int):
    q_emb = text_embedder.embed_query(q).reshape(1,-1).astype("float32")
    D, I = text_idxs[cls].search(q_emb, k)
    return [ text_ds[cls][i] for i in I[0] ]

def retrieve_image_hits(cls: str, q: str, k: int):
    tok = clip.tokenize([q]).to(clip_device)
    with torch.no_grad():
        q_emb = clip_model.encode_text(tok).cpu().numpy().reshape(1,-1)
    D, I = img_idxs[cls].search(q_emb.astype("float32"), k)
    return [ img_meta[cls][i] for i in I[0] ]

# --- STREAMLIT UI -----------------------------------------------------------

st.sidebar.title("âš™ï¸ Settings")
st.sidebar.markdown("Configure your query and model behavior here.")

reaction_class = st.sidebar.selectbox("Reaction Class", list(REACTION_FOLDERS.keys()))
user_query     = st.text_input("ðŸ”Ž Ask about this reaction:")

if user_query:
    st.header("ðŸ“ Retrieved Procedure, Yield & Tables")
    chunks = retrieve_text_chunks(reaction_class, user_query, TOP_K_TEXT)
    for doc in chunks:
        st.markdown(f"**Source:** {doc.metadata['source']} â€” Chunk `{doc.metadata['chunk']}`")
        # detect CSV tables
        if doc.page_content.lstrip().startswith(("sep=", "col1")) or "\n" in doc.page_content and doc.page_content.count(",")>3:
            df = pd.read_csv(io.StringIO(doc.page_content))
            st.dataframe(df)
        else:
            st.text_area("", doc.page_content, height=200)

    st.header("ðŸ”Ž Retrieved Schemes & Figures")
    hits = retrieve_image_hits(reaction_class, user_query, TOP_K_IMAGES)
    for m in hits:
        if m["type"] == "caption":
            st.markdown(f"*ðŸ“‘ Synthetic Scheme caption detected on page {m['page']} of {m['source']}*")
        else:
            # reopen PDF and display the actual image
            pdf = fitz.open(os.path.join(REACTION_FOLDERS[reaction_class], m["source"]))
            img_obj = pdf[m["page"]-1].get_images(full=True)[0]
            xref = img_obj[0]
            img = Image.open(io.BytesIO(pdf.extract_image(xref)["image"]))
            st.image(img, caption=f"{m['source']} â€” page {m['page']}", use_column_width=True)

    st.header("ðŸ¤– AI Summary & Correlation")
    system_msg = SystemMessage(content="You are a world-class synthetic chemist. Summarize the retrieved procedures, yields, tables, and schemes, and explain how they relate.")
    human_msg  = HumanMessage(content=user_query)
    answer     = chat_model([system_msg, human_msg])
    st.markdown(answer.content)
