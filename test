import numpy as np
import pandas as pd
import numpy as np
from scipy.stats import norm
from sklearn.utils import resample
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import rpy2
import rpy2.robjects as robjects
from rpy2.robjects.packages import importr, data
import pandas as pd
import numpy as np
import pandas as pd
import warnings
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
utils = importr('utils')
base = importr('base')
bootf2 = importr('bootf2')
read = importr('readxl')
open = importr('writexl')
warnings.filterwarnings('ignore')

file_path=r"C:\Users\p00095189\Desktop\WORK\Formulations\Similarity_Analyzer\SIMILARITY_ANALYZER\SIMILARITY_ANALYZER\Phase3_Validation\70-80\70-80_Test2_vs_ref4.xlsx"
reference_df =pd.read_excel(file_path, sheet_name=0)
test_df=pd.read_excel(file_path, sheet_name=1)
arrayboot=[]

np.random.seed(306)


def dissolution_curve_interval(reference_df, test_df):
    # Print dataframes (optional for debugging)
    #print(reference_df)
    #print(test_df)

    # Calculate means, maxs, and mins for reference data
    ref_data_means = pd.DataFrame(reference_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Reference'])
    ref_data_max = pd.DataFrame(reference_df.iloc[:, 1:].max(axis=1), columns=['Max_Reference'])
    ref_data_min = pd.DataFrame(reference_df.iloc[:, 1:].min(axis=1), columns=['Min_Reference'])

    # Calculate means, maxs, and mins for test data
    test_data_means = pd.DataFrame(test_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Test'])
    test_data_max = pd.DataFrame(test_df.iloc[:, 1:].max(axis=1), columns=['Max_Test'])
    test_data_min = pd.DataFrame(test_df.iloc[:, 1:].min(axis=1), columns=['Min_Test'])

    # Add Time column
    ref_data_means.insert(0, 'Time', reference_df.iloc[:, 0])
    ref_data_max.insert(0, 'Time', reference_df.iloc[:, 0])
    ref_data_min.insert(0, 'Time', reference_df.iloc[:, 0])

    test_data_means.insert(0, 'Time', test_df.iloc[:, 0])
    test_data_max.insert(0, 'Time', test_df.iloc[:, 0])
    test_data_min.insert(0, 'Time', test_df.iloc[:, 0])

    # Create a figure and axis object
    plt.figure(figsize=(12, 6))

    # Plotting reference data with error bars
    plt.errorbar(ref_data_means['Time'], ref_data_means['Mean_Reference'],
                 yerr=[ref_data_means['Mean_Reference'] - ref_data_min['Min_Reference'],
                       ref_data_max['Max_Reference'] - ref_data_means['Mean_Reference']],
                 fmt='o', label='Reference Mean', color='blue', linestyle='-')

    # Plotting test data with error bars
    plt.errorbar(test_data_means['Time'], test_data_means['Mean_Test'],
                 yerr=[test_data_means['Mean_Test'] - test_data_min['Min_Test'],
                       test_data_max['Max_Test'] - test_data_means['Mean_Test']],
                 fmt='o', label='Test Mean', color='green', linestyle='--')

    # Adding horizontal segments for min and max values for reference data
    for time, min_val, max_val in zip(ref_data_means['Time'], ref_data_min['Min_Reference'], ref_data_max['Max_Reference']):
        plt.hlines(min_val, time - 0.2, time + 0.2, colors='blue', linestyles='--', alpha=0.5)
        plt.hlines(max_val, time - 0.2, time + 0.2, colors='blue', linestyles='--', alpha=0.5)

    # Adding horizontal segments for min and max values for test data
    for time, min_val, max_val in zip(test_data_means['Time'], test_data_min['Min_Test'], test_data_max['Max_Test']):
        plt.hlines(min_val, time - 0.2, time + 0.2, colors='green', linestyles='--', alpha=0.5)
        plt.hlines(max_val, time - 0.2, time + 0.2, colors='green', linestyles='--', alpha=0.5)

    # Add labels and title
    plt.xlabel('Time')
    plt.ylabel('Dissolution (%)')
    plt.title('Dissolution Curves with Intervals')
    plt.grid(True)
    plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
    plt.gca().set_yticks(range(0, 101, 5))
    plt.legend(loc='lower right')
    
    # Save the plot as an image file
    plt.savefig('dissolution_curves_with_intervals.png', format='png', dpi=300)

    # Optionally, display the plot (uncomment if you want to see it as well)
    # plt.show()

    # Close the plot to free up memory
    plt.close()


def dissolution_curve(reference_df,test_df):
    ref_data_means=pd.DataFrame(reference_df.iloc[:,1:].mean(axis=1),columns=['Mean_Reference'])
    test_data_means=pd.DataFrame(test_df.iloc[:,1:].mean(axis=1),columns=['Mean_Test'])


    ref_data_means.insert(0, 'Time', reference_df.iloc[:, 0])
    test_data_means.insert(0, 'Time', test_df.iloc[:, 0])



    # Create a figure and axis object
    plt.figure(figsize=(12, 6))

    # Plotting reference data
    for column in ref_data_means.columns[1:]:  # Skip the first column which is 'Time'
        plt.plot(ref_data_means.iloc[:, 0], ref_data_means[column], label=f'Reference {column}',marker='o')

    # Plotting test data
    for column in test_data_means.columns[1:]:  # Skip the first column which is 'Time'
        plt.plot(test_data_means.iloc[:, 0], test_data_means[column], label=f'Test {column}', marker='o',linestyle='--')

    # Add labels and title
    plt.xlabel('Time')
    plt.ylabel('Dissolution (%)')
    plt.title('Dissolution Curves')
    plt.grid(True)
    plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
    plt.gca().yaxis.set_major_locator(MaxNLocator(nbins='auto', integer=True))
    plt.gca().set_yticks(range(0, 101, 5))
    plt.legend(loc='lower right')
    # Save the plot as an image file
    plt.savefig('dissolution_curves.png', format='png', dpi=300)

    # Optionally, display the plot (uncomment if you want to see it as well)
    # plt.show()

    # Close the plot to free up memory
    plt.close()

# Check 1: A minimum of three-time points (time zero excluded) is considered for both products
def check_time_points(df):

    if df.iloc[0, 0] == 0 or df.iloc[0, 0]=='0':  # Check if the first time point is zero
        return len(df.iloc[:,0].values)-1 >=3 # Exclude the zero time point 
    else:
        return len(df.iloc[:,0].values) >=3 
    
# Check 2: 1,2 time points > 85%? If > 85%, No f2 calculations
def two_time_points(df):
    mean_values = df.iloc[:, 1:].mean(axis=1)  # Exclude the Time Points column
    if mean_values[0]>85 or mean_values[1]>85:
         return False
    else:
         return True

#Check 4: If in Min check if in 15 min > 85% the no f2 required
def min15__check(df):
     time_string=df.columns[0]
     mean_values = df.iloc[:, 1:].mean(axis=1) 
     
     if "min" in time_string.lower() or "minutes" in time_string.lower():
          condition = df.iloc[:, 0] <= 15
          indices = df.index[condition]
          filtered_df = mean_values.loc[indices]
          condition_other_columns = filtered_df > 85
          final_indices = filtered_df.index[condition_other_columns].tolist()
          if len(final_indices)>0:
               return True
          else:
               return False
     else:
          return False
          
# The coefficient of variation (CV) of both product should be less than 20% at the first (non-zero) time point and less than 10% at the following time points
def check_cv(df):
    if df.iloc[0, 0] == 0:  # Check if the first time point is zero
        cv_values = df.iloc[1:, 1:].std(axis=1) / df.iloc[1:, 1:].mean(axis=1) * 100  # Exclude the zero time point and Time Points column
        return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()
    else:
        cv_values = df.iloc[:, 1:].std(axis=1) / df.iloc[:, 1:].mean(axis=1) * 100 # Calculate CV for all time points
        #print(df)
        #print(cv_values)
        return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()



def row_variance(df):
    return df.iloc[:,1:].var(axis=1,ddof=1)

def stand(p,sum_diff_df_sqr):
    f2_v1 = 100 - 25 * np.log10((1 + (1 / p) * sum_diff_df_sqr))  # correct F2_DRL
    return f2_v1

def expected(p,sum_diff_df_sqr,left_side):
    f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) + (left_side)))))
    return f2
    
def BiasCor(p,sum_diff_df_sqr,left_side):
    # f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
    # print("Bias corrected f2", f2)
    Right_side=sum_diff_df_sqr+p
    if left_side >= Right_side:
        f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
        return round(f2,3)
    else:
        f2="Baised Corrected f2 can not be calculated"
        return f2
    return f2



def f2s(ref_data,test_data):   
    ref_data_means=ref_data.iloc[:,1:].mean(axis=1)
    test_data_means=test_data.iloc[:,1:].mean(axis=1)
    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    diff_df=test_data_df-ref_data_df
    diff_df_sqr=diff_df**2
    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    ref_data_var_df=pd.DataFrame(row_variance(ref_data),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(test_data),columns=['Unbiased Variance'])
    addition_df=test_data_var_df+ref_data_var_df
    sum_addition_df=addition_df['Unbiased Variance'].sum()
    n_r,n_c=ref_data.shape
    n=n_c-1
    p=len(ref_data.iloc[:,0])
    left_side=(1/n)*(sum_addition_df)
    f2=stand(p,sum_diff_df_sqr)
    print("Conventional f2 :",round(f2,3))
    expf2=expected(p,sum_diff_df_sqr,left_side)
    print("Expected f2     :", round(expf2,3))
    BiCf2=BiasCor(p,sum_diff_df_sqr,left_side)
    print("BiasCorrected f2:", BiCf2)



def changed_data_either85_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)



def changed_data_both85_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    # print("mean ref",mean_values_reference)
    # print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    # print(ref_index)
    # print(test_index)

    if ref_index>test_index:
        final_index=ref_index
    elif test_index>ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    # print(final_index)
    # print(final_index+1)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)

def changed_data_either85_FDA_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)




def cal_f2(resampled_test,resampled_reference):
    #print(resampled_reference.iloc[:,1:])
    ref_data_means=resampled_reference.iloc[:,1:].mean(axis=1)
    test_data_means=resampled_test.iloc[:,1:].mean(axis=1)

    #print(ref_data_means)
    #print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    #print(ref_data_df)
    #print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)

    p=len(resampled_reference.iloc[:,0])
    #print('Time points',p)

    f2_v1 = 100 - 25 * np.log10((1 + (1 / p) * sum_diff_df_sqr))  # correct F2_DRL
    #print("f2 score is: ",f2_v1)
    arrayboot.append(f2_v1)
    return f2_v1

def bca(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=cal_f2(test_data,ref_data)
    n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_f2(dft,dfr)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for f2: [{lower_percentile}, {upper_percentile}]")

def bca_exp_f2(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=cal_exp_f2(test_data,ref_data)
    n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_exp_f2(dft,dfr)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for expected f2: [{lower_percentile}, {upper_percentile}]")

def plots(arrayboot,plot):
    plt.figure(figsize=(10, 6))
    sns.histplot(arrayboot, kde=True, bins=10, color='blue')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    tiltename=f"histogram_plot_{plot}"
    plt.title(tiltename)
    filename=f"histogram_plot_{plot}.png"
    plt.savefig(filename)  # Save the plot as a PNG file
    plt.close()  # Close the figure to avoid displaying it inline
    plt.figure(figsize=(8, 8))
    stats.probplot(arrayboot, dist="norm", plot=plt)
    tiltename=f"qq_plot_{plot}"
    plt.title(tiltename)
    filename=f"qq_plot_{plot}.png"
    plt.savefig(filename)  # Save the plot as a PNG file
    plt.close()  # Close the figure to avoid displaying it inline


def changed_data_either85_bca(reference_df,test_df):
    arrayboot.clear()
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index
    
    if final_index==0:
        final_index=change_reference_df.index[-1]
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("f2 Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    #print("Len1",len(arrayboot))
    plot="f2"
    plots(arrayboot,plot)

    print(" ")
    bca(change_test_df,change_reference_df)
    #print("Len2",len(arrayboot))
    plot="f2_bca"
    plots(arrayboot,plot)


    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    #print("Len3",len(arrayboot))
    plot="expected_f2"
    plots(arrayboot,plot)

    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    #print("Len4",len(arrayboot))
    plot="expected_f2_bca"
    plots(arrayboot,plot)

    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Baised Corrected f2 can not be calculated")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        #print("Len5",len(arrayboot))
        plot="baiscorrected_f2"
        plots(arrayboot,plot)

        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)
        #print("Len6",len(arrayboot))
        plot="baiscorrected_f2_bca"
        plots(arrayboot,plot)



def changed_data_both85_bca(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    # print(ref_index)
    # print(test_index)

    if ref_index>test_index:
        final_index=ref_index
    elif test_index>ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    print(" ")
    bca(change_test_df,change_reference_df)
    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Cannot caclulate f2")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)


def changed_data_either85_FDA_bca(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    print(" ")
    bca(change_test_df,change_reference_df)
    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Cannot caclulate f2")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)




def f2_bootstral_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_f2(dft,dfr)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

# Check 3: The time points at which the dissolutions are measured are the same for both products
def check_same_time_points(df1, df2):
    return df1.iloc[:,0].equals(df2.iloc[:,0])

# Check 4: At least 12 individual dosage units are used for both products
def check_sample_units(df):
    return df.shape[1] - 1 >= 12  # Exclude the Time Points column


def cal_exp_f2(resampled_test,resampled_reference):
    ref_data_means=resampled_reference.iloc[:,1:].mean(axis=1)
    test_data_means=resampled_test.iloc[:,1:].mean(axis=1)

    #print(ref_data_means)
    #print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    #print(ref_data_df)
    #print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)

    ref_data_var_df=pd.DataFrame(row_variance(resampled_reference),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(resampled_test),columns=['Unbiased Variance'])

    #print(ref_data_var_df)
    #print(test_data_var_df)

    addition_df=test_data_var_df+ref_data_var_df
    #print(addition_df)

    sum_addition_df=addition_df['Unbiased Variance'].sum()
    #print(sum_addition_df)

    n_r,n_c=resampled_test.shape
    n=n_c-1
    #print(n)

    p=len(resampled_test.iloc[:,0])
    #print('Time points',p)

    left_side=(1/n)*(sum_addition_df)
    #print(left_side)

    f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) + (left_side)))))
    #print("Expected f2", f2)
    arrayboot.append(f2)
    return f2

def f2_bootstra_expf2_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_exp_f2(dft,dfr)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

def biascorrectedf2(ref_data,test_data):
    ref_data_means=ref_data.iloc[:,1:].mean(axis=1)
    test_data_means=test_data.iloc[:,1:].mean(axis=1)

    # print(ref_data_means)
    # print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    # print(ref_data_df)
    # print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)


    ref_data_var_df=pd.DataFrame(row_variance(ref_data),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(test_data),columns=['Unbiased Variance'])

    # print(ref_data_var_df)
    # print(test_data_var_df)

    addition_df=test_data_var_df+ref_data_var_df
    # print(addition_df)

    sum_addition_df=addition_df['Unbiased Variance'].sum()
    # print(sum_addition_df)

    n_r,n_c=ref_data.shape
    n=n_c-1
    # print(n)

    p=len(ref_data.iloc[:,0])
    # print('Time points',p)

    left_side=(1/n)*(sum_addition_df)
    # print(left_side)

    # f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
    # print("Bias corrected f2", f2)
    Right_side=sum_diff_df_sqr+p

    if left_side >= Right_side:
        with warnings.catch_warnings():
            warnings.simplefilter("error", RuntimeWarning)
            try:
                f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
            except RuntimeWarning as e:
                print(f"Warning occurred: {e}")
                f2=0
                return f2
        #print("Baised Corrected f2", f2)
        return f2
    else:
        f2=0
        #print("No f2",f2)
        return f2

def f2_bootstratp_bias_f2_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=biascorrectedf2(dfr,dft)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

def bca_bias_f2(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=f2_bootstratp_bias_f2_tile(ref_data,test_data)
    n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=f2_bootstratp_bias_f2_tile(dfr,dft)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for expected f2: [{lower_percentile}, {upper_percentile}]")


def R_Regulation(file_path):
    dr=read.read_excel(file_path,sheet='Sheet1')
    dt=read.read_excel(file_path,sheet='Sheet2')
    print("Enter Name of Regulation either EMA, FDA, WHO, Canada, ANVISA:")
    reg=input()
    b=bootf2.bootf2(dt,dr,file_out='test',regulation=reg)
    print(b.rx2('boot.summary'))
    print("Please check file created for full report")

def R_all_Regulation(file_path):
    dr=read.read_excel(file_path,sheet='Sheet1')
    dt=read.read_excel(file_path,sheet='Sheet2')
    b=bootf2.bootf2(dt,dr,file_out='EMA_Results',regulation="EMA")
    print("EMA Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='FDA_Results',regulation="FDA")
    print("FDA Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='WHO_Results',regulation="WHO")
    print("WHO Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='Canada_Results',regulation="Canada")
    print("Canada Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='ANVISA_Results',regulation="ANVISA")
    print("ANVISA Summary:")
    print(b.rx2('boot.summary'))
    print("Please check files created for full report")


if check_time_points(test_df) and check_time_points(reference_df):
    print("Check 1: A minimum of three-time points - PASSED")
else:
    print("Check 1: A minimum of three-time points - FAILED, NO f2 Calculated")

if two_time_points(test_df) and two_time_points(reference_df):
    print("Check 2: 1,2 time points less than 85 percent dissoultion - PASSED")
else:
    print("Check 2: 1,2 time points less than 85 percent dissoultion- FAILED, NO f2 Calculated")

if min15__check(test_df) and min15__check(reference_df):
    print("Check 3: In 15 min their is greater than 85 percent dissolution - PASSED, NO f2 Calculated")
else:
    print("Check 4: In 15 min their is No greater than 85 percent dissolution - FAILED")

if check_same_time_points(test_df, reference_df):
    print("Check 4: The time points same for both products - PASSED")
else:
    print("Check 4: The time points same for both products - FAILED")

if check_sample_units(test_df) and check_sample_units(reference_df):
    print("Check 5: At least 12 individual sample units - PASSED")
else:
    print("Check 5: At least 12 individual sample units - FAILED")



print(" ")
dissolution_curve(reference_df,test_df)
dissolution_curve_interval(reference_df, test_df)
print(" ")
print("Choose Market:")
print("Choose 1 for FDA Both >= 85")
print("Choose 2 for FDA either >= 85")
print("Choose 3 for EMA/ICH/Canda/AUs")
print("Choose 4 for China")
print("Choose 5 for ASEAN")
print("Choose 6 for ANVISa")
print("Choose 7 for all f2s (Conventional/obvserved, Expected, Bias-corrected)")
print("Choose 8 for Both products >= 85, all f2s (Conventional/obvserved, Expected, Bias-corrected)")
print("Choose 9 for either products >= 85, all f2s (Conventional/obvserved, Expected, Bias-corrected)")
print("Choose 10 for either products > 85, all f2s (Conventional/obvserved, Expected, Bias-corrected)")
print("Choose 11 for Bootstrap bca")
print("Choose 12 for Both products >= 85 Bootstrap for all f2s")
print("Choose 13 for either products >= 85 Bootstrap for all f2s")
print("Choose 14 for either products > 85 Bootstrap for all f2s")
print("Choose 15 for calculation and regulation using R results for specific regulatory")
print("Choose 16 for calculation and regulation using R results for all regulatory")


input1=int(input("Input number: "))
if input1==1:
    print("According to FDA Both >= 85 guidelines")
    print(" ")
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products - ",check_cv(test_df) and check_cv(reference_df))
    print(" ")
    print("Both >= 85, Ignoring time point 0")
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_cv(test_df) and check_cv(reference_df):
        if check_same_time_points(test_df, reference_df):
            changed_data_both85_f2s(reference_df,test_df)
    else:
        if check_same_time_points(test_df, reference_df):
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_both85_bca(reference_df,test_df)

elif input1==2:
    print("According to FDA either >= 85 guidelines")
    print(" ")
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products - ",check_cv(test_df) and check_cv(reference_df))
    print(" ")
    print("either >= 85, Ignoring time point 0")
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_cv(test_df) and check_cv(reference_df):
        if check_same_time_points(test_df, reference_df):
            changed_data_either85_FDA_f2s(reference_df,test_df)
    else:
        if check_same_time_points(test_df, reference_df):
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_FDA_bca(reference_df,test_df)

elif input1==3:
    print("According to EMA/ICH/Canda/AUs guidelines")
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products - ",check_cv(test_df) and check_cv(reference_df))
    print("either > 85, Ignoring time point 0")
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_cv(test_df) and check_cv(reference_df):
        if check_same_time_points(test_df, reference_df):
            changed_data_either85_f2s(reference_df,test_df)
    else:
        if check_same_time_points(test_df, reference_df):
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df,test_df)
        
elif input1==4:
    print("According to China guidelines")
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products - ",check_cv(test_df) and check_cv(reference_df))
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_cv(test_df) and check_cv(reference_df):
        if check_same_time_points(test_df, reference_df):
            changed_data_either85_f2s(reference_df,test_df)
    else:
        if check_same_time_points(test_df, reference_df):
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df,test_df)
                
elif input1==5:
    print("According to ASEAN guidelines")
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products - ",check_cv(test_df) and check_cv(reference_df))
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_cv(test_df) and check_cv(reference_df):
        if check_same_time_points(test_df, reference_df):
            changed_data_either85_f2s(reference_df,test_df)
    else:
        if check_same_time_points(test_df, reference_df):
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df,test_df)

elif input1==6:
    print("According to ANVISa guidelines")
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products - ",check_cv(test_df) and check_cv(reference_df))
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_cv(test_df) and check_cv(reference_df):
        if check_same_time_points(test_df, reference_df):
            changed_data_either85_f2s(reference_df,test_df)
    else:
        if check_same_time_points(test_df, reference_df):
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df,test_df)
                         
elif input1==7:
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_same_time_points(test_df, reference_df):
        f2s(reference_df,test_df)  

elif input1==8:
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_same_time_points(test_df, reference_df):
        changed_data_both85_f2s(reference_df,test_df)

elif input1==9:
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_same_time_points(test_df, reference_df):
        changed_data_either85_FDA_f2s(reference_df,test_df)

elif input1==10:
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_same_time_points(test_df, reference_df):
        changed_data_either85_f2s(reference_df,test_df)       

elif input1==11:
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_same_time_points(test_df, reference_df):
        bca(test_df,reference_df) 

elif input1==12:
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_same_time_points(test_df, reference_df):
        print("Loading bootstrap percentile.. & Loading BCA...")
        changed_data_both85_bca(reference_df,test_df)

elif input1==13:
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_same_time_points(test_df, reference_df):
        print("Loading bootstrap percentile.. & Loading BCA...")
        changed_data_either85_FDA_bca(reference_df,test_df)

elif input1==14:
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0]=='0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df=reference_df.reset_index(drop=True)
    
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0]=='0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_same_time_points(test_df, reference_df):
        print("Loading bootstrap percentile.. & Loading BCA...")
        changed_data_either85_bca(reference_df,test_df)  

elif input1==15:
    R_Regulation(file_path)

elif input1==16:
    R_all_Regulation(file_path)

