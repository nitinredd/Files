import os
import io
import json
import uuid
import asyncio
import logging
from typing import Dict, List, Optional

import pandas as pd
import speech_recognition as sr
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from gtts import gTTS
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA

# ─── AZURE GPT CONFIG (UNCHANGED) ─────────────────────────────────
base_url = ""  # e.g. https://your-resource.openai.azure.com/
api_version = "2024-02-15-preview"
api_key = ""
deployment_name = "GPT4o"
model_name = "GPT4o"

# ─── EMBEDDING CONFIG (UNCHANGED) ────────────────────────────────
embed_endpoint = ""
embed_api_version = "2023-07-01-preview"
embed_key = ""
embed_deployment = "Def_data_qa"
embed_model = "text-embedding-ada-002"

# ─── PATHS & TILES ────────────────────────────────────────────────
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\DEC_Agent\Datasets\DEC_Agent_Json.json",
}
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    # ... other tiles
}

# ─── LOGGING ──────────────────────────────────────────────────────
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ─── AZURE CLIENTS ───────────────────────────────────────────────
file_store = LocalFileStore('langchain-embeddings')
embed_client = AzureOpenAIEmbeddings(
    model=embed_model,
    api_version=embed_api_version,
    azure_endpoint=embed_endpoint,
    api_key=embed_key,
    azure_deployment=embed_deployment
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    embed_client, file_store, namespace=embed_client.model
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)

# ─── RETRY STRATEGY ───────────────────────────────────────────────
@retry(
    retry=retry_if_exception_type(Exception),
    wait=wait_exponential(multiplier=1, min=2, max=30),
    stop=stop_after_attempt(5)
)
async def embed_texts(texts: List[str]) -> List[List[float]]:
    return embed_client.embed_documents(texts)

# ─── AGENT CLASSES ───────────────────────────────────────────────
class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=False
        )
    async def ask(self, query: str) -> str:
        result = await asyncio.to_thread(self.chain.invoke, {"query": query})
        return result.get("result", "")

class CoordinatorAgent:
    def __init__(self, children: List[ChildAgent]):
        self.children = children
    async def coordinate(self, query: str) -> Optional[str]:
        for child in self.children:
            try:
                ans = await child.ask(query)
                if ans and "not found" not in ans.lower():
                    logger.info(f"[Coordinator] '{child.name}' answered")
                    return ans
            except Exception as e:
                logger.warning(f"[Coordinator] {child.name} error: {e}")
        return None

class OversightAgent:
    def validate(self, answer: str) -> str:
        return answer

class LearningAgent:
    def __init__(self):
        self.logs: List[Dict] = []
    def log(self, q: str, a: str):
        self.logs.append({"query": q, "response": a})

class AgentManager:
    def __init__(self, agents: List[ChildAgent]):
        self.coordinator = CoordinatorAgent(agents)
        self.oversight = OversightAgent()
        self.learning = LearningAgent()
    async def handle_query(self, query: str) -> str:
        raw = await self.coordinator.coordinate(query)
        ans = raw or "Oops! No relevant information found."
        validated = self.oversight.validate(ans)
        self.learning.log(query, validated)
        return validated

# ─── INGESTION & VECTORSTORE ────────────────────────────────────
async def chunk(items: List[str], size: int):
    for i in range(0, len(items), size):
        yield items[i:i+size]

async def ingest_json(path: str, batch_size: int = 1000, concurrency: int = 4) -> List[Document]:
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    records = data if isinstance(data, list) else [data]
    texts = [json.dumps(rec, sort_keys=True) for rec in records]

    sem = asyncio.Semaphore(concurrency)
    async def process_batch(batch_texts):
        async with sem:
            vectors = await embed_texts(batch_texts)
            return [Document(page_content=t, metadata={"vector": v})
                    for t, v in zip(batch_texts, vectors)]

    tasks = []
    async for batch in chunk(texts, batch_size):
        tasks.append(asyncio.create_task(process_batch(batch)))

    docs: List[Document] = []
    for task in asyncio.as_completed(tasks):
        docs.extend(await task)
    return docs

async def build_agents(paths: Dict[str, str]) -> List[ChildAgent]:
    agents: List[ChildAgent] = []
    for name, path in paths.items():
        try:
            docs = await ingest_json(path)
            if not docs:
                continue
            store = FAISS.from_documents(docs, cached_embeddings)
            retr = store.as_retriever(search_kwargs={"k": 5})
            agents.append(ChildAgent(name, retr))
            logger.info(f"[Vectorstore] '{name}' with {len(docs)} docs")
        except Exception as e:
            logger.error(f"Failed '{name}': {e}")
    return agents

# ─── FASTAPI INITIALIZATION ──────────────────────────────────────
app = FastAPI(title="Multi-Agent JSON Chatbot")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Startup: load embedded files
@app.on_event("startup")
async def startup():
    app.state.agents = await build_agents(EMBEDDED_FILES)
    app.state.manager = AgentManager(app.state.agents)

class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1)

@app.get("/sample-tiles")
def sample_tiles():
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
async def chat(req: ChatRequest):
    try:
        resp = await app.state.manager.handle_query(req.message)
        return {"response": resp}
    except Exception as e:
        logger.error(f"/chat: {e}")
        raise HTTPException(500, "Internal server error")

@app.post("/upload-json")
async def upload_json(name: str, file: UploadFile = File(...)):
    try:
        content = await file.read()
        tmp = f"/tmp/{uuid.uuid4()}.json"
        with open(tmp, 'wb') as f:
            f.write(content)
        agent = (await build_agents({name: tmp}))[0]
        app.state.agents.append(agent)
        return {"status": "ingested", "agent": name}
    except Exception as e:
        logger.error(f"upload-json: {e}")
        raise HTTPException(400, str(e))

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = sr.Recognizer().record(src)
        text = sr.Recognizer().recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"STT: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"TTS: {e}")
        raise HTTPException(500, str(e))

@app.get("/health")
def health():
    names = [a.name for a in app.state.agents]
    return {"status": "ok", "agents": names}

# To run: uvicorn app:app --host 0.0.0.0 --port 8000 --reload
