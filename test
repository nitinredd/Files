import numpy as np
from scipy.interpolate import interp1d

def interpolate_linear(df, new_times):
    """
    Simple linear interpolation (used for candidate window determination
    and for candidate selection).
    """
    df_sorted = df.sort_values(by=df.columns[0])
    known_times = df_sorted.iloc[:, 0].values.astype(float)
    known_values = df_sorted.iloc[:, 1].values.astype(float)
    f = interp1d(known_times, known_values, kind='linear', 
                 fill_value=(known_values[0], known_values[-1]), bounds_error=False)
    return f(new_times)

def determine_candidate_window(ref_df, test_df, step=1, initial_threshold=10):
    """
    Identify candidate window based on the actual time range in the data.
    """
    max_ref_time = ref_df.iloc[:, 0].max()
    max_test_time = test_df.iloc[:, 0].max()
    fixed_max = max(max_ref_time, max_test_time)
    fixed_min = 0

    times = np.arange(fixed_min, fixed_max + 1, step)
    ref_vals = interpolate_linear(ref_df, times)
    test_vals = interpolate_linear(test_df, times)
    diff = np.abs(ref_vals - test_vals)
    
    valid_times = times[diff <= initial_threshold]
    if len(valid_times) == 0:
        print(f"No time points found within {initial_threshold}% difference; trying threshold=20.")
        valid_times = times[diff <= 20]
        if len(valid_times) == 0:
            print("No candidate window found even with 20% threshold. Using full range.")
            return fixed_min, fixed_max
        else:
            window_max = valid_times[-1]
            print(f"Candidate window determined (threshold 20): {fixed_min} to {window_max}")
            return fixed_min, window_max
    else:
        window_max = valid_times[-1]
        print(f"Candidate window determined (threshold {initial_threshold}): {fixed_min} to {window_max}")
        return fixed_min, window_max

def interpolate_dissolution_curve(df, new_times, method='linear'):
    """
    Predict dissolution values at new time points using linear interpolation.
    """
    df_sorted = df.sort_values(by=df.columns[0])
    known_times = df_sorted.iloc[:, 0].values.astype(float)
    known_values = df_sorted.iloc[:, 1].values.astype(float)
    
    new_times_clamped = np.clip(new_times, known_times.min(), known_times.max())
    f = interp1d(known_times, known_values, kind='linear', 
                 fill_value=(known_values[0], known_values[-1]), bounds_error=False)
    return f(new_times_clamped)
##############################
def predictive_optimal_combinations_advanced(ref_df, test_df, regulation, 
                                             window_min, window_max, diff_threshold=None,
                                             interp_method='linear', points_per_stratum=None):
    """
    Deterministic selection of candidate time points using linear interpolation.
    
    Steps:
      1. Use only times from the grid of 3- and 5-minute increments within [window_min, window_max].
      2. Compute predicted dissolution values (via linear interpolation) for reference and test.
      3. Split valid times into three strata based on predicted reference values:
           - 0-30: select exactly 2 candidate times (min and max among those with ref in [0,30))
           - 30-60: select exactly 2 candidate times (min and max among those with ref in [30,60))
           - 60-90: select exactly 2 candidate times (min and max among those with ref in [60,90))
         Then, for the 60–90 stratum, if neither candidate has both ref and test ≥80%, replace the later candidate with the candidate from the eligible set that maximizes min(ref, test).
      4. The final candidate combination (non‑FDA) is the union of these selections (6 points total).
      5. For FDA mode, append one extra candidate—the first time after the last candidate where both predictions are ≥85%—to yield 7 points.
      6. Compute f2 using:
             f2 = 50 * log10(100 / (1 + sqrt(mean((test - ref)^2))))
         (For the first candidate, force the value to 0.)
         
    Raises an error if any stratum (other than 60–90) has insufficient eligible times.
    """
    import numpy as np
    import math
    
    # 1. Build valid time grid (3- and 5-minute increments)
    valid_times = np.sort(np.unique(np.concatenate([
        np.arange(window_min, window_max+1, 3),
        np.arange(window_min, window_max+1, 5)
    ])))
    
    # 2. Compute predicted dissolution values using linear interpolation.
    all_ref_pred = interpolate_linear(ref_df, valid_times)
    all_test_pred = interpolate_linear(test_df, valid_times)
    
    # 3. Define dissolution strata (using predicted reference values)
    strata = {
        "0-30": (0, 30),
        "30-60": (30, 60),
        "60-90": (60, 90)
    }
    required_points = 2  # exactly two candidate times per stratum
    
    def eligible_times_for_stratum(stratum_range):
        low, high = stratum_range
        return [t for t, pred in zip(valid_times, all_ref_pred) if low <= pred < high]
    
    candidate = []
    
    # 4. For each stratum, select exactly two candidate times deterministically.
    for key in ["0-30", "30-60", "60-90"]:
        eligible = eligible_times_for_stratum(strata[key])
        if len(eligible) < required_points:
            raise ValueError(f"Insufficient eligible times in stratum {key}.")
        candidate_stratum = [min(eligible), max(eligible)]
        candidate.extend(candidate_stratum)
    
    candidate = sorted(list(set(candidate)))
    if len(candidate) != 6:
        raise ValueError(f"Final candidate count ({len(candidate)}) is not equal to 6 as required.")
    
    # 5. Enforce the 60-90 stratum rule.
    eligible_60_90 = eligible_times_for_stratum(strata["60-90"])
    cand_60_90 = [t for t in candidate if t in eligible_60_90]
    has_80 = False
    for t in cand_60_90:
        idx = int(np.where(valid_times == t)[0][0])
        if all_ref_pred[idx] >= 80 and all_test_pred[idx] >= 80:
            has_80 = True
            break
    if not has_80:
        # Replace the later candidate from the 60-90 group with the candidate that maximizes min(ref, test)
        best_val = -np.inf
        best_t = None
        for t in eligible_60_90:
            idx = int(np.where(valid_times == t)[0][0])
            val = min(all_ref_pred[idx], all_test_pred[idx])
            if val > best_val:
                best_val = val
                best_t = t
        # Determine current 60-90 candidates from our selection:
        current_60_90 = sorted([t for t in candidate if t in eligible_60_90])
        if len(current_60_90) < 2:
            raise ValueError("Insufficient candidates in 60-90 stratum to adjust.")
        # Replace the later candidate with best_t
        candidate = [t for t in candidate if t not in current_60_90]
        candidate.extend([min(current_60_90), best_t])
        candidate = sorted(list(set(candidate)))
        if len(candidate) != 6:
            raise ValueError("After adjustment, candidate count is not 6.")
    
    # 6. For FDA mode: Append extra candidate—the first time after the last candidate with both predictions >=85%
    if regulation == "FDA":
        expected_count = 7
        last_candidate = candidate[-1]
        post_times = [t for t in valid_times if t > last_candidate]
        extra_point = None
        for t in post_times:
            idx = int(np.where(valid_times == t)[0][0])
            if all_ref_pred[idx] >= 85 and all_test_pred[idx] >= 85:
                extra_point = t
                break
        if extra_point is None:
            raise ValueError("No valid extra candidate found for FDA mode.")
        candidate.append(extra_point)
        candidate = sorted(list(set(candidate)))
        if len(candidate) != expected_count:
            raise ValueError(f"Final candidate count ({len(candidate)}) does not equal expected count ({expected_count}).")
    else:
        if len(candidate) != 6:
            raise ValueError(f"Non-FDA candidate count ({len(candidate)}) is not equal to 6.")
    
    # 7. Compute f2.
    ref_vals = np.array([float(val) for val in interpolate_linear(ref_df, candidate)])
    test_vals = np.array([float(val) for val in interpolate_linear(test_df, candidate)])
    if candidate[0] == window_min:
        ref_vals[0] = 0.0
        test_vals[0] = 0.0
    diff = test_vals - ref_vals
    f2 = 50 * math.log10(100 / (1 + math.sqrt(np.mean(diff**2))))
    
    result = {
        'sequence': candidate,
        'f2': round(f2, 2),
        'compliant': True,
        'reasons': [],
        'ref_vals': ref_vals.tolist(),
        'test_vals': test_vals.tolist()
    }
    return [result], [result]
#################################################################3
if run_predictive.lower() == 'yes':
    # Determine candidate window.
    window_min, window_max = determine_candidate_window(
        reference_mean_df,
        test_mean_df,
        step=5,
        initial_threshold=10
    )
    
    # Map regulation input.
    regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
    selected_regulation = regulation_map.get(input1, "FDA")
    
    print(f"\nCandidate window for combination search: {window_min} to {window_max}")
    
    # Run deterministic predictive analysis.
    results, all_results = predictive_optimal_combinations_advanced(
        reference_mean_df,
        test_mean_df,
        regulation=selected_regulation,
        window_min=window_min,
        window_max=window_max,
        diff_threshold=None,
        interp_method='linear',
        points_per_stratum=None
    )
    
    # Convert candidate time points to integers.
    for cand in results:
        cand['sequence'] = [int(t) for t in cand['sequence']]
    
    overall_best = results[0] if results else None
    
    if overall_best:
        print("\n=== Optimal Predictive Combination ===")
        print(f"Time Points (best candidate): {overall_best['sequence']}")
        print(f"Length: {len(overall_best['sequence'])}")
        print(f"Predicted f2 Score: {overall_best['f2']}")
        if overall_best['reasons']:
            print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
        else:
            print("Regulatory Compliance: Passed")
        
        # Plot predicted dissolution curves.
        import matplotlib.pyplot as plt
        plt.figure(figsize=(12, 6))
        time_points = overall_best['sequence']
        ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(time_points), method='linear')
        test_diss = interpolate_dissolution_curve(test_mean_df, np.array(time_points), method='linear')
        plt.plot(time_points, ref_diss, 'bo-', label='Reference')
        plt.plot(time_points, test_diss, 'r*--', label='Test')
        plt.title(f"Optimal Profile: Predicted Dissolution (f2 = {overall_best['f2']})")
        plt.xlabel('Time (min)')
        plt.ylabel('Dissolution (%)')
        plt.legend()
        plt.grid(True)
        plt.show()
        
        print("\nPredicted Reference Dissolution Percentages:")
        for t, d in zip(time_points, ref_diss):
            print(f"Time {t} min: {d:.2f}%")
        print("\nPredicted Test Dissolution Percentages:")
        for t, d in zip(time_points, test_diss):
            print(f"Time {t} min: {d:.2f}%")
    else:
        print("❌ No candidate sequence was generated.")
    
    print("\n=== All Candidate Combination (Diverse) ===")
    for idx, cand in enumerate(results):
        seq_print = [int(t) for t in cand['sequence']]
        print(f"{idx+1:3d}. | Points: {seq_print} | Length: {len(seq_print)} | f2: {cand['f2']} | Compliant: {cand['compliant']}")
