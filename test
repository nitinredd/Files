import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd as autograd
import gym
from stable_baselines3 import PPO

# ---------------------- Streamlit Configuration ----------------------
st.set_page_config(page_title="AI-Powered Drug Release Simulator", layout="wide")
st.title("üß™ AI-Powered Drug Release Simulator")
st.sidebar.title("‚öôÔ∏è Controls")

# ---------------------- Physics-Informed Neural Networks (PINNs) ----------------------
st.sidebar.subheader("PINN: Drug Diffusion Solver")

# PINN Model Definition
class PINN(nn.Module):
    def __init__(self):
        super(PINN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 32),
            nn.Tanh(),
            nn.Linear(32, 32),
            nn.Tanh(),
            nn.Linear(32, 1)
        )

    def forward(self, x, t):
        inputs = torch.cat((x, t), dim=1)
        return self.net(inputs)

# PINN Loss (Solving Diffusion PDE)
def pde_loss(model, x, t, D=0.1):
    u = model(x, t)
    u_t = autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]
    u_xx = autograd.grad(autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0], x, grad_outputs=torch.ones_like(u), create_graph=True)[0]
    
    return ((u_t - D * u_xx) ** 2).mean()

# PINN Training
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pinn_model = PINN().to(device)
optimizer = torch.optim.Adam(pinn_model.parameters(), lr=1e-3)

if st.sidebar.button("Train PINN Model"):
    x_train = torch.rand(100, 1, device=device, requires_grad=True)
    t_train = torch.rand(100, 1, device=device, requires_grad=True)
    
    for epoch in range(5000):
        optimizer.zero_grad()
        loss = pde_loss(pinn_model, x_train, t_train)
        loss.backward()
        optimizer.step()

        if epoch % 500 == 0:
            st.sidebar.text(f"Epoch {epoch}, Loss: {loss.item():.6f}")

    st.sidebar.success("PINN Model Trained!")

# PINN Prediction
st.subheader("üìà PINN Prediction of Drug Diffusion")
x_test = torch.linspace(0, 1, 100).reshape(-1, 1).to(device)
t_test = torch.ones_like(x_test) * 0.5
u_pred = pinn_model(x_test, t_test).detach().cpu().numpy()

fig_pinn, ax_pinn = plt.subplots()
ax_pinn.plot(x_test.cpu().numpy(), u_pred, label="PINN Prediction", color='blue')
ax_pinn.set_xlabel("Position (x)")
ax_pinn.set_ylabel("Concentration (u)")
ax_pinn.set_title("Drug Diffusion Prediction using PINN")
ax_pinn.legend()
st.pyplot(fig_pinn)

# ---------------------- Reinforcement Learning (RL) for Optimization ----------------------
st.sidebar.subheader("RL: Optimize Drug Formulation")

# Custom Gym Environment for Drug Release Optimization
class DrugReleaseEnv(gym.Env):
    def __init__(self):
        super(DrugReleaseEnv, self).__init__()
        self.action_space = gym.spaces.Box(low=np.array([0.01, 0.1]), high=np.array([1.0, 2.0]), dtype=np.float32)  # (D, k)
        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(100,), dtype=np.float32)
        self.time = np.linspace(0, 10, 100)

    def step(self, action):
        D, k = action
        release_profile = 1 - np.exp(-k * self.time)  # Approximate drug release
        reward = -np.abs(release_profile[-1] - 0.8)  # Reward reaching 80% release
        return release_profile, reward, False, {}

    def reset(self):
        return np.zeros(100, dtype=np.float32)

# Train RL Agent
if st.sidebar.button("Train RL Agent"):
    env = DrugReleaseEnv()
    rl_model = PPO("MlpPolicy", env, verbose=1)
    rl_model.learn(total_timesteps=10000)
    st.sidebar.success("RL Agent Trained!")

    # Test Optimized Formulation
    obs = env.reset()
    action, _states = rl_model.predict(obs)
    st.sidebar.write("Optimized Parameters:", action)

# ---------------------- Visualization of RL Optimization ----------------------
st.subheader("ü§ñ RL Optimization of Drug Formulation")
if "rl_model" in locals():
    env = DrugReleaseEnv()
    obs = env.reset()
    action, _states = rl_model.predict(obs)
    
    D_opt, k_opt = action
    release_opt = 1 - np.exp(-k_opt * env.time)

    fig_rl, ax_rl = plt.subplots()
    ax_rl.plot(env.time, release_opt, label="Optimized Release Profile", color='green')
    ax_rl.set_xlabel("Time (hours)")
    ax_rl.set_ylabel("Fractional Release")
    ax_rl.set_title("Optimized Drug Release Using RL")
    ax_rl.legend()
    st.pyplot(fig_rl)
else:
    st.warning("Train the RL model first to see optimized results.")

# ---------------------- Download Options ----------------------
st.subheader("üì• Export Simulation Data")
csv_data = np.column_stack((env.time, release_opt if "release_opt" in locals() else np.zeros(100)))
csv_content = "Time,Fractional Release\n" + "\n".join([f"{t},{r}" for t, r in csv_data])
st.download_button("Download Results", csv_content, file_name="optimized_drug_release.csv", mime="text/csv")

st.sidebar.markdown("---")
st.sidebar.markdown("Developed with **AI-powered modeling** for better drug release predictions.")
