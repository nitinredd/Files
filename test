import os
import streamlit as st
import pandas as pd
import google.auth
from vertexai.preview.generative_models import GenerativeModel, HarmCategory, HarmBlockThreshold, SafetySetting
from typing import Dict, List, Tuple

# Configure Gemini
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "D:/datascience-254609-genai.json"
credentials, project_id = google.auth.default()

safety_config = [
    SafetySetting(
        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        threshold=HarmBlockThreshold.BLOCK_NONE,
    ),
    # Add other safety categories as per your original config
]

model = GenerativeModel("gemini-2.0-flash-thinking-exp-01-21")

# Agent System Components
class ExcelAnalyzer:
    def __init__(self):
        self.data: Dict[str, Dict[str, pd.DataFrame]] = {}
        
    def load_excel(self, file_path: str) -> None:
        """Load all sheets from an Excel file"""
        xls = pd.ExcelFile(file_path)
        self.data[file_path] = {sheet_name: xls.parse(sheet_name) 
                               for sheet_name in xls.sheet_names}

class QueryProcessor:
    def __init__(self, analyzer: ExcelAnalyzer):
        self.analyzer = analyzer
        self.cache = {}

    def _search_data(self, query: str) -> Tuple[str, pd.DataFrame]:
        """Multi-agent search process"""
        # Coordinator Agent: Determine search parameters
        search_params = self._parse_query(query)
        
        # Child Agents: Search across all sheets
        results = []
        for file in self.analyzer.data.values():
            for sheet_name, df in file.items():
                # Manager Agent: Apply search filters
                filtered = self._filter_df(df, search_params)
                if not filtered.empty:
                    results.append((sheet_name, filtered))
        
        # Oversight Agent: Validate results
        if not results:
            return None
        return results

    def _filter_df(self, df: pd.DataFrame, params: dict) -> pd.DataFrame:
        """Filter dataframe based on search parameters"""
        # Implement actual filtering logic based on params
        return df

class ResponseGenerator:
    def __init__(self, model: GenerativeModel):
        self.model = model
        self.base_prompt = """You're an Excel analysis expert. Given the following context from our spreadsheets:
        {context}
        Answer this query: {query}
        If information isn't available, say 'Data not found in documents'"""

    def generate(self, query: str, context: str) -> str:
        prompt = self.base_prompt.format(context=context, query=query)
        response = self.model.generate_content(
            prompt,
            safety_settings=safety_config
        )
        return response.text

# Streamlit App
def main():
    st.title("Excel Analysis Chatbot")
    
    # Initialize session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.analyzer = ExcelAnalyzer()
        st.session_state.analyzer.load_excel("formula master_osd.xlsx")
        st.session_state.analyzer.load_excel("masterlist osd equipments.xlsx")
        st.session_state.processor = QueryProcessor(st.session_state.analyzer)
        st.session_state.generator = ResponseGenerator(model)

    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Dynamic sample prompts
    sample_questions = [
        "Do we have Tapasya make with Capacity 600L in FTO-3?",
        "What is the impeller speed range in RMG for the Equipment Glatt in FTO-2?",
        "What are the various literature sources considered in RMG?"
    ]

    # Show sample questions as buttons
    if len(st.session_state.messages) == 0:
        with st.chat_message("assistant"):
            st.write("Hi! How can I help you with the equipment data today?")
            for q in sample_questions:
                if st.button(q):
                    st.session_state.messages.append({"role": "user", "content": q})
                    process_query(q)

    # User input
    if prompt := st.chat_input("Ask about equipment data..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        process_query(prompt)

def process_query(query: str):
    # Query Processing Agents
    results = st.session_state.processor._search_data(query)
    
    # Validation Agent
    if not results:
        response = "This information is not available in the documents."
    else:
        # Format context for LLM
        context = "\n\n".join(
            [f"Sheet: {sheet}\nData:\n{df.head(3).to_markdown()}" 
             for sheet, df in results]
        )
        
        # Generate response with safety checks
        response = st.session_state.generator.generate(query, context)
    
    # Add response to chat
    with st.chat_message("assistant"):
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()
