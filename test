#S2L
import os
import io
import json
import time
import asyncio
import pandas as pd
import logging
import speech_recognition as sr
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Union, List, Dict, Optional
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
from gtts import gTTS
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai
import inspect

# â”€â”€â”€ YOUR EXACT CONFIGURATION (KEPT AS-IS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
base_url=""
api_version="2024-02-15-preview"

api_key=""
deployment_name="GPT4o"
model_name="GPT4o"

# Paths to embedded JSON files
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\ROW\S2L\Datasets\Json\Master.json",
    # 'data_2': r"C:\Users\p00095189\Desktop\S2L\portfolio_D.json",
    # additional files can be specified dynamically, and can be left None
}

# Sample-prompt tiles
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B": ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X": ["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# Embedding store & cache
file_store = LocalFileStore('langchain-embeddings')

# Directly use embedding configuration values
embedding_model = "text-embedding-ada-002"
embedding_api_version = "2023-07-01-preview"
embedding_azure_endpoint = ""
embedding_api_key = ""
embedding_deployment = "Def_data_qa"

# Chat model initialization
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)

# â”€â”€â”€ ENHANCEMENTS FOR LARGE FILES & PRODUCTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = logging.getLogger(__name__)

# Configuration for large file handling
CHUNK_SIZE = 1500  # Optimal for embedding models
CHUNK_OVERLAP = 200
EMBEDDING_BATCH_SIZE = 32
MAX_RETRIES = 10
BACKOFF_FACTOR = 1.5

class RobustAzureEmbeddings(AzureOpenAIEmbeddings):
    """Enhanced embeddings with retry logic for rate limits"""
    @retry(
        wait=wait_exponential(multiplier=BACKOFF_FACTOR, min=4, max=60),
        stop=stop_after_attempt(MAX_RETRIES),
        retry=retry_if_exception_type((openai.RateLimitError, openai.APITimeoutError))
    )
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return super().embed_documents(texts)

    @retry(
        wait=wait_exponential(multiplier=BACKOFF_FACTOR, min=4, max=60),
        stop=stop_after_attempt(MAX_RETRIES),
        retry=retry_if_exception_type((openai.RateLimitError, openai.APITimeoutError))
    )
    def embed_query(self, text: str) -> List[float]:
        return super().embed_query(text)

# Create robust embeddings directly using configuration values
base_embeddings = RobustAzureEmbeddings(
    model=embedding_model,
    api_version=embedding_api_version,
    azure_endpoint=embedding_azure_endpoint,
    api_key=embedding_api_key,
    azure_deployment=embedding_deployment
)

cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    base_embeddings, 
    file_store, 
    namespace=base_embeddings.model
)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
)

# â”€â”€â”€ AGENT CLASSES (ENHANCED) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=False
        )

    @retry(wait=wait_exponential(min=1, max=10), stop=stop_after_attempt(3))
    def ask(self, query: str) -> str:
        try:
            resp = self.chain.invoke({"query": query})
            return resp.get("result", "")
        except Exception as e:
            logger.error(f"Agent {self.name} failed: {str(e)}")
            return f"Error processing request: {str(e)}"

class CoordinatorAgent:
    def __init__(self, children: List[ChildAgent]):
        self.children = children

    def coordinate(self, query: str) -> Union[str, None]:
        for child in self.children:
            try:
                ans = child.ask(query)
                if ans and "not found" not in ans.lower() and "error" not in ans.lower():
                    logger.info(f"Coordinator selected {child.name}")
                    return ans
            except Exception as e:
                logger.warning(f"Child agent {child.name} error: {str(e)}")
        return None

class AgentManager:
    def __init__(self, agents: List[ChildAgent]):
        self.coordinator = CoordinatorAgent(agents)
        self.learning = self.LearningAgent()
    
    def handle_query(self, query: str) -> str:
        raw = self.coordinator.coordinate(query)
        answer = raw if raw else "I couldn't find relevant information for your query."
        self.learning.log(query, answer)
        return answer
    
    class LearningAgent:
        def __init__(self):
            self.logs: List[Dict] = []
        
        def log(self, q: str, a: str):
            self.logs.append({
                "timestamp": time.time(),
                "query": q,
                "response": a
            })

# â”€â”€â”€ LARGE FILE HANDLING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def stream_json_data(path: str) -> List[Dict]:
    """Load a JSON file that may be:
       - a single JSON object ({}),
       - a JSON array ([]), or
       - newline-delimited JSON (JSONL).
    """
    if not os.path.exists(path):
        logger.error(f"File not found: {path}")
        return []

    data: List[Dict] = []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            text = f.read()
            # Strip any BOM then whitespace
            stripped = text.lstrip('\ufeff').strip()

            # Try parsing as a full JSON document
            try:
                parsed = json.loads(text)
                if isinstance(parsed, dict):
                    data = [parsed]
                elif isinstance(parsed, list):
                    data = parsed
                else:
                    logger.warning(f"Unexpected JSON root type: {type(parsed)}")
                logger.info(f"Loaded {len(data)} record(s) from full-JSON parse of {path}")
                return data
            except json.JSONDecodeError:
                # Not a single JSON doc â†’ maybe JSONL
                logger.info(f"Full-JSON parse failed, falling back to JSONL for {path}")
                f.seek(0)

            # JSONL branch
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    record = json.loads(line)
                    data.append(record)
                except json.JSONDecodeError:
                    logger.warning(f"Invalid JSON line in {path}: {line[:100]}...")

        logger.info(f"Loaded {len(data)} record(s) from JSONL of {path}")
        return data

    except Exception as e:
        logger.error(f"Error loading {path}: {e}")
        return []

def process_data_chunks(data: List[Dict], source: str) -> List[Document]:
    """Process data into document chunks with metadata"""
    documents = []
    total_chunks = 0
    
    for idx, record in enumerate(data):
        try:
            content = json.dumps(record, ensure_ascii=False)
            chunks = text_splitter.split_text(content)
            for chunk_idx, chunk in enumerate(chunks):
                documents.append(Document(
                    page_content=chunk,
                    metadata={
                        "source": source,
                        "record_id": idx,
                        "chunk_id": chunk_idx,
                        "file_path": EMBEDDED_FILES[source]
                    }
                ))
            total_chunks += len(chunks)
        except Exception as e:
            logger.error(f"Error processing record {idx}: {str(e)}")
    
    logger.info(f"Created {total_chunks} chunks from {len(data)} records for {source}")
    return documents

def create_vectorstore(documents: List[Document]) -> FAISS:
    """Create vectorstore with progress tracking"""
    if not documents:
        return None
    
    logger.info(f"Building vectorstore with {len(documents)} chunks...")
    start_time = time.time()
    
    # Batch processing for large document sets
    batch_size = EMBEDDING_BATCH_SIZE
    vector_store = None
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        try:
            if vector_store is None:
                vector_store = FAISS.from_documents(batch, cached_embeddings)
            else:
                vector_store.add_documents(batch)
            
            # Log progress
            if (i // batch_size) % 10 == 0:
                elapsed = time.time() - start_time
                logger.info(f"Processed {min(i+batch_size, len(documents))}/{len(documents)} chunks "
                          f"({elapsed:.1f}s elapsed)")
        except Exception as e:
            logger.error(f"Error processing batch {i//batch_size}: {str(e)}")
            time.sleep(5)  # Brief pause before retry
    
    logger.info(f"Vectorstore built in {time.time()-start_time:.1f} seconds")
    return vector_store

# â”€â”€â”€ APPLICATION STATE MANAGEMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AppState:
    def __init__(self):
        self.ready = False
        self.initializing = False
        self.manager = None
        self.last_update = 0
        self.data_stats = {}

    async def initialize(self):
        """Kick off initialization in the background (non-blocking)."""
        # If weâ€™re already ready or in the middle of initializing, do nothing
        if self.ready or self.initializing:
            return
        self.initializing = True
        # Fire-and-forget the heavy-lifting coroutine
        asyncio.create_task(self._initialize_background())

    async def _initialize_background(self):
        """Background initialization with progress tracking."""
        try:
            logger.info("Starting data loading process...")
            agents = []

            for name, path in EMBEDDED_FILES.items():
                if not path:
                    logger.warning(f"Skipping empty path for {name}")
                    continue

                logger.info(f"Processing {name}: {path}")
                start_time = time.time()

                # Load and process data
                data = stream_json_data(path)
                if not data:
                    logger.warning(f"No data loaded for {name}")
                    continue

                documents = process_data_chunks(data, name)
                if not documents:
                    logger.warning(f"No documents created for {name}")
                    continue

                # Create vector store
                store = create_vectorstore(documents)
                if store is None:
                    logger.error(f"Vectorstore creation failed for {name}")
                    continue

                # Instantiate the child agent
                agent = ChildAgent(
                    name=name,
                    retriever=store.as_retriever(search_kwargs={"k": 5})
                )
                agents.append(agent)

                # Record stats
                self.data_stats[name] = {
                    "records": len(data),
                    "chunks": len(documents),
                    "load_time": time.time() - start_time
                }
                logger.info(
                    f"Created agent for {name} in {self.data_stats[name]['load_time']:.1f}s"
                )

            # Finalize
            self.manager = AgentManager(agents)
            self.ready = True
            self.initializing = False
            self.last_update = time.time()
            logger.info(f"Application initialized with {len(agents)} agents")

        except Exception as e:
            logger.critical(f"Initialization failed: {e}")
            self.ready = False
            self.initializing = False



app_state = AppState()
recognizer = sr.Recognizer()

# â”€â”€â”€ FASTAPI APP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI(title="Enterprise JSON Query System")

# CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request models
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None

# â”€â”€â”€ ENDPOINTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.on_event("startup")
async def startup_event():
    await app_state.initialize()

@app.get("/health")
async def health():
    status = {
        "status": "ready" if app_state.ready else "initializing",
        "agents": list(EMBEDDED_FILES.keys()),
        "data_stats": app_state.data_stats,
        "last_update": app_state.last_update
    }
    return JSONResponse(status)

@app.get("/sample-tiles")
def sample_tiles():
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
async def chat(req: ChatRequest):
    if not app_state.ready:
        raise HTTPException(503, detail="Service initializing. Please try again later.")
    
    q = req.message.strip()
    if not q:
        raise HTTPException(400, detail="Empty query")
    if len(q) > 1000:
        raise HTTPException(400, detail="Query too long (max 1000 characters)")
    
    logger.info(f"Processing query: {q}")
    start_time = time.time()
    ans = app_state.manager.handle_query(q)
    latency = time.time() - start_time
    
    logger.info(f"Query processed in {latency:.2f}s")
    return {"response": ans, "latency": latency}

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    try:
        if file.content_type not in ["audio/wav", "audio/mpeg", "audio/webm"]:
            raise HTTPException(400, detail="Unsupported audio format. Use WAV, MP3, or WEBM.")
        
        # Limit file size (10MB)
        max_size = 10 * 1024 * 1024
        if file.size > max_size:
            raise HTTPException(400, detail="File too large. Max 10MB.")
        
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except sr.UnknownValueError:
        raise HTTPException(400, detail="Audio not understandable")
    except sr.RequestError as e:
        raise HTTPException(500, detail=f"Speech recognition error: {str(e)}")
    except Exception as e:
        logger.exception("STT processing failed")
        raise HTTPException(500, detail=f"Processing error: {str(e)}")

@app.get("/text-to-speech")
async def text_to_speech(text: str):
    if len(text) > 2000:
        raise HTTPException(400, detail="Text too long (max 2000 characters)")
    
    try:
        buf = io.BytesIO()
        tts = gTTS(text=text, lang='en', slow=False)
        tts.write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"TTS failed: {str(e)}")
        raise HTTPException(500, detail="Speech synthesis failed")

@app.post("/reinitialize")
async def reinitialize(background_tasks: BackgroundTasks):
    """Endpoint to reload data"""
    if app_state.initializing:
        return {"status": "already_initializing"}
    
    app_state.ready = False
    await app_state.initialize(background_tasks)
    return {"status": "reinitialization_started"}

# â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler("application.log")
        ]
    )
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        timeout_keep_alive=600,  # Longer timeout for large files
        log_config=None
    )


#######################################################################################
#SCPCB
import os
import io
import json
import uuid
import pandas as pd
import logging
import speech_recognition as sr
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Union

from gtts import gTTS
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA

# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure GPT configuration (fill these in)
base_url=""
api_version="2024-02-15-preview"

api_key=""
deployment_name="GPT4o"
model_name="GPT4o"

# Paths to embedded JSON files
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Prime_M+F_JSON.json",
    # 'data_2': r"C:\Users\p00095189\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Formulas.json",
    # additional files can be specified dynamically, and can be left None
}

# Sampleâ€‘prompt tiles (for frontend to fetch)
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X":["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# â”€â”€â”€ SETUP LLM + EMBEDDINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Embedding store & cache
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment="Def_data_qa"
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# â”€â”€â”€ AGENT CLASSES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=False
        )
    def ask(self, query: str) -> str:
        resp = self.chain.invoke({"query": query})
        return resp.get("result", "")

class CoordinatorAgent:
    def __init__(self, children: list[ChildAgent]):
        self.children = children

    def coordinate(self, query: str) -> Union[str, None]:
        for child in self.children:
            try:
                ans = child.ask(query)
                if ans and "not found" not in ans.lower() and "error" not in ans.lower():
                    logger.info(f"Coordinator selected {child.name}")
                    return ans
            except Exception as e:
                logger.warning(f"Child agent {child.name} error: {str(e)}")
        return None

class OversightAgent:
    def validate(self, answer: str) -> str:
        return answer

class LearningAgent:
    def __init__(self):
        self.logs: list[dict] = []
    def log(self, q: str, a: str):
        self.logs.append({"query": q, "response": a})

class AgentManager:
    def __init__(self, agents: list[ChildAgent]):
        self.coordinator = CoordinatorAgent(agents)
        self.oversight  = OversightAgent()
        self.learning   = LearningAgent()
    def handle_query(self, query: str) -> str:
        raw = self.coordinator.coordinate(query)
        answer = raw if raw else "Oops! No relevant information found."
        validated = self.oversight.validate(answer)
        self.learning.log(query, validated)
        return validated

# â”€â”€â”€ DATA LOADING & VECTORSTORE BUILD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_json_data(paths: dict[str, str]) -> dict[str, pd.DataFrame]:
    dfs: dict[str, pd.DataFrame] = {}
    for name, path in paths.items():
        if not path:
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            records = []
            if isinstance(data, list):
                for item in data:
                    records.append({"content": json.dumps(item, sort_keys=True)})
            else:
                records.append({"content": json.dumps(data, sort_keys=True)})
            df = pd.DataFrame(records)
            dfs[name] = df
            logger.info(f"[Data] Loaded {len(df)} records for '{name}'")
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    return dfs

def build_vectorstores(dfs: dict[str, pd.DataFrame]) -> list[ChildAgent]:
    agents: list[ChildAgent] = []
    for key, df in dfs.items():
        docs = [
            Document(page_content=row["content"], metadata={"source": key})
            for _, row in df.iterrows()
        ]
        if not docs:
            continue
        store = FAISS.from_documents(docs, cached_embeddings)
        retriever = store.as_retriever(search_kwargs={"k": 5})
        agents.append(ChildAgent(name=key, retriever=retriever))
        logger.info(f"[Vectorstore] Built store for '{key}' ({len(docs)} docs)")
    return agents

# Initialize on startup
DATAFRAMES = load_json_data(EMBEDDED_FILES)
AGENTS     = build_vectorstores(DATAFRAMES)
MANAGER    = AgentManager(AGENTS)
recognizer = sr.Recognizer()

# â”€â”€â”€ FASTAPI APP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app = FastAPI(title="Multi-Agent JSON Chatbot")

# CORS for your React frontend on :5173
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request models
class ChatRequest(BaseModel):
    message: str

# â”€â”€â”€ ENDPOINTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/sample-tiles")
def sample_tiles():
    """Return the sample-prompt tiles and questions."""
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
def chat(req: ChatRequest):
    q = req.message.strip()
    if not q:
        raise HTTPException(400, "Empty query")
    logger.info(f"[API] /chat query='{q}'")
    ans = MANAGER.handle_query(q)
    return {"response": ans}

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    """
    Accepts an uploaded audio file (wav/mp3) and returns the transcribed text.
    """
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"[API] STT error: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    """
    Returns an MP3 audio stream of the given text.
    """
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"[API] TTS error: {e}")
        raise HTTPException(500, str(e))

@app.get("/health")
def health():
    return {"status": "ok", "agents": [a.name for a in AGENTS]}
#################################################################################################
#RDB
import os
import re
import glob
import base64
import io
import json
from typing import Union
from jose import jwt
import threading
from typing import List, Optional, Dict, Any
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Depends
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBasic
from datetime import datetime, timedelta
from pydantic import BaseModel
import fitz  # PyMuPDF
import pandas as pd
from PIL import Image
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import config
import mimetypes
from fastapi import HTTPException
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List
from difflib import SequenceMatcher
from typing import Tuple
import traceback



# ---------------------------
# Configuration (as provided by you â€” DO NOT CHANGE unless you need to)
# ---------------------------
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Azure Configuration (kept exactly as requested)
base_url=""
api_version="2025-01-01-preview"

api_key=""
deployment_name="api-ai4o"
model_name="gpt-4o"

# Initialize Azure services
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-3-large",
    api_version="2025-01-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment=""
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)
# ---------------------------

# ðŸ” JWT Config
SECRET_KEY = "myFAVsecretKEY"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

# Reaction types and filesystem layout (kept from your code)
REACTION_TYPES = [
    "C-C_Bond_Formation", "C-N_Bond_Formation", "Salt_Formation", "Hydrolysis",
    "Amidation", "Reduction", "Oxidation", "Cyclization", "Purification",
    "Metal_mediated_catalyzed", "C-halogen Bond Formation", "Miscellaneous"
]

BASE_DIR = r"C:\Users\Desktop\WORK\API\Reaction_Database\Datasets_O\Reaction_Database"
PRODUCTS_DIR = os.path.join(BASE_DIR, "Products")
SCHEMES_DIR = os.path.join(BASE_DIR, "Synthetic_Schemes")

# Ensure dirs exist (idempotent)
os.makedirs(PRODUCTS_DIR, exist_ok=True)
os.makedirs(SCHEMES_DIR, exist_ok=True)

# Prompt template (kept largely the same)
EXTRACTION_PROMPT_TEMPLATE = """
You are a pharmaceutical chemistry expert specializing in reaction chemistry. Extract the following information from the document in a structured format Mandatorily:
1. **API Name**: The active pharmaceutical ingredient
2. **Reaction Chemistry**: Type and description
3. **Yield**: Exact yield percentages or values mentioned in the source
4. **Procedure**: Summarize the complete procedure into clear, concise numbered bullet points, preserving the key steps and important details. Do NOT omit any steps or essential content.
5. **Tabular Data**: Provide COMPLETE tabular data in markdown table format. Do NOT omit, summarize, or transform any content.

Structure your response as follows (literal headers must appear exactly like below):

### API Name
[API name here]
### Reaction Chemistry
[Reaction chemistry description here]
### Yield
[Yield value here]
### Procedure
[Complete procedure here]
### Tabular Data
[Markdown table here]

Document Content:
{context}
Question: {question}
Answer:
"""
EXTRACTION_PROMPT = PromptTemplate(template=EXTRACTION_PROMPT_TEMPLATE, input_variables=["context", "question"])


QA_PROMPT_TEMPLATE = """
You are a concise pharmaceutical chemistry expert. Use the provided document context to answer the user's question directly and concisely.

Rules (follow exactly):
- Use only the information present in the context. Do NOT hallucinate.
- Answer in one short paragraph (2-6 sentences) unless the user explicitly asks for step-by-step procedure.
- Do NOT reproduce the full document content. Do NOT provide unrequested long verbatim passages.
- If multiple documents were used, identify only those documents that were actually the sources of the answer.
- If the answer is not present in the context, say "I could not find an answer in the provided documents." (do not guess).

Context:
{context}

Question: {question}

Answer:
"""
QA_PROMPT = PromptTemplate(template=QA_PROMPT_TEMPLATE, input_variables=["context", "question"])

# ------------------------------
# Helper functions
# ------------------------------
def find_scheme_image(reaction_type: str, product_name: str) -> Optional[str]:
    for ext in ['.jpeg', '.jpg', '.png', '.gif']:
        scheme_path = os.path.join(SCHEMES_DIR, reaction_type, f"{product_name}{ext}")
        if os.path.exists(scheme_path):
            return scheme_path
    return None

def list_products() -> List[Dict[str, Any]]:
    products = []
    for reaction_type in REACTION_TYPES:
        reaction_dir = os.path.join(PRODUCTS_DIR, reaction_type)
        if not os.path.exists(reaction_dir):
            continue
        pdf_files = glob.glob(os.path.join(reaction_dir, "*.pdf"))
        for pdf_path in pdf_files:
            filename = os.path.basename(pdf_path)
            product_name = os.path.splitext(filename)[0]
            scheme_image = find_scheme_image(reaction_type, product_name)
            scheme_cdx = os.path.join(SCHEMES_DIR, reaction_type, f"{product_name}.cdx")
            product_id = f"{reaction_type}_{product_name}"
            products.append({
                "id": product_id,
                "name": product_name,
                "reaction_type": reaction_type,
                "pdf_path": pdf_path,
                "scheme_image": scheme_image if scheme_image else None,
                "scheme_cdx": scheme_cdx if os.path.exists(scheme_cdx) else None
            })
    return products

def extract_pdf_text(pdf_path: str) -> str:
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text() + "\n"
        return text
    except Exception as e:
        raise RuntimeError(f"Error reading PDF {pdf_path}: {e}")

# Cache for vectorstores and product details
_vectorstore_cache: Dict[str, FAISS] = {}
_product_details_cache: Dict[str, Dict[str, Any]] = {}
_vectorstore_lock = threading.Lock()

def build_product_vector_store(product: Dict[str, Any]) -> Optional[FAISS]:
    pid = product["id"]
    with _vectorstore_lock:
        if pid in _vectorstore_cache:
            return _vectorstore_cache[pid]
        text = extract_pdf_text(product["pdf_path"])
        if not text or len(text.strip()) < 50:
            return None
        doc = Document(page_content=text, metadata={
            "product_id": pid,
            "product_name": product["name"],
            "reaction_type": product["reaction_type"],
            "source": product["pdf_path"]
        })
        vs = FAISS.from_documents([doc], cached_embeddings)
        _vectorstore_cache[pid] = vs
        return vs

def parse_structured_response(text: str) -> Dict[str, Any]:
    """
    Parse the LLM output (which we instructed to use literal '### API Name', etc.)
    Return JSON with keys: api_name, reaction_chemistry, yield, procedure, tables (list)
    Each table is {headers: [], rows: [[], [], ...]}
    """
    result = {"raw": text, "api_name": None, "reaction_chemistry": None, "yield": None, "procedure": None, "tables": []}
    def grab(section):
        m = re.search(rf"###\s*{re.escape(section)}\s*(.*?)\s*(?=###\s*\w+|\Z)", text, re.DOTALL | re.IGNORECASE)
        return m.group(1).strip() if m else None

    result["api_name"] = grab("API Name")
    result["reaction_chemistry"] = grab("Reaction Chemistry")
    result["yield"] = grab("Yield")
    result["procedure"] = grab("Procedure")
    tab_raw = grab("Tabular Data")
    if tab_raw:
        # find markdown tables starting with |...|
        table_patterns = re.findall(r"(\|[^\n]*\|\s*\n\|[-:\s|]*\|\s*\n(?:\|[^\n]*\|\s*\n?)*)", tab_raw, re.DOTALL)
        if table_patterns:
            for tbl_md in table_patterns:
                # normalize lines
                lines = [ln.strip().strip("|").strip() for ln in tbl_md.splitlines() if ln.strip()]
                if len(lines) >= 2:
                    # header line and separator
                    header = [h.strip() for h in lines[0].split("|")]
                    rows = []
                    for rowline in lines[2:]:
                        cols = [c.strip() for c in rowline.split("|")]
                        rows.append(cols)
                    result["tables"].append({"headers": header, "rows": rows, "raw_md": tbl_md})
        else:
            # fallback: provide the raw block
            result["tables"].append({"headers": [], "rows": [], "raw_md": tab_raw})
    return result

# ------------------------------
# FastAPI app
# ------------------------------
app = FastAPI(title="Reaction Database AI (FastAPI)")

# Allow the React dev server origin (adjust if you host differently)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # add your host/origin if different
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ------------------------------
# Pydantic models
# ------------------------------
class ProductOut(BaseModel):
    id: str
    name: str
    reaction_type: str
    has_scheme_image: bool
    has_scheme_cdx: bool

class QARequest(BaseModel):
    product_id: Optional[str] = None
    question: str = "Extract API Name, Reaction Chemistry, Yield, Procedure, and Tabular Data"

class QueryRequest(BaseModel):
    product_ids: List[str]
    question: str


# ------------------------------
# Endpoints
# ------------------------------
@app.get("/reactions", response_model=List[str])
def get_reactions():
    return REACTION_TYPES

@app.get("/products", response_model=List[ProductOut])
def get_products(reaction_type: Optional[str] = None):
    allp = list_products()
    if reaction_type:
        allp = [p for p in allp if p["reaction_type"] == reaction_type]
    out = []
    for p in allp:
        out.append(ProductOut(
            id=p["id"],
            name=p["name"],
            reaction_type=p["reaction_type"],
            has_scheme_image=bool(p["scheme_image"]),
            has_scheme_cdx=bool(p["scheme_cdx"])
        ))
    return out

@app.get("/product/{product_id}/meta")
def product_meta(product_id: str):
    products = list_products()
    for p in products:
        if p["id"] == product_id:
            return {
                "id": p["id"],
                "name": p["name"],
                "reaction_type": p["reaction_type"],
                "pdf_path": p["pdf_path"],
                "scheme_image": p["scheme_image"],
                "scheme_cdx": p["scheme_cdx"]
            }
    raise HTTPException(status_code=404, detail="Product not found")

@app.get("/product/{product_id}/pdf")
def product_pdf(product_id: str):
    meta = product_meta(product_id)
    pdf_path = meta["pdf_path"]
    if os.path.exists(pdf_path):
        return FileResponse(pdf_path, media_type="application/pdf", filename=os.path.basename(pdf_path))
    raise HTTPException(status_code=404, detail="PDF not found")

@app.get("/product/{product_id}/scheme-image")
def product_scheme_image(product_id: str):
    meta = product_meta(product_id)
    path = meta.get("scheme_image")
    if path and os.path.exists(path):
        return FileResponse(path, media_type="image/png", filename=os.path.basename(path))
    raise HTTPException(status_code=404, detail="Scheme image not found")

@app.post("/product/details")
def product_details(req: QARequest):
    """
    Robust product/details endpoint with strong product-name detection and detailed server-side debugging prints.

    Behavior:
      - If req.product_id provided:
          * If question == canonical extraction -> run extraction prompt and return parsed structured data (cached).
          * Else -> run retrieval QA on that product and return {"answer":..., "sources":[...]}.
      - If req.product_id not provided:
          * Attempt to detect product name inside question by:
              1) normalized alnum substring match (prefer long names),
              2) token overlap check,
              3) similarity ratio fallback (SequenceMatcher).
          * If a product is detected -> build retriever on that product PDF and run QA (returns answer + sources).
          * Otherwise fallback to generative model response (no retriever).
    NOTE: This function prints detailed information to the backend logs for debugging detection issues.
    """
    try:
        q_text = (req.question or "").strip()
        if not q_text:
            return JSONResponse(status_code=400, content={"error": "question is required"})

        # canonical extraction text (case-insensitive)
        CANONICAL_EXTRACTION = "extract api name, reaction chemistry, yield, procedure, and tabular data"

        # helper: normalize to alphanumeric lowercase
        def _normalize_alnum(s: str) -> str:
            return re.sub(r"[^a-z0-9]", "", (s or "").lower())

        # helper: tokenized set (alnum tokens)
        def _tokens(s: str):
            return [t for t in re.split(r'[^a-z0-9]+', (s or "").lower()) if t]

        # BEGIN DEBUG LOG
        print("=== /product/details called ===")
        print("Question:", q_text)
        # END DEBUG LOG

        # helper to detect product by multiple strategies
        def _detect_product_by_name(question: str) -> Optional[Dict[str, Any]]:
            products = list_products()
            if not products:
                print("DEBUG: No products available to match.")
                return None

            q_norm_alnum = _normalize_alnum(question)
            q_tokens = set(_tokens(question))

            # 1) Exact normalized substring match (prefer longer product names)
            sorted_products = sorted(products, key=lambda p: len(p["name"]), reverse=True)
            exact_matches = []
            for p in sorted_products:
                name_norm = _normalize_alnum(p["name"])
                if not name_norm:
                    continue
                if name_norm in q_norm_alnum:
                    exact_matches.append((p, name_norm))
            if exact_matches:
                chosen = exact_matches[0][0]
                print(f"DEBUG: Exact normalized substring match -> '{chosen['name']}' (id: {chosen['id']})")
                return chosen

            # 2) Token overlap heuristic: count how many name tokens appear in question tokens
            token_matches = []
            for p in sorted_products:
                pname_tokens = set(_tokens(p["name"]))
                if not pname_tokens:
                    continue
                overlap = pname_tokens.intersection(q_tokens)
                if overlap:
                    token_matches.append((p, len(overlap), len(pname_tokens), overlap))
            if token_matches:
                # prefer highest overlap ratio (over name token count), then more tokens matched
                token_matches.sort(key=lambda x: (-(x[1] / x[2]), -x[1]))
                best = token_matches[0]
                p_best, match_count, token_count, overlap = best
                ratio = match_count / token_count
                print(f"DEBUG: Token-overlap candidate -> '{p_best['name']}' (id: {p_best['id']}) overlap {match_count}/{token_count} tokens {overlap} ratio={ratio:.2f}")
                # require at least 50% token overlap to be confident
                if ratio >= 0.5:
                    return p_best

            # 3) Similarity fallback: pick best SequenceMatcher ratio over product.name vs question
            best = None
            best_ratio = 0.0
            for p in products:
                pname = (p["name"] or "").lower()
                if not pname.strip():
                    continue
                # Compute two similarity scores and average them:
                # a) between full names (lower) and question
                r1 = SequenceMatcher(None, pname, question.lower()).ratio()
                # b) between normalized alnum strings (robust to spacing/punctuation)
                r2 = SequenceMatcher(None, _normalize_alnum(pname), q_norm_alnum).ratio()
                ratio = (r1 + r2) / 2.0
                if ratio > best_ratio:
                    best_ratio = ratio
                    best = (p, ratio, r1, r2)
            # log top candidate
            if best:
                p_best, ratio, r1, r2 = best
                print(f"DEBUG: Best similarity candidate -> '{p_best['name']}' (id: {p_best['id']}) ratio_avg={ratio:.3f} r1={r1:.3f} r2={r2:.3f}")
                # Apply a conservative threshold to avoid false positives
                if ratio >= 0.60:
                    return p_best
                else:
                    print(f"DEBUG: Best similarity below threshold (0.60): {ratio:.3f} -> will NOT auto-select")
            return None

        # helper: run retrieval QA over a given product and return {answer, sources}
        def _run_retrieval_for_product(product: Dict[str, Any], question: str, k: int = 3) -> Dict[str, Any]:
            print(f"DEBUG: Running retrieval for product: {product['name']} (id: {product['id']})")
            pdf_path = product.get("pdf_path")
            if not pdf_path or not os.path.exists(pdf_path):
                raise HTTPException(status_code=404, detail=f"PDF not found for product {product.get('id')}: {pdf_path}")

            # extract text and verify
            try:
                text = extract_pdf_text(pdf_path)
            except Exception as e:
                print(f"ERROR: extract_pdf_text failed for {pdf_path}: {e}")
                raise HTTPException(status_code=500, detail=f"Failed to extract text: {e}")

            if not text or len(text.strip()) < 20:
                raise HTTPException(status_code=500, detail="Document content empty or unreadable")

            doc = Document(page_content=text, metadata={
                "product_id": product["id"],
                "product_name": product["name"],
                "reaction_type": product["reaction_type"],
                "source": pdf_path
            })

            try:
                vs = FAISS.from_documents([doc], cached_embeddings)
            except Exception as e:
                print(f"ERROR: FAISS.from_documents failed: {e}")
                raise HTTPException(status_code=500, detail=f"Failed building FAISS index: {e}")

            retriever = vs.as_retriever(search_kwargs={"k": k})

            # choose QA prompt (prefer QA_PROMPT)
            try:
                prompt_to_use = QA_PROMPT
            except NameError:
                # fallback to PROMPT or EXTRACTION_PROMPT
                prompt_to_use = globals().get("PROMPT") or globals().get("EXTRACTION_PROMPT")
                print("DEBUG: QA_PROMPT not found, fallback to available prompt.")

            qa_chain = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt_to_use},
                return_source_documents=True,
            )

            out = qa_chain({"query": question})
            answer_text = out.get("result") or out.get("output_text") or ""
            source_docs = out.get("source_documents", []) or []

            # build unique ordered sources
            seen = set()
            sources = []
            for sd in source_docs:
                pid = sd.metadata.get("product_id")
                pname = sd.metadata.get("product_name")
                if pid and pid not in seen:
                    seen.add(pid)
                    sources.append({"product_id": pid, "product_name": pname})
            print(f"DEBUG: Retrieval produced answer length={len(answer_text)} and {len(sources)} source(s)")
            return {"answer": answer_text, "sources": sources}

        # -------------------------
        # Main branching logic
        # -------------------------

        # 1) explicit product_id path (keep prior behavior)
        if req.product_id:
            products = list_products()
            product = next((p for p in products if p["id"] == req.product_id), None)
            if not product:
                return JSONResponse(status_code=404, content={"error": "Product not found"})

            is_extraction = q_text.strip().lower() == CANONICAL_EXTRACTION

            if not is_extraction:
                # question about this product -> run retrieval QA
                return _run_retrieval_for_product(product, q_text, k=3)

            # structured extraction -> cached or run
            if req.product_id in _product_details_cache:
                print("DEBUG: returning cached parsed details")
                return _product_details_cache[req.product_id]

            vs = build_product_vector_store(product)
            if not vs:
                return JSONResponse(status_code=500, content={"error": "Failed to build vector store (empty/invalid PDF)"})

            retriever = vs.as_retriever(search_kwargs={"k": 1})
            # extraction prompt chooser
            try:
                prompt_for_extraction = EXTRACTION_PROMPT
            except NameError:
                prompt_for_extraction = globals().get("PROMPT")
            if not prompt_for_extraction:
                return JSONResponse(status_code=500, content={"error": "No extraction prompt configured on the server."})

            qa_chain = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt_for_extraction},
                return_source_documents=False,
            )

            raw_response = qa_chain.run(q_text)
            parsed = parse_structured_response(raw_response)
            _product_details_cache[req.product_id] = parsed
            print("DEBUG: structured extraction completed and cached")
            return parsed

        # 2) no explicit product_id: attempt detection from free-text
        detected = _detect_product_by_name(q_text)
        if detected:
            # IMPORTANT: run retrieval on detected product (this was the missing step previously)
            print(f"DEBUG: Detected product by name: {detected['name']} (id: {detected['id']}) -> running retrieval")
            return _run_retrieval_for_product(detected, q_text, k=3)

        # 3) fallback: generative response (concise QA prompt)
        print("DEBUG: No product detected in question -> falling back to generative QA (no retriever)")
        try:
            prompt_for_gen = QA_PROMPT
        except NameError:
            prompt_for_gen = globals().get("PROMPT") or globals().get("EXTRACTION_PROMPT")

        qa_chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=None,
            chain_type_kwargs={"prompt": prompt_for_gen},
            return_source_documents=False,
        )

        raw_response = qa_chain.run(q_text)
        return {"response": raw_response}

    except HTTPException as he:
        # raise HTTP exceptions normally
        raise he
    except Exception as e:
        tb = traceback.format_exc()
        print("=== /product/details ERROR ===")
        print(tb)
        trace_lines = tb.splitlines()[-30:]
        return JSONResponse(status_code=500, content={
            "error": "Internal server error in /product/details",
            "message": str(e),
            "trace": trace_lines
        })

@app.post("/query")
def query_documents(req: QueryRequest):
    """
    Query multiple product documents (product_ids) and return an answer + sources.
    """
    if not req.product_ids:
        raise HTTPException(status_code=400, detail="product_ids must be a non-empty list")
    if not req.question or not req.question.strip():
        raise HTTPException(status_code=400, detail="question required")

    # Collect documents
    docs = []
    for pid in req.product_ids:
        try:
            meta = product_meta(pid)
        except HTTPException:
            continue
        pdf_path = meta.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            continue
        text = extract_pdf_text(pdf_path)
        if not text or len(text.strip()) < 50:
            continue
        doc = Document(
            page_content=text,
            metadata={
                "product_id": pid,
                "product_name": meta.get("name", ""),
                "reaction_type": meta.get("reaction_type", "")
            }
        )
        docs.append(doc)

    if len(docs) == 0:
        raise HTTPException(status_code=404, detail="No documents available for the provided product_ids")

    # Build FAISS on-the-fly (safe for moderate numbers of docs)
    try:
        vector_store = FAISS.from_documents(docs, cached_embeddings)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed building vector index: {e}")

    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    qa_chain = RetrievalQA.from_chain_type(
        llm=chat_model,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": QA_PROMPT},
        return_source_documents=True
    )

    try:
        out = qa_chain({"query": req.question})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"QA execution failed: {e}")

    answer_text = out.get("result") or out.get("output_text") or str(out)
    source_docs = out.get("source_documents", [])

    # Build sources: unique product ids preserving order
    seen = set()
    sources = []
    for sd in source_docs:
        pid = sd.metadata.get("product_id")
        pname = sd.metadata.get("product_name")
        if pid and pid not in seen:
            seen.add(pid)
            sources.append({"product_id": pid, "product_name": pname})

    return {"answer": answer_text, "sources": sources}

@app.get("/products/search")
def search_products(q: str = "", limit: int = 10):
    """
    Return up to `limit` products that match the query `q`.
    Matching strategy:
      1. prefix matches on normalized name (best)
      2. substring matches on normalized name (fallback)
    Normalization: lowercased, non-alphanumeric removed (so 'Prod-1' matches 'prod1', 'Prod 1', etc).
    """
    if q is None:
        q = ""
    q_norm = re.sub(r'[^a-z0-9]', '', q.lower())
    if q_norm == "":
        # return first N products as a fallback (stable order)
        prods = list_products()[:limit]
        return [{"id": p["id"], "name": p["name"], "reaction_type": p["reaction_type"]} for p in prods]

    products = list_products()
    # collect prefix matches first (longer names first)
    prefix_matches = []
    substring_matches = []
    for p in products:
        name_norm = re.sub(r'[^a-z0-9]', '', p["name"].lower())
        if name_norm.startswith(q_norm):
            prefix_matches.append(p)
        elif q_norm in name_norm:
            substring_matches.append(p)

    # sort prefix matches by name length (prefer longer exact names)
    prefix_matches = sorted(prefix_matches, key=lambda x: -len(x["name"]))
    substring_matches = sorted(substring_matches, key=lambda x: -len(x["name"]))

    combined = prefix_matches + substring_matches
    combined = combined[:limit]

    return [{"id": p["id"], "name": p["name"], "reaction_type": p["reaction_type"]} for p in combined]

# ------------------------------
# Transcription endpoint (optional Google Cloud)
# ------------------------------
# If you want server-side Google STT you can paste your credentials JSON string into
# GOOGLE_CREDENTIALS_JSON below (NOT recommended for public repos). For demos, prefer browser Web Speech API.
GOOGLE_CREDENTIALS_JSON = ""  # <<< If you want to enable server-side Google STT, paste credentials JSON string here.

@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...), use_google: Optional[bool] = Form(False)):
    """
    Accept an audio file (wav/webm/mp3) and return a transcript.
    By default this endpoint will only provide a simple error unless GOOGLE_CREDENTIALS_JSON is provided.
    The recommended demo approach is to use the browser Web Speech API for instant transcripts.
    """
    content = await file.read()
    if not use_google:
        # We recommend using browser speech API â€” but if user uploaded audio anyway, we can attempt a naive fallback:
        # Try to return an error telling front-end to use browser API.
        return {"error": "Server-side transcription disabled. Use browser Web Speech API for demo, or set use_google=True and provide GOOGLE_CREDENTIALS_JSON in main.py."}

    # If user wants Google transcription server-side:
    if not GOOGLE_CREDENTIALS_JSON:
        return {"error": "GOOGLE_CREDENTIALS_JSON not configured in main.py. Paste credentials JSON string into file to enable."}

    # Server-side Google Cloud Speech-to-Text usage (if enabled)
    try:
        from google.cloud import speech_v1p1beta1 as speech
        from google.oauth2 import service_account
    except Exception as e:
        return {"error": f"google-cloud-speech library not installed: {e}"}

    # Create credentials from JSON string (in-memory)
    creds_info = json.loads(GOOGLE_CREDENTIALS_JSON)
    credentials = service_account.Credentials.from_service_account_info(creds_info)
    client = speech.SpeechClient(credentials=credentials)

    audio = speech.RecognitionAudio(content=content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="en-US",
        enable_automatic_punctuation=True,
    )

    response = client.recognize(config=config, audio=audio)
    transcripts = [r.alternatives[0].transcript for r in response.results]
    return {"transcript": " ".join(transcripts)}

# ------------------------------
# Health
# ------------------------------
@app.get("/health")
def health():
    return {"status": "ok"}


# ðŸ” JWT Token Creation Function
def create_access_token(data: dict, expires_delta: Union[timedelta , None] = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

# ðŸ”“ Login Endpoint
@app.post("/login")
async def login(user_info: dict = Depends(config.check_ldap_auth)):
    if not user_info:
        raise HTTPException(status_code=400, detail="Invalid Credentials")

    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user_info["first_name"]},
        expires_delta=access_token_expires
    )

    return {
        "access_token": access_token,
        "token_type": "Bearer",
        "firstname": user_info["first_name"],
        "lastname": user_info["last_name"],
    }

#############################################################################
#SA
import os
import streamlit as st
import pandas as pd
import numpy as np
from zipfile import ZipFile
from scipy.stats import skew, kurtosis, norm, probplot
from scipy import stats
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# ----------------------------------------
# Required Helper Functions
# ----------------------------------------

def prepare_data(reference_df, test_df):
    """Remove time zero if present and reset index"""
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.iloc[1:].reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.iloc[1:].reset_index(drop=True)
    return reference_df, test_df

def conventional_f2(ref_means, test_means):
    """Calculate conventional f2"""
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    p = len(ref_means)
    return 50 if p == 0 else 100 - 25 * np.log10(1 + (1/p) * sum_sq_diff)

def expected_f2(ref_df, test_df):
    """Calculate expected f2"""
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    
    # Conventional f2 component
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    
    # Variance components
    ref_var = row_variance(ref_df)
    test_var = row_variance(test_df)
    sum_var = (ref_var + test_var).sum()
    
    n = ref_df.shape[1] - 1  # Number of units per time point
    p = len(ref_means)
    
    adjustment = (1/n) * sum_var
    return 100 - 25 * np.log10(1 + (1/p) * (sum_sq_diff + adjustment))

def bias_corrected_f2(ref_df, test_df):
    """Calculate bias-corrected f2"""
    try:
        ref_means = ref_df.iloc[:, 1:].mean(axis=1)
        test_means = test_df.iloc[:, 1:].mean(axis=1)

        diff = test_means - ref_means
        sum_sq_diff = (diff ** 2).sum()

        ref_var = row_variance(ref_df)
        test_var = row_variance(test_df)
        sum_var = (ref_var + test_var).sum()

        n = ref_df.shape[1] - 1
        p = len(ref_means)

        adjustment = (1 / n) * sum_var
        right_side = sum_sq_diff + p

        if adjustment < right_side:
            adjusted_diff = sum_sq_diff - adjustment
            if adjusted_diff > 0:
                return 100 - 25 * np.log10(1 + (1 / p) * adjusted_diff)
            else:
                return None
        else:
            return None

    except Exception:
        return None

def row_variance(df):
    """Calculate row-wise variance"""
    return df.iloc[:, 1:].var(axis=1, ddof=1)

def bootstrap_f2(ref_df, test_df, calc_func, n_iterations=10000):
    """Bootstrap f2 calculation (returns mean, median, skewness, and kurtosis)."""
    n_ref_units = ref_df.shape[1] - 1
    n_test_units = test_df.shape[1] - 1

    original_f2 = calc_func(ref_df, test_df)
    f2_values = []
    
    for _ in range(n_iterations):
        ref_sample_idx = np.random.choice(range(1, ref_df.shape[1]), n_ref_units, replace=True)
        test_sample_idx = np.random.choice(range(1, test_df.shape[1]), n_test_units, replace=True)

        ref_sample = ref_df.iloc[:, [0] + list(ref_sample_idx)]
        test_sample = test_df.iloc[:, [0] + list(test_sample_idx)]

        f2_val = calc_func(ref_sample, test_sample)

        if f2_val is not None and isinstance(f2_val, (int, float)):
            f2_values.append(f2_val)
    
    if not f2_values:
        return original_f2, None, None, None, None, None, None, []
    
    f2_values = np.array(f2_values)
    mean_f2 = np.mean(f2_values)
    median_f2 = np.median(f2_values)
    skewness_f2 = skew(f2_values)
    kurtosis_f2 = kurtosis(f2_values)
    lower_bound = np.percentile(f2_values, 5)
    upper_bound = np.percentile(f2_values, 95)

    return original_f2, lower_bound, upper_bound, mean_f2, median_f2, skewness_f2, kurtosis_f2, f2_values

def simulate_lilliefors_critical_value(n, alpha=0.05, num_simulations=1000):
    """Simulate Lilliefors critical value for confidence bands"""
    D_values = []
    for _ in range(num_simulations):
        sample = np.random.normal(loc=0, scale=1, size=n)
        sample_mean = np.mean(sample)
        sample_std = np.std(sample, ddof=1)
        z_scores = (sample - sample_mean) / sample_std
        z_scores.sort()
        ecdf_sim = np.arange(1, n + 1) / (n + 1)
        cdf = norm.cdf(z_scores)
        D = np.max(np.abs(ecdf_sim - cdf))
        D_values.append(D)
    return np.percentile(D_values, 100 * (1 - alpha))

def create_jmp_style_qq_plot(data, title, method_name, file_name):
    """NA"""
    if len(data) == 0:
        return None
    
    n = len(data)
    
    # Sort data and calculate plotting positions
    sorted_data = np.sort(data)
    plotting_positions = norm.ppf((np.arange(1, n + 1)) / (n + 2))
    
    # Fit line for normal distribution
    slope, intercept, r, _, _ = stats.linregress(plotting_positions, sorted_data)
    fit_line = intercept + slope * plotting_positions
    
    # Calculate empirical CDF
    ecdf = (np.arange(1, n + 1)) / (n + 1)
    
    # Get Lilliefors critical value for confidence bands
    D_critical = simulate_lilliefors_critical_value(n=n, alpha=0.05)
    
    # Calculate confidence bands using ECDF approach
    lower_quantile = norm.ppf(np.clip(ecdf - D_critical, 1e-10, 1 - 1e-10))
    upper_quantile = norm.ppf(np.clip(ecdf + D_critical, 1e-10, 1 - 1e-10))
    
    # Transform quantile bands to data space using the fitted line
    lower_band = intercept + slope * lower_quantile
    upper_band = intercept + slope * upper_quantile
    
    # Create the plot
    fig = go.Figure()
    
    # Add curved confidence bands first (so they appear behind)
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=upper_band,
        mode='lines',
        name='Upper Confidence Band',
        line=dict(color='red', width=1, dash='dot'),
        showlegend=False,
        hoverinfo='skip'
    ))
    
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=lower_band,
        mode='lines',
        name='Lower Confidence Band', 
        line=dict(color='red', width=1, dash='dot'),
        showlegend=False,
        hoverinfo='skip'
    ))
    
    # Add the main reference line (solid red)
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=fit_line,
        mode='lines',
        name='Normal Reference Line',
        line=dict(color='red', width=2, dash='solid'),
        showlegend=False,
        hovertemplate='Normal Reference Line<extra></extra>'
    ))
    
    # Add the data points (black circles)
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=sorted_data,
        mode='markers',
        name='Data Points',
        marker=dict(
            color='black',
            size=6,
            symbol='circle',
            line=dict(width=0)
        ),
        showlegend=False,
        hovertemplate=
        '<b>Normal Quantile:</b> %{x:.3f}<br>' +
        '<b>Sample Quantile:</b> %{y:.3f}<br>' +
        '<extra></extra>'
    ))
    
    # Calculate nice axis ranges
    x_range = max(plotting_positions) - min(plotting_positions)
    y_range = max(sorted_data) - min(sorted_data)
    x_margin = x_range * 0.1
    y_margin = y_range * 0.1
    
    # Set up axis ticks similar to JMP
    x_ticks = [-2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]
    x_ticks = [tick for tick in x_ticks if min(plotting_positions)-x_margin <= tick <= max(plotting_positions)+x_margin]
    
    # Calculate statistics for annotation
    mean_val = np.mean(data)
    std_val = np.std(data)
    skewness_val = skew(data)
    kurtosis_val = kurtosis(data)
    
    
    fig.update_layout(
        title=dict(
            text=f'<b>Normal Quantile Plot</b><br><span style="font-size: 12px;">{method_name} - {file_name.replace(".xlsx", "")}</span>',
            x=0.5,
            y=0.95,
            font=dict(size=14, color='black', family='Arial')
        ),
        xaxis=dict(
            title=dict(
                text='<b>Normal Quantile</b>',
                font=dict(size=12, color='black', family='Arial')
            ),
            range=[min(plotting_positions)-x_margin, max(plotting_positions)+x_margin],
            tickvals=x_ticks,
            tickmode='array',
            gridcolor='lightgray',
            gridwidth=0.5,
            showgrid=True,
            zeroline=True,
            zerolinecolor='lightgray',
            zerolinewidth=0.8,
            tickfont=dict(size=10, color='black', family='Arial'),
            linecolor='black',
            linewidth=1,
            mirror=True
        ),
        yaxis=dict(
            title=dict(
                text='<b>Sample Quantile</b>',
                font=dict(size=12, color='black', family='Arial')
            ),
            range=[min(sorted_data)-y_margin, max(sorted_data)+y_margin],
            gridcolor='lightgray',
            gridwidth=0.5,
            showgrid=True,
            zeroline=True,
            zerolinecolor='lightgray',
            zerolinewidth=0.8,
            tickfont=dict(size=10, color='black', family='Arial'),
            linecolor='black',
            linewidth=1,
            mirror=True
        ),
        plot_bgcolor='white',
        paper_bgcolor='white',
        font=dict(family="Arial, sans-serif"),
        hovermode='closest',
        width=600,
        height=500,
        margin=dict(l=80, r=100, t=80, b=60),
        showlegend=False
    )
    
    
    stats_text = (
        f'<b>Statistics:</b><br>'
        f'Mean: {mean_val:.4f}<br>'
        f'Std Dev: {std_val:.4f}<br>'
        f'Skewness: {skewness_val:.4f}<br>'
        f'Kurtosis: {kurtosis_val:.4f}<br>'
        f'RÂ²: {r**2:.4f}<br>'
        f'N: {n:,}'
    )
    
    fig.add_annotation(
        x=0.98,
        y=0.98,
        xref="paper",
        yref="paper",
        text=stats_text,
        showarrow=False,
        align="left",
        bgcolor="white",
        bordercolor="black",
        borderwidth=1,
        borderpad=6,
        font=dict(size=9, color='black', family='Arial')
    )
    
    return fig

def create_combined_qq_plots(bootstrap_results, file_name):
    """Create a combined view of all QQ plots for a file"""
    methods_with_data = [(method, data) for method, data in bootstrap_results.items() if len(data) > 0]
    
    if not methods_with_data:
        return None
    
    n_methods = len(methods_with_data)
    cols = min(3, n_methods)
    rows = (n_methods + cols - 1) // cols
    
    subplot_titles = [method for method, _ in methods_with_data]
    
    fig = make_subplots(
        rows=rows,
        cols=cols,
        subplot_titles=subplot_titles,
        vertical_spacing=0.12,
        horizontal_spacing=0.1
    )
    
    colors = ['#3498DB', '#E74C3C', '#2ECC71', '#F39C12', '#9B59B6', '#1ABC9C']
    
    for idx, (method, data) in enumerate(methods_with_data):
        row = idx // cols + 1
        col = idx % cols + 1
        color = colors[idx % len(colors)]
        
        # Calculate QQ plot data
        (osm, osr), (slope, intercept, r) = probplot(data, dist="norm", plot=None)
        line_x = np.linspace(osm.min(), osm.max(), 100)
        line_y = slope * line_x + intercept
        
        # Add reference line
        fig.add_trace(
            go.Scatter(
                x=line_x,
                y=line_y,
                mode='lines',
                line=dict(color='#E74C3C', width=2),
                showlegend=False,
                hoverinfo='skip'
            ),
            row=row, col=col
        )
        
        # Add data points
        fig.add_trace(
            go.Scatter(
                x=osm,
                y=osr,
                mode='markers',
                marker=dict(
                    color=color,
                    size=6,
                    opacity=0.7,
                    line=dict(width=1, color=color)
                ),
                showlegend=False,
                hovertemplate=f'<b>{method}</b><br>Theoretical: %{{x:.3f}}<br>Sample: %{{y:.3f}}<extra></extra>'
            ),
            row=row, col=col
        )
    
    fig.update_layout(
        title=dict(
            text=f'<b>Normal Quantile Plots Comparison</b><br><sub>{file_name}</sub>',
            x=0.5,
            font=dict(size=16, color='#2C3E50')
        ),
        plot_bgcolor='white',
        paper_bgcolor='white',
        font=dict(family="Arial, sans-serif"),
        height=300 * rows,
        margin=dict(l=60, r=40, t=80, b=40)
    )
    
    fig.update_xaxes(
        title_text="Normal Quantile",
        gridcolor='#ECF0F1',
        showgrid=True,
        tickfont=dict(size=9)
    )
    
    fig.update_yaxes(
        title_text="f2 Value",
        gridcolor='#ECF0F1',
        showgrid=True,
        tickfont=dict(size=9)
    )
    
    return fig

# ----------------------------------------
# Batch Processing Functions
# ----------------------------------------

def load_batch_data(folder_path):
    """Load test and reference data from multiple Excel files in a folder."""
    workbook_data = {}
    
    for file_name in os.listdir(folder_path):
        if file_name.endswith(".xlsx"):
            file_path = os.path.join(folder_path, file_name)
            try:
                reference_df = pd.read_excel(file_path, sheet_name=0)
                test_df = pd.read_excel(file_path, sheet_name=1)
                workbook_data[file_name] = (reference_df, test_df)
            except Exception as e:
                st.warning(f"Skipping file `{file_name}` due to error: {e}")
    
    return workbook_data

def process_batch(workbook_data, methods, n_iterations=10000):
    """Process multiple workbooks and calculate f2 metrics."""
    all_results = []
    bootstrap_data = {}  # Store bootstrap data for QQ plots
    
    for file_name, (reference_df, test_df) in workbook_data.items():
        try:
            ref_clean, test_clean = prepare_data(reference_df, test_df)
            
            results = {"File Name": file_name}
            file_bootstrap_data = {}
            
            # Bootstrap calculation methods
            if "Conventional Bootstrap" in methods:
                def conv_func(r, t): 
                    return conventional_f2(r.iloc[:, 1:].mean(axis=1), t.iloc[:, 1:].mean(axis=1))
                orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                    ref_clean, test_clean, conv_func, n_iterations)
                results["Conventional Bootstrap f2"] = orig
                results["Conventional Bootstrap CI"] = f"{lower:.2f} - {upper:.2f}" if lower and upper else None
                results["Conventional Bootstrap Mean"] = mean
                results["Conventional Bootstrap Median"] = median
                results["Conventional Bootstrap Skewness"] = skewness
                results["Conventional Bootstrap Kurtosis"] = kurt
                file_bootstrap_data["Conventional Bootstrap"] = vals

            if "Expected Bootstrap" in methods:
                orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                    ref_clean, test_clean, expected_f2, n_iterations)
                results["Expected Bootstrap f2"] = orig
                results["Expected Bootstrap CI"] = f"{lower:.2f} - {upper:.2f}" if lower and upper else None
                results["Expected Bootstrap Mean"] = mean
                results["Expected Bootstrap Median"] = median
                results["Expected Bootstrap Skewness"] = skewness
                results["Expected Bootstrap Kurtosis"] = kurt
                file_bootstrap_data["Expected Bootstrap"] = vals

            if "Bias Corrected Bootstrap" in methods:
                def bc_func(r, t): 
                    bc = bias_corrected_f2(r, t)
                    return bc if isinstance(bc, float) else None
                orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                    ref_clean, test_clean, bc_func, n_iterations)
                results["Bias Corrected Bootstrap f2"] = orig
                results["Bias Corrected Bootstrap CI"] = f"{lower:.2f} - {upper:.2f}" if lower and upper else None
                results["Bias Corrected Bootstrap Mean"] = mean
                results["Bias Corrected Bootstrap Median"] = median
                results["Bias Corrected Bootstrap Skewness"] = skewness
                results["Bias Corrected Bootstrap Kurtosis"] = kurt
                file_bootstrap_data["Bias Corrected Bootstrap"] = vals
            
            all_results.append(results)
            bootstrap_data[file_name] = file_bootstrap_data
        
        except Exception as e:
            st.warning(f"Error processing file `{file_name}`: {e}")
    
    return pd.DataFrame(all_results), bootstrap_data

def create_zip_report(report_df, qq_plots_data=None):
    """Create a ZIP file containing the report CSV and QQ plots."""
    report_file = "f2_similarities_report.csv"
    zip_file = "f2_similarities_report.zip"
    
    report_df.to_csv(report_file, index=False)
    
    with ZipFile(zip_file, "w") as zipf:
        zipf.write(report_file)
        
        # Add QQ plots if available
        if qq_plots_data:
            for file_name, plots in qq_plots_data.items():
                # Create individual plots
                for method, fig in plots['individual'].items():
                    if fig:
                        plot_filename = f"QQ_Plot_{file_name.replace('.xlsx', '')}_{method.replace(' ', '_')}.html"
                        fig.write_html(plot_filename)
                        zipf.write(plot_filename)
                        os.remove(plot_filename)  # Clean up temp file
                
                # Add combined plot
                if plots['combined']:
                    combined_filename = f"QQ_Plot_Combined_{file_name.replace('.xlsx', '')}.html"
                    plots['combined'].write_html(combined_filename)
                    zipf.write(combined_filename)
                    os.remove(combined_filename)  # Clean up temp file
    
    # Clean up temp CSV file
    if os.path.exists(report_file):
        os.remove(report_file)
    
    return zip_file

def generate_qq_plots(bootstrap_data):
    """Generate all QQ plots and return them as a dictionary."""
    qq_plots_data = {}
    
    for file_name, file_bootstrap_data in bootstrap_data.items():
        plots = {
            'individual': {},
            'combined': None
        }
        
        # Generate individual plots
        for method, data in file_bootstrap_data.items():
            if len(data) > 0:
                fig = create_jmp_style_qq_plot(data, f"QQ Plot", method, file_name)
                plots['individual'][method] = fig
        
        # Generate combined plot
        combined_fig = create_combined_qq_plots(file_bootstrap_data, file_name)
        plots['combined'] = combined_fig
        
        qq_plots_data[file_name] = plots
    
    return qq_plots_data

# ----------------------------------------
# Streamlit App Code
# ----------------------------------------

def main():
    st.set_page_config(page_title="Batch Similarity Analyzer", layout="wide")
    st.title("Batch Similarity Analyzer")
    st.markdown("""
    This tool calculates f2 similarity for multiple Excel workbooks at once
    """)
    
    folder_path = st.text_input("Enter path to folder containing Excel files:", "")
    
    options = ["Conventional", "Expected", "Bias Corrected",
               "Conventional Bootstrap", "Expected Bootstrap", "Bias Corrected Bootstrap"]
    selected_methods = st.multiselect("Select calculation methods:", options, 
                                    default=["Conventional Bootstrap", "Expected Bootstrap", "Bias Corrected Bootstrap"])
    
    n_iterations = st.slider("Number of bootstrap iterations:", 1000, 50000, 10000, 1000)
    
    if st.button("Calculate and Generate Report"):
        if os.path.isdir(folder_path):
            with st.spinner("Processing files and generating QQ plots..."):
                workbook_data = load_batch_data(folder_path)
                if workbook_data:
                    # Check if any bootstrap methods are selected for QQ plots
                    bootstrap_methods = [m for m in selected_methods if "Bootstrap" in m]
                    
                    report_df, bootstrap_data = process_batch(workbook_data, selected_methods, n_iterations)
                    
                    st.subheader("ðŸ“Š Results Preview")
                    st.dataframe(report_df.head())
                    
                    # Generate QQ Plots for bootstrap methods
                    qq_plots_data = None
                    if bootstrap_methods and bootstrap_data:
                        with st.spinner("Generating QQ plots for download..."):
                            qq_plots_data = generate_qq_plots(bootstrap_data)
                            
                            # Show summary of generated plots
                            st.subheader("ðŸ“ˆ QQ Plot Generation Summary")
                            st.success("âœ… QQ plots have been generated successfully!")
                            
                            total_plots = 0
                            for file_name, plots in qq_plots_data.items():
                                individual_count = len([fig for fig in plots['individual'].values() if fig is not None])
                                combined_count = 1 if plots['combined'] is not None else 0
                                file_total = individual_count + combined_count
                                total_plots += file_total
                                
                                st.write(f"**{file_name}:** {individual_count} individual plots + {combined_count} combined plot = {file_total} plots")
                            
                            st.info(f"ðŸ“Š **Total QQ plots generated:** {total_plots}")
                            st.markdown("""
                            **QQ Plot Files Included in Download:**
                            - Individual method plots: `QQ_Plot_[FileName]_[Method].html`
                            - Ignore Combined comparison plots: `QQ_Plot_Combined_[FileName].html`
                
                            """)
                    
                    # Download report with QQ plots
                    with st.spinner("Creating downloadable ZIP file..."):
                        zip_file_path = create_zip_report(report_df, qq_plots_data)
                    
                    with open(zip_file_path, "rb") as f:
                        file_content = f.read()
                    
                    # Clean up the zip file
                    if os.path.exists(zip_file_path):
                        os.remove(zip_file_path)
                    
                    st.download_button(
                        label="ðŸ“¥ Download Complete Report with QQ Plots (ZIP)",
                        data=file_content,
                        file_name="f2_similarities_report_with_plots.zip",
                        mime="application/zip",
                        help="Download includes CSV report and all QQ plot HTML files"
                    )
                    
                    st.success("Report and QQ plots are ready for download!")
                    
                else:
                    st.warning("No valid Excel files found in selected folder.")
        else:
            st.error("Invalid folder path. Please check the path and try again.")
    
    # Add information section
    with st.expander("â„¹ï¸ About Similarity Analyzer"):
        st.markdown("NA")

if __name__ == "__main__":
    main()
