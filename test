import os
import io
import json
import pandas as pd
import logging
import speech_recognition as sr
import re
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional

from gtts import gTTS
from langchain_openai import AzureChatOpenAI
from langchain.prompts import ChatPromptTemplate

# ─── CONFIG ──────────────────────────────────────────────────────

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure GPT configuration (fill these in)
base_url=""
api_version="2024-02-15-preview"
api_key=""
deployment_name="GPT4o"
model_name="GPT4o"

# Paths to embedded JSON files
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Prime_M+F_JSON.json",
}

# Sample‑prompt tiles (for frontend to fetch)
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X":["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# ─── SETUP LLM ───────────────────────────────────────────────────

chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url,
    temperature=0
)

# ─── INTELLIGENT DATA LOADER ─────────────────────────────────────

def normalize_json_to_dataframe(data: Any) -> pd.DataFrame:
    """
    Convert JSON (list or single object) to a normalized DataFrame.
    Flattens nested structures for easy querying.
    """
    if isinstance(data, list):
        df = pd.json_normalize(data)
    else:
        df = pd.json_normalize([data])
    
    # Convert all column names to lowercase for case-insensitive search
    df.columns = [col.lower().replace('.', '_') for col in df.columns]
    
    return df

def load_data_as_dataframes(paths: Dict[str, str]) -> Dict[str, pd.DataFrame]:
    """Load JSON files and convert to DataFrames."""
    dataframes = {}
    
    for name, path in paths.items():
        if not path:
            continue
        try:
            logger.info(f"[Loading] Reading file: {name}")
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            df = normalize_json_to_dataframe(data)
            dataframes[name] = df
            
            logger.info(f"[Data] Loaded {len(df)} records with {len(df.columns)} columns from '{name}'")
            logger.info(f"[Data] Columns: {list(df.columns[:10])}...")
            
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    
    return dataframes

# ─── INTELLIGENT QUERY ROUTER ────────────────────────────────────

class QueryRouter:
    """
    Routes queries to the appropriate search strategy:
    - Exact search for IDs, codes, names
    - Filtered search for ranges, comparisons
    - Semantic search for complex natural language queries
    """
    
    def __init__(self, dataframes: Dict[str, pd.DataFrame]):
        self.dataframes = dataframes
        self.all_columns = self._get_all_columns()
    
    def _get_all_columns(self) -> List[str]:
        """Get all unique columns across all dataframes."""
        all_cols = set()
        for df in self.dataframes.values():
            all_cols.update(df.columns)
        return sorted(list(all_cols))
    
    def extract_search_terms(self, query: str) -> Dict[str, Any]:
        """Extract key search terms from query."""
        query_lower = query.lower()
        
        # Extract quoted terms (exact matches)
        quoted_terms = re.findall(r'"([^"]+)"', query)
        
        # Extract numbers (batch sizes, IDs, etc.)
        numbers = re.findall(r'\b\d+(?:\.\d+)?\b', query)
        
        # Extract comparison operators
        comparisons = {
            'greater_than': re.search(r'(?:greater than|more than|above|>)\s*(\d+(?:\.\d+)?)', query_lower),
            'less_than': re.search(r'(?:less than|below|under|<)\s*(\d+(?:\.\d+)?)', query_lower),
            'equals': re.search(r'(?:equals?|is|=)\s*(\d+(?:\.\d+)?)', query_lower)
        }
        
        return {
            'quoted_terms': quoted_terms,
            'numbers': numbers,
            'comparisons': {k: v.group(1) if v else None for k, v in comparisons.items()},
            'original_query': query
        }
    
    def find_relevant_columns(self, query: str) -> List[str]:
        """Find columns that might be relevant to the query."""
        query_lower = query.lower()
        relevant = []
        
        for col in self.all_columns:
            # Check if column name or parts of it appear in query
            col_parts = col.split('_')
            if any(part in query_lower for part in col_parts if len(part) > 2):
                relevant.append(col)
        
        return relevant
    
    def search_exact(self, df: pd.DataFrame, search_terms: Dict[str, Any]) -> pd.DataFrame:
        """Perform exact search on DataFrame."""
        result = df.copy()
        
        # Search for quoted terms in all text columns
        if search_terms['quoted_terms']:
            for term in search_terms['quoted_terms']:
                mask = pd.Series([False] * len(result))
                for col in result.select_dtypes(include=['object']).columns:
                    mask |= result[col].astype(str).str.contains(term, case=False, na=False)
                result = result[mask]
        
        # Search for numbers in numeric columns
        if search_terms['numbers']:
            for num in search_terms['numbers']:
                mask = pd.Series([False] * len(result))
                for col in result.select_dtypes(include=['number']).columns:
                    mask |= result[col] == float(num)
                if mask.any():
                    result = result[mask]
        
        return result
    
    def search_keyword(self, df: pd.DataFrame, keywords: List[str]) -> pd.DataFrame:
        """Search for keywords across all columns."""
        if not keywords:
            return df
        
        mask = pd.Series([False] * len(df))
        for keyword in keywords:
            for col in df.columns:
                mask |= df[col].astype(str).str.contains(keyword, case=False, na=False, regex=False)
        
        return df[mask]

# ─── SMART QUERY HANDLER ─────────────────────────────────────────

class SmartQueryHandler:
    """
    Handles queries intelligently:
    1. First tries direct DataFrame search (fast, accurate, no API calls)
    2. If results found, uses LLM to format answer from data
    3. Only uses LLM for complex reasoning, not for retrieval
    """
    
    def __init__(self, dataframes: Dict[str, pd.DataFrame], llm):
        self.router = QueryRouter(dataframes)
        self.dataframes = dataframes
        self.llm = llm
    
    def search_all_dataframes(self, query: str) -> tuple[pd.DataFrame, str]:
        """Search across all dataframes."""
        all_results = []
        source_info = []
        
        search_terms = self.router.extract_search_terms(query)
        
        # Extract keywords from query
        keywords = re.findall(r'\b[a-zA-Z]{3,}\b', query.lower())
        keywords = [k for k in keywords if k not in ['what', 'the', 'is', 'are', 'for', 'and', 'or']]
        
        for name, df in self.dataframes.items():
            # Try exact search first
            exact_results = self.router.search_exact(df, search_terms)
            
            if len(exact_results) > 0:
                all_results.append(exact_results)
                source_info.append(f"{name} (exact match)")
                logger.info(f"[Search] Found {len(exact_results)} exact matches in {name}")
                continue
            
            # Try keyword search
            keyword_results = self.router.search_keyword(df, keywords)
            
            if len(keyword_results) > 0:
                all_results.append(keyword_results.head(20))  # Limit to top 20
                source_info.append(f"{name} (keyword match)")
                logger.info(f"[Search] Found {len(keyword_results)} keyword matches in {name}")
        
        if all_results:
            combined_df = pd.concat(all_results, ignore_index=True)
            return combined_df, ", ".join(source_info)
        
        return pd.DataFrame(), "No sources"
    
    def format_dataframe_for_llm(self, df: pd.DataFrame, max_rows: int = 10) -> str:
        """Convert DataFrame to a readable format for LLM."""
        if df.empty:
            return "No data found."
        
        # Limit rows to prevent token overflow
        df_limited = df.head(max_rows)
        
        # Convert to dict format for clarity
        records = df_limited.to_dict('records')
        
        formatted = "Found Records:\n\n"
        for idx, record in enumerate(records, 1):
            formatted += f"Record {idx}:\n"
            for key, value in record.items():
                if pd.notna(value):
                    formatted += f"  {key}: {value}\n"
            formatted += "\n"
        
        if len(df) > max_rows:
            formatted += f"\n(Showing {max_rows} of {len(df)} total records)\n"
        
        return formatted
    
    def generate_answer(self, query: str, data: str, source: str) -> str:
        """Use LLM to generate answer from retrieved data."""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a precise data assistant. Answer the user's question using ONLY the data provided below.

CRITICAL RULES:
1. ONLY use information explicitly stated in the provided data
2. If the answer is in the data, provide specific details with exact values
3. If the data doesn't contain the answer, say "I don't have that information in the available data"
4. Never make up, infer, or assume information not in the data
5. Quote exact values when available (e.g., "Batch size: 500 kg")
6. If multiple records match, mention all relevant ones

Provided Data:
{data}

Source: {source}"""),
            ("human", "{question}")
        ])
        
        chain = prompt | self.llm
        response = chain.invoke({
            "question": query,
            "data": data,
            "source": source
        })
        
        return response.content
    
    def handle_query(self, query: str) -> str:
        """Main entry point for handling queries."""
        logger.info(f"[Query] Processing: '{query}'")
        
        # Search dataframes
        results_df, source = self.search_all_dataframes(query)
        
        if results_df.empty:
            return "I couldn't find any relevant information for your query. Please try rephrasing or ask about specific products, batches, or formulations in the dataset."
        
        # Format data for LLM
        data_context = self.format_dataframe_for_llm(results_df)
        
        # Generate answer using LLM
        answer = self.generate_answer(query, data_context, source)
        
        logger.info(f"[Query] Found {len(results_df)} results from {source}")
        
        return answer

# ─── INITIALIZE ON STARTUP ───────────────────────────────────────

logger.info("=" * 70)
logger.info("[Startup] Initializing Smart Query System (NO EMBEDDINGS NEEDED!)")
logger.info("=" * 70)

try:
    logger.info("[Startup] Loading data as DataFrames...")
    DATAFRAMES = load_data_as_dataframes(EMBEDDED_FILES)
    
    logger.info("[Startup] Initializing Smart Query Handler...")
    QUERY_HANDLER = SmartQueryHandler(DATAFRAMES, chat_model)
    
    recognizer = sr.Recognizer()
    
    logger.info("=" * 70)
    logger.info("[Startup] ✓ System ready! (Instant startup, no rate limits)")
    logger.info("=" * 70)
except Exception as e:
    logger.error("=" * 70)
    logger.error(f"[Startup] ✗ FATAL ERROR: {e}")
    logger.error("=" * 70)
    raise

# ─── FASTAPI APP ─────────────────────────────────────────────────

app = FastAPI(title="Smart Query System - Zero Hallucinations")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

# ─── ENDPOINTS ────────────────────────────────────────────────────

@app.get("/sample-tiles")
def sample_tiles():
    """Return the sample-prompt tiles and questions."""
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
def chat(req: ChatRequest):
    q = req.message.strip()
    if not q:
        raise HTTPException(400, "Empty query")
    logger.info(f"[API] /chat query='{q}'")
    
    try:
        ans = QUERY_HANDLER.handle_query(q)
        return {"response": ans}
    except Exception as e:
        logger.error(f"[API] Error processing query: {e}")
        return {"response": "I encountered an error processing your query. Please try rephrasing it."}

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    """Accepts an uploaded audio file and returns transcribed text."""
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"[API] STT error: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    """Returns an MP3 audio stream of the given text."""
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"[API] TTS error: {e}")
        raise HTTPException(500, str(e))

@app.get("/health")
def health():
    total_records = sum(len(df) for df in DATAFRAMES.values())
    return {
        "status": "ok",
        "total_records": total_records,
        "dataframes": {name: len(df) for name, df in DATAFRAMES.items()},
        "approach": "Direct DataFrame search (no embeddings)"
    }

@app.get("/stats")
def stats():
    """Get statistics about the loaded data."""
    stats_info = {}
    for name, df in DATAFRAMES.items():
        stats_info[name] = {
            "total_records": len(df),
            "columns": list(df.columns[:20]),
            "sample_data": df.head(2).to_dict('records') if len(df) > 0 else []
        }
    return stats_info

@app.get("/columns")
def get_columns():
    """Get all available columns across datasets."""
    all_cols = set()
    for df in DATAFRAMES.values():
        all_cols.update(df.columns)
    return {"columns": sorted(list(all_cols))}
