import numpy as np
import matplotlib
from scipy.stats import norm
from sklearn.utils import resample
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import warnings
from matplotlib.ticker import MaxNLocator
warnings.filterwarnings('ignore')
import time
from tqdm import tqdm
#R_regulation
# import rpy2
# import rpy2.robjects as robjects
# from rpy2.robjects.packages import importr, data
# utils = importr('utils')
# base = importr('base')
# bootf2 = importr('bootf2')
# read = importr('readxl')
# open = importr('writexl')
#interpolation
import itertools
from itertools import combinations
from scipy.interpolate import interp1d
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel

file_path= r"C:\Users\\Desktop\SA_TEST\RB_4.xlsx"
# "C:\Users\p00095189\Downloads\Sample_Template_DPS 1.xlsx"
reference_df =pd.read_excel(file_path, sheet_name=0)
test_df=pd.read_excel(file_path, sheet_name=1)
arrayboot=[]

np.random.seed(306)
reference_mean = reference_df.iloc[:, 1:].mean(axis=1)
test_mean = test_df.iloc[:, 1:].mean(axis=1)
reference_mean_df = pd.DataFrame({
    reference_df.columns[0]: reference_df.iloc[:, 0],
    'Dissolution': reference_mean
})
test_mean_df = pd.DataFrame({
    test_df.columns[0]: test_df.iloc[:, 0],
    'Dissolution': test_mean
})


# def dissolution_curve_interval(reference_df, test_df):
#     # Print dataframes (optional for debugging)
#     #print(reference_df)
#     #print(test_df)

#     # Calculate means, maxs, and mins for reference data
#     ref_data_means = pd.DataFrame(reference_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Reference'])
#     ref_data_max = pd.DataFrame(reference_df.iloc[:, 1:].max(axis=1), columns=['Max_Reference'])
#     ref_data_min = pd.DataFrame(reference_df.iloc[:, 1:].min(axis=1), columns=['Min_Reference'])

#     # Calculate means, maxs, and mins for test data
#     test_data_means = pd.DataFrame(test_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Test'])
#     test_data_max = pd.DataFrame(test_df.iloc[:, 1:].max(axis=1), columns=['Max_Test'])
#     test_data_min = pd.DataFrame(test_df.iloc[:, 1:].min(axis=1), columns=['Min_Test'])

#     # Add Time column
#     ref_data_means.insert(0, 'Time', reference_df.iloc[:, 0])
#     ref_data_max.insert(0, 'Time', reference_df.iloc[:, 0])
#     ref_data_min.insert(0, 'Time', reference_df.iloc[:, 0])

#     test_data_means.insert(0, 'Time', test_df.iloc[:, 0])
#     test_data_max.insert(0, 'Time', test_df.iloc[:, 0])
#     test_data_min.insert(0, 'Time', test_df.iloc[:, 0])

#     # Create a figure and axis object
#     plt.figure(figsize=(12, 6))

#     # Plotting reference data with error bars
#     plt.errorbar(ref_data_means['Time'], ref_data_means['Mean_Reference'],
#                  yerr=[ref_data_means['Mean_Reference'] - ref_data_min['Min_Reference'],
#                        ref_data_max['Max_Reference'] - ref_data_means['Mean_Reference']],
#                  fmt='o', label='Reference Mean', color='blue', linestyle='-')

#     # Plotting test data with error bars
#     plt.errorbar(test_data_means['Time'], test_data_means['Mean_Test'],
#                  yerr=[test_data_means['Mean_Test'] - test_data_min['Min_Test'],
#                        test_data_max['Max_Test'] - test_data_means['Mean_Test']],
#                  fmt='o', label='Test Mean', color='green', linestyle='--')

#     # Adding horizontal segments for min and max values for reference data
#     for time, min_val, max_val in zip(ref_data_means['Time'], ref_data_min['Min_Reference'], ref_data_max['Max_Reference']):
#         plt.hlines(min_val, time - 0.2, time + 0.2, colors='blue', linestyles='--', alpha=0.5)
#         plt.hlines(max_val, time - 0.2, time + 0.2, colors='blue', linestyles='--', alpha=0.5)

#     # Adding horizontal segments for min and max values for test data
#     for time, min_val, max_val in zip(test_data_means['Time'], test_data_min['Min_Test'], test_data_max['Max_Test']):
#         plt.hlines(min_val, time - 0.2, time + 0.2, colors='green', linestyles='--', alpha=0.5)
#         plt.hlines(max_val, time - 0.2, time + 0.2, colors='green', linestyles='--', alpha=0.5)

#     # Add labels and title
#     plt.xlabel('Time')
#     plt.ylabel('Dissolution (%)')
#     plt.title('Dissolution Curves with Intervals')
#     plt.grid(True)
#     plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
#     plt.gca().set_yticks(range(0, 101, 5))
#     plt.legend(loc='lower right')
    
#     #Save the plot as an image file
#     plt.savefig('dissolution_curves_with_intervals.png', format='png', dpi=300)

#     #Optionally, display the plot (uncomment if you want to see it as well)
#     plt.show()

#     # Close the plot to free up memory
#     plt.close()


# def dissolution_curve(reference_df,test_df):
#     ref_data_means=pd.DataFrame(reference_df.iloc[:,1:].mean(axis=1),columns=['Mean_Reference'])
#     test_data_means=pd.DataFrame(test_df.iloc[:,1:].mean(axis=1),columns=['Mean_Test'])


#     ref_data_means.insert(0, 'Time', reference_df.iloc[:, 0])
#     test_data_means.insert(0, 'Time', test_df.iloc[:, 0])



#     # Create a figure and axis object
#     plt.figure(figsize=(12, 6))

#     # Plotting reference data
#     for column in ref_data_means.columns[1:]:  # Skip the first column which is 'Time'
#         plt.plot(ref_data_means.iloc[:, 0], ref_data_means[column], label=f'Reference {column}',marker='o')

#     # Plotting test data
#     for column in test_data_means.columns[1:]:  # Skip the first column which is 'Time'
#         plt.plot(test_data_means.iloc[:, 0], test_data_means[column], label=f'Test {column}', marker='o',linestyle='--')

#     # Add labels and title
#     plt.xlabel('Time')
#     plt.ylabel('Dissolution (%)')
#     plt.title('Dissolution Curves')
#     plt.grid(True)
#     plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
#     plt.gca().yaxis.set_major_locator(MaxNLocator(nbins='auto', integer=True))
#     plt.gca().set_yticks(range(0, 101, 5))
#     plt.legend(loc='lower right')
#     #Save the plot as an image file
#     plt.savefig('dissolution_curves.png', format='png', dpi=300)

#     #Optionally, display the plot (uncomment if you want to see it as well)
#     plt.show()

#     # Close the plot to free up memory
#     plt.close()

# Check 1: A minimum of three-time points (time zero excluded) is considered for both products
def check_time_points(df):

    if df.iloc[0, 0] == 0 or df.iloc[0, 0]=='0':  # Check if the first time point is zero
        return len(df.iloc[:,0].values)-1 >=3 # Exclude the zero time point 
    else:
        return len(df.iloc[:,0].values) >=3 
    
# Check 2: 1,2 time points > 85%? If > 85%, No f2 calculations
def two_time_points(df):
    mean_values = df.iloc[:, 1:].mean(axis=1)  # Exclude the Time Points column
    if mean_values[0]>85 or mean_values[1]>85:
         return False
    else:
         return True

#Check 4: If in Min check if in 15 min > 85% the no f2 required
def min15__check(df):
     time_string=df.columns[0]
     mean_values = df.iloc[:, 1:].mean(axis=1) 
     
     if "min" in time_string.lower() or "minutes" in time_string.lower():
          condition = df.iloc[:, 0] <= 15
          indices = df.index[condition]
          filtered_df = mean_values.loc[indices]
          condition_other_columns = filtered_df > 85
          final_indices = filtered_df.index[condition_other_columns].tolist()
          if len(final_indices)>0:
               return True
          else:
               return False
     else:
          return False
          
# The coefficient of variation (CV) of both product should be less than 20% at the first (non-zero) time point and less than 10% at the following time points
def check_cv(df):
    if df.iloc[0, 0] == 0:  # Check if the first time point is zero
        cv_values = df.iloc[1:, 1:].std(axis=1) / df.iloc[1:, 1:].mean(axis=1) * 100  # Exclude the zero time point and Time Points column
        return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()
    else:
        cv_values = df.iloc[:, 1:].std(axis=1) / df.iloc[:, 1:].mean(axis=1) * 100 # Calculate CV for all time points
        #print(df)
        #print(cv_values)
        return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()



def row_variance(df):
    return df.iloc[:,1:].var(axis=1,ddof=1)

def stand(p,sum_diff_df_sqr):
    f2_v1 = 100 - 25 * np.log10((1 + (1 / p) * sum_diff_df_sqr))  # correct F2_DRL
    return f2_v1

def expected(p,sum_diff_df_sqr,left_side):
    f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) + (left_side)))))
    return f2
    
def BiasCor(p,sum_diff_df_sqr,left_side):
    # f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
    # print("Bias corrected f2", f2)
    Right_side=sum_diff_df_sqr+p
    if left_side >= Right_side:
        f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
        return round(f2,3)
    else:
        f2="Baised Corrected f2 can not be calculated"
        return f2
    return f2



def f2s(ref_data,test_data):   
    ref_data_means=ref_data.iloc[:,1:].mean(axis=1)
    test_data_means=test_data.iloc[:,1:].mean(axis=1)
    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    diff_df=test_data_df-ref_data_df
    diff_df_sqr=diff_df**2
    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    ref_data_var_df=pd.DataFrame(row_variance(ref_data),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(test_data),columns=['Unbiased Variance'])
    addition_df=test_data_var_df+ref_data_var_df
    sum_addition_df=addition_df['Unbiased Variance'].sum()
    n_r,n_c=ref_data.shape
    n=n_c-1
    p=len(ref_data.iloc[:,0])
    left_side=(1/n)*(sum_addition_df)
    f2=stand(p,sum_diff_df_sqr)
    print("Conventional f2 :",round(f2,3))
    expf2=expected(p,sum_diff_df_sqr,left_side)
    print("Expected f2     :", round(expf2,3))
    BiCf2=BiasCor(p,sum_diff_df_sqr,left_side)
    print("BiasCorrected f2:", BiCf2)



def changed_data_either85_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)



def changed_data_both85_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    # print("mean ref",mean_values_reference)
    # print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    # print(ref_index)
    # print(test_index)

    if ref_index>test_index:
        final_index=ref_index
    elif test_index>ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    # print(final_index)
    # print(final_index+1)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)

def changed_data_either85_FDA_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)
def calculate_f2_either85(reference_df, test_df):
    """
    1. Keep time 0.
    2. Find the first non‑zero timepoint where either mean(reference) or mean(test) > 85%.
    3. Trim both DataFrames to include rows [0 .. that timepoint].
    4. Compute:
         - f2_conv: conventional f2
         - exp_f2 : expected f2 (variance‑adjusted)
    Returns:
       f2_conv, exp_f2, trimmed_ref_df, trimmed_test_df
    """
    # 1) Extract times & means
    times     = reference_df.iloc[:, 0].astype(float).values
    ref_mean  = reference_df.iloc[:, 1:].mean(axis=1).values
    test_mean = test_df.iloc[:, 1:].mean(axis=1).values

    # 2) Find cutoff index
    cutoff = next((i for i in range(1, len(times))
                   if ref_mean[i] > 85 or test_mean[i] > 85),
                  None)
    if cutoff is None:
        raise ValueError("Neither profile exceeds 85% at any non‑zero timepoint.")

    # 3) Trim full DataFrames
    trimmed_ref_df  = reference_df.iloc[:cutoff+1].copy()
    trimmed_test_df = test_df.iloc[:cutoff+1].copy()

    # Force 0% at t=0
    if float(trimmed_ref_df.iloc[0, 0]) == 0:
        trimmed_ref_df.iloc[0, 1:]  = 0.0
        trimmed_test_df.iloc[0, 1:] = 0.0

    # 4a) conventional f2
    diff    = (trimmed_test_df.iloc[:,1:].mean(axis=1).values
             - trimmed_ref_df.iloc[:,1:].mean(axis=1).values)
    p       = len(diff)
    sum_sq  = np.sum(diff**2)
    f2_conv = 100 - 25 * np.log10(1 + sum_sq / p)

    # 4b) expected f2
    var_ref   = trimmed_ref_df.iloc[:,1:].var(axis=1, ddof=1).values
    var_test  = trimmed_test_df.iloc[:,1:].var(axis=1, ddof=1).values
    avg_var   = np.mean(var_ref + var_test)
    exp_f2    = 100 - 25 * np.log10(1 + (sum_sq + p * avg_var) / p)

    return f2_conv, exp_f2, trimmed_ref_df, trimmed_test_df




def cal_f2(resampled_test,resampled_reference):
    #print(resampled_reference.iloc[:,1:])
    ref_data_means=resampled_reference.iloc[:,1:].mean(axis=1)
    test_data_means=resampled_test.iloc[:,1:].mean(axis=1)

    #print(ref_data_means)
    #print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    #print(ref_data_df)
    #print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)

    p=len(resampled_reference.iloc[:,0])
    #print('Time points',p)

    f2_v1 = 100 - 25 * np.log10((1 + (1 / p) * sum_diff_df_sqr))  # correct F2_DRL
    #print("f2 score is: ",f2_v1)
    arrayboot.append(f2_v1)
    return f2_v1

def bca(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=cal_f2(test_data,ref_data)
    n_iterations=12 # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_f2(dft,dfr)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for f2: [{lower_percentile}, {upper_percentile}]")

def bca_exp_f2(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=cal_exp_f2(test_data,ref_data)
    n_iterations=12  # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_exp_f2(dft,dfr)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for expected f2: [{lower_percentile}, {upper_percentile}]")

def plots(arrayboot,plot):
    plt.figure(figsize=(10, 6))
    sns.histplot(arrayboot, kde=True, bins=10, color='blue')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    tiltename=f"histogram_plot_{plot}"
    plt.title(tiltename)
    filename=f"histogram_plot_{plot}.png"
    plt.savefig(filename)  # Save the plot as a PNG file
    plt.close()  # Close the figure to avoid displaying it inline
    plt.figure(figsize=(8, 8))
    stats.probplot(arrayboot, dist="norm", plot=plt)
    tiltename=f"qq_plot_{plot}"
    plt.title(tiltename)
    filename=f"qq_plot_{plot}.png"
    plt.savefig(filename)  # Save the plot as a PNG file
    plt.close()  # Close the figure to avoid displaying it inline


def changed_data_either85_bca(reference_df,test_df):
    arrayboot.clear()
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index
    
    if final_index==0:
        final_index=change_reference_df.index[-1]
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("f2 Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    #print("Len1",len(arrayboot))
    plot="f2"
    plots(arrayboot,plot)

    print(" ")
    bca(change_test_df,change_reference_df)
    #print("Len2",len(arrayboot))
    plot="f2_bca"
    plots(arrayboot,plot)


    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    #print("Len3",len(arrayboot))
    plot="expected_f2"
    plots(arrayboot,plot)

    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    #print("Len4",len(arrayboot))
    plot="expected_f2_bca"
    plots(arrayboot,plot)

    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Baised Corrected f2 can not be calculated")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        #print("Len5",len(arrayboot))
        plot="baiscorrected_f2"
        plots(arrayboot,plot)

        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)
        #print("Len6",len(arrayboot))
        plot="baiscorrected_f2_bca"
        plots(arrayboot,plot)



def changed_data_both85_bca(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    # print(ref_index)
    # print(test_index)

    if ref_index>test_index:
        final_index=ref_index
    elif test_index>ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    print(" ")
    bca(change_test_df,change_reference_df)
    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Cannot caclulate f2")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)


def changed_data_either85_FDA_bca(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    print(" ")
    bca(change_test_df,change_reference_df)
    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Cannot caclulate f2")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)




def f2_bootstral_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=12 # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_f2(dft,dfr)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

# Check 3: The time points at which the dissolutions are measured are the same for both products
def check_same_time_points(df1, df2):
    return df1.iloc[:,0].equals(df2.iloc[:,0])

# Check 4: At least 12 individual dosage units are used for both products
def check_sample_units(df):
    return df.shape[1] - 1 >= 12  # Exclude the Time Points column


def cal_exp_f2(resampled_test,resampled_reference):
    ref_data_means=resampled_reference.iloc[:,1:].mean(axis=1)
    test_data_means=resampled_test.iloc[:,1:].mean(axis=1)

    #print(ref_data_means)
    #print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    #print(ref_data_df)
    #print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)

    ref_data_var_df=pd.DataFrame(row_variance(resampled_reference),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(resampled_test),columns=['Unbiased Variance'])

    #print(ref_data_var_df)
    #print(test_data_var_df)

    addition_df=test_data_var_df+ref_data_var_df
    #print(addition_df)

    sum_addition_df=addition_df['Unbiased Variance'].sum()
    #print(sum_addition_df)

    n_r,n_c=resampled_test.shape
    n=n_c-1
    #print(n)

    p=len(resampled_test.iloc[:,0])
    #print('Time points',p)

    left_side=(1/n)*(sum_addition_df)
    #print(left_side)

    f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) + (left_side)))))
    #print("Expected f2", f2)
    arrayboot.append(f2)
    return f2

def f2_bootstra_expf2_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=12 # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_exp_f2(dft,dfr)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

def biascorrectedf2(ref_data,test_data):
    ref_data_means=ref_data.iloc[:,1:].mean(axis=1)
    test_data_means=test_data.iloc[:,1:].mean(axis=1)

    # print(ref_data_means)
    # print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    # print(ref_data_df)
    # print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)


    ref_data_var_df=pd.DataFrame(row_variance(ref_data),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(test_data),columns=['Unbiased Variance'])

    # print(ref_data_var_df)
    # print(test_data_var_df)

    addition_df=test_data_var_df+ref_data_var_df
    # print(addition_df)

    sum_addition_df=addition_df['Unbiased Variance'].sum()
    # print(sum_addition_df)

    n_r,n_c=ref_data.shape
    n=n_c-1
    # print(n)

    p=len(ref_data.iloc[:,0])
    # print('Time points',p)

    left_side=(1/n)*(sum_addition_df)
    # print(left_side)

    # f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
    # print("Bias corrected f2", f2)
    Right_side=sum_diff_df_sqr+p

    if left_side >= Right_side:
        with warnings.catch_warnings():
            warnings.simplefilter("error", RuntimeWarning)
            try:
                f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
            except RuntimeWarning as e:
                print(f"Warning occurred: {e}")
                f2=0
                return f2
        #print("Baised Corrected f2", f2)
        return f2
    else:
        f2=0
        #print("No f2",f2)
        return f2

def f2_bootstratp_bias_f2_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=12 # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=biascorrectedf2(dfr,dft)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

def bca_bias_f2(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=f2_bootstratp_bias_f2_tile(ref_data,test_data)
    n_iterations=12 # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=f2_bootstratp_bias_f2_tile(dfr,dft)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for expected f2: [{lower_percentile}, {upper_percentile}]")


def R_Regulation(file_path):
    dr=read.read_excel(file_path,sheet='Sheet1')
    dt=read.read_excel(file_path,sheet='Sheet2')
    print("Enter Name of Regulation either EMA, FDA, WHO, Canada, ANVISA:")
    reg=input()
    b=bootf2.bootf2(dt,dr,file_out='test',regulation=reg)
    print(b.rx2('boot.summary'))
    print("Please check file created for full report")

def R_all_Regulation(file_path):
    dr=read.read_excel(file_path,sheet='Sheet1')
    dt=read.read_excel(file_path,sheet='Sheet2')
    b=bootf2.bootf2(dt,dr,file_out='EMA_Results',regulation="EMA")
    print("EMA Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='FDA_Results',regulation="FDA")
    print("FDA Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='WHO_Results',regulation="WHO")
    print("WHO Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='Canada_Results',regulation="Canada")
    print("Canada Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='ANVISA_Results',regulation="ANVISA")
    print("ANVISA Summary:")
    print(b.rx2('boot.summary'))
    print("Please check files created for full report")
def interpolate_linear(df, new_times):
    """
    Simple linear interpolation (used for determining the candidate window
    and for candidate selection).
    """
    df_sorted = df.sort_values(by=df.columns[0])
    known_times = df_sorted.iloc[:, 0].values.astype(float)
    known_values = df_sorted.iloc[:, 1].values.astype(float)
    f = interp1d(known_times, known_values, kind='linear', 
                 fill_value=(known_values[0], known_values[-1]), bounds_error=False)
    return f(new_times)

def determine_candidate_window(ref_df, test_df, step=1, initial_threshold=10):
    """
    Identify candidate window based on the actual time range in the data.
    """
    max_ref_time = ref_df.iloc[:, 0].max()
    max_test_time = test_df.iloc[:, 0].max()
    fixed_max = max(max_ref_time, max_test_time)
    fixed_min = 0

    times = np.arange(fixed_min, fixed_max + 1, step)
    ref_vals = interpolate_linear(ref_df, times)
    test_vals = interpolate_linear(test_df, times)
    diff = np.abs(ref_vals - test_vals)
    
    valid_times = times[diff <= initial_threshold]
    if len(valid_times) == 0:
        print(f"No time points found within {initial_threshold}% difference; trying threshold=20.")
        valid_times = times[diff <= 20]
        if len(valid_times) == 0:
            print("No candidate window found even with 20% threshold. Using full range.")
            return fixed_min, fixed_max
        else:
            window_max = valid_times[-1]
            print(f"Candidate window determined (threshold 20): {fixed_min} to {window_max}")
            return fixed_min, window_max
    else:
        window_max = valid_times[-1]
        print(f"Candidate window determined (threshold {initial_threshold}): {fixed_min} to {window_max}")
        return fixed_min, window_max

# ======================== STAGE 2: Helper & Predictive Analysis Functions ========================
def generate_all_time_combinations(min_time, max_time, step=1):
    """
    Generate all possible time point sequences within the interval [min_time, max_time],
    ensuring that 0 is fixed at the start and that there are at least 3 points.
    """
    times = list(range(min_time, max_time + 1, step))
    if 0 not in times:
        times.insert(0, 0)
    all_combinations = []
    for r in range(2, len(times)):
        for combo in itertools.combinations(times[1:], r):
            seq = [0] + list(combo)
            all_combinations.append(sorted(seq))
    return list(all_combinations)

def interpolate_dissolution_curve(df, new_times, method='linear'):
    """
    Predict dissolution values at new time points, using linear interpolation.
    """
    df_sorted = df.sort_values(by=df.columns[0])
    known_times = df_sorted.iloc[:, 0].values.astype(float)
    known_values = df_sorted.iloc[:, 1].values.astype(float)
    
    new_times_clamped = np.clip(new_times, known_times.min(), known_times.max())
    f = interp1d(known_times, known_values, kind='linear', 
                 fill_value=(known_values[0], known_values[-1]), bounds_error=False)
    return f(new_times_clamped)
    

def check_regulatory_compliance(seq, regulation, ref_means, test_means):
    """
    Check if a candidate sequence satisfies the selected regulatory rules.
    Revised rules:
      - FDA:
          * For each of reference and test, exactly one time point must have dissolution ≥85%.
      - EMA:
          * Across reference or test, at most one time point may be ≥85%.
          * At least 4 time points are required.
      - China:
          * For the reference, exactly one time point must be "around 85%" (i.e. between 84 and 86)
            and all other time points must be below 85%.
      - ASEAN:
          * There must be at least 3 time points collected before 15 minutes.
          * At the 15-minute time point (if present), the reference dissolution should be close to 80%
            (here defined as between 75 and 85).
          * Across both reference and test, only one time point in the full profile may be ≥85%.
      - ANVISA:
          * The collection must have at least 5 time points.
          * The first time point must be 0.
          * Across reference and test, only one time point may be ≥85%.
          * For f2 calculation, use the first 3 time points (excluding the 0 time point);
            these “earlier time points” should be around 40% (here defined as between 35 and 45).
    Returns a tuple (compliant, reasons) where compliant is a boolean and reasons is a list of strings.
    """
    compliant = True
    reasons = []
    
    if regulation == "FDA":
        # Count time points with ≥85% dissolution for both reference and test separately.
        ref_count = sum(1 for t in seq if ref_means[t] >= 85)
        test_count = sum(1 for t in seq if test_means[t] >= 85)
        if ref_count != 1:
            compliant = False
            reasons.append(f"Reference must have exactly one time point ≥85% (found {ref_count}).")
        if test_count != 1:
            compliant = False
            reasons.append(f"Test must have exactly one time point ≥85% (found {test_count}).")
    
    elif regulation == "EMA":
        # Combined count: only one time point overall may be ≥85%
        combined_count = sum(1 for t in seq if ref_means[t] >= 85 or test_means[t] >= 85)
        if combined_count > 1:
            compliant = False
            reasons.append("More than one time point ≥85% detected across reference/test.")
        if len(seq) < 4:
            compliant = False
            reasons.append("Insufficient time points (<4).")
    
    elif regulation == "China":
        # For reference: exactly one time point must be around 85% (between 84 and 86) 
        # and all others must be below 85%.
        around_85 = [t for t in seq if 84 <= ref_means[t] <= 86]
        above_85 = [t for t in seq if ref_means[t] > 85]
        if len(around_85) != 1:
            compliant = False
            reasons.append(f"Reference must have exactly one time point around 85% (found {len(around_85)}).")
        # Even if one time point is around 85, any other time point ≥85 is not allowed.
        if len(above_85) - len(around_85) > 0:
            compliant = False
            reasons.append("Other reference time points above 85% detected.")
    
    elif regulation == "ASEAN":
        # At least 3 time points must be before 15 minutes.
        before_15 = [t for t in seq if t < 15]
        if len(before_15) < 3:
            compliant = False
            reasons.append("Fewer than 3 time points collected before 15 minutes.")
        # At the 15-minute time point, reference should be close to 80% (between 75 and 85) if present.
        if 15 in seq:
            if not (75 <= ref_means[15] <= 85):
                compliant = False
                reasons.append("At 15 minutes, reference dissolution is not close to 80%.")
        # Combined count for any dissolution ≥85% (both reference and test) must be only one.
        combined_count = sum(1 for t in seq if ref_means[t] >= 85 or test_means[t] >= 85)
        if combined_count != 1:
            compliant = False
            reasons.append(f"ASEAN profile must have exactly one time point ≥85% (found {combined_count}).")
    
    elif regulation == "ANVISA":
        # Must have at least 5 time points.
        if len(seq) < 5:
            compliant = False
            reasons.append("Less than 5 time points collected.")
        # The first time point must be 0.
        if seq[0] != 0:
            compliant = False
            reasons.append("The first time point must be 0.")
        # Count time points with ≥85% dissolution for both reference and test separately.
        ref_count = sum(1 for t in seq if ref_means[t] >= 85)
        test_count = sum(1 for t in seq if test_means[t] >= 85)
        if ref_count != 1:
            compliant = False
            reasons.append(f"Reference must have exactly one time point ≥85% (found {ref_count}).")
        if test_count != 1:
            compliant = False
            reasons.append(f"Test must have exactly one time point ≥85% (found {test_count}).")
        # For f2 calculation, use the first 3 time points (excluding the 0 time point);
        # these early time points should be around 40% (between 35 and 45).
        nonzero_points = [t for t in seq if t != 0]
        early_points = nonzero_points[:3]
        for t in early_points:
            if not (35 <= ref_means[t] <= 45 and 35 <= test_means[t] <= 45):
                compliant = False
                reasons.append(f"Early time point at t={t} not around 40% for both reference and test.")
    
    return (compliant, reasons)

def predictive_optimal_combinations_advanced(ref_df, test_df, regulation, 
                                             window_min, window_max, diff_threshold=None,
                                             interp_method='linear', points_per_stratum=None):
    """
    Deterministic selection of candidate time points using linear interpolation.
    
    Steps:
      1. Build a valid time grid from the union of 3- and 5-minute increments within [window_min, window_max].
      2. Compute predicted dissolution values (via linear interpolation) for both reference and test.
      3. Fix the starting time (window_min) as the first candidate.
      4. Split the remaining valid times into three strata based on predicted reference values:
           - 0-30: select exactly 2 candidate times (excluding the starting time)
           - 30-60: select exactly 2 candidate times (min and max among those with ref in [30,60))
           - 60-90: select exactly 2 candidate times (min and max among those with ref in [60,90))
         For the 60–90 stratum, if neither candidate has both ref and test ≥80%, replace the later candidate
         with the candidate (from eligible times) that maximizes min(ref, test).
      5. For non‑FDA mode, the final candidate combination will have 1 + 2 + 2 + 2 = 7 points.
      6. For FDA mode, append one extra candidate—the first valid time (after the last candidate) where both
         predictions are ≥85%—yielding 8 points.
      7. Compute f2 using:
             f2 = 50 * log10(100 / (1 + sqrt(mean((test - ref)^2))))
         (The prediction at the starting time is forced to 0.)
         
    Raises an error if any stratum (other than 60–90, which is adjusted) lacks sufficient eligible times.
    """
    import numpy as np
    import math
    
    # 1. Build valid time grid (only 3- and 5-minute increments)
    valid_times = np.sort(np.unique(np.concatenate([
        np.arange(window_min, window_max+1, 3),
        np.arange(window_min, window_max+1, 5)
    ])))
    
    # 2. Compute predicted dissolution values using linear interpolation.
    all_ref_pred = interpolate_linear(ref_df, valid_times)
    all_test_pred = interpolate_linear(test_df, valid_times)
    
    # 3. Fix the starting time.
    fixed_start = window_min  # typically 0
    
    # 4. Define dissolution strata (using predicted reference values).
    strata = {
        "0-30": (0, 30),
        "30-60": (30, 60),
        "60-90": (60, 90)
    }
    required_points = 2  # exactly two candidate times per stratum (excluding the fixed starting time)
    
    # For 0-30, we exclude the fixed starting time.
    def eligible_times_for_stratum(stratum_range, exclude_start=False):
        low, high = stratum_range
        if exclude_start:
            return [t for t, pred in zip(valid_times, all_ref_pred) if t != fixed_start and low <= pred < high]
        else:
            return [t for t, pred in zip(valid_times, all_ref_pred) if low <= pred < high]
    
    candidate = []
    
    # 5. For each stratum, select exactly two candidate times deterministically.
    # For the 0-30 stratum, exclude the fixed starting time.
    for key in ["0-30", "30-60", "60-90"]:
        if key == "0-30":
            eligible = eligible_times_for_stratum(strata[key], exclude_start=True)
        else:
            eligible = eligible_times_for_stratum(strata[key])
        if len(eligible) < required_points:
            raise ValueError(f"Insufficient eligible times in stratum {key}.")
        # Deterministically choose the minimum and maximum eligible time.
        candidate_stratum = [min(eligible), max(eligible)]
        candidate.extend(candidate_stratum)
    
    candidate = sorted(list(set(candidate)))
    # Now, add the fixed starting time at the beginning.
    if fixed_start not in candidate:
        candidate.insert(0, fixed_start)
    
    # For non-FDA mode, we expect candidate count = 1 + 2 + 2 + 2 = 7.
    if regulation != "FDA" and len(candidate) != 7:
        raise ValueError(f"Final candidate count ({len(candidate)}) is not equal to 7 as required for non-FDA mode.")
    
    # 6. Enforce the 60-90 stratum rule:
    eligible_60_90 = eligible_times_for_stratum(strata["60-90"])
    cand_60_90 = [t for t in candidate if t in eligible_60_90]
    has_80 = False
    for t in cand_60_90:
        idx = int(np.where(valid_times == t)[0][0])
        if all_ref_pred[idx] >= 80 and all_test_pred[idx] >= 80:
            has_80 = True
            break
    if not has_80:
        # Replace the later candidate from the 60-90 group with the candidate that maximizes min(ref, test)
        best_val = -np.inf
        best_t = None
        for t in eligible_60_90:
            idx = int(np.where(valid_times == t)[0][0])
            val = min(all_ref_pred[idx], all_test_pred[idx])
            if val > best_val:
                best_val = val
                best_t = t
        current_60_90 = sorted([t for t in candidate if t in eligible_60_90])
        if len(current_60_90) < 2:
            raise ValueError("Insufficient candidates in 60-90 stratum to adjust.")
        # Replace the later candidate with best_t.
        candidate = [t for t in candidate if t not in current_60_90]
        candidate.extend([min(current_60_90), best_t])
        candidate = sorted(list(set(candidate)))
        if len(candidate) != 7:
            raise ValueError("After adjustment, candidate count is not 7 for non-FDA mode.")
    
    # 7. For FDA mode: Append one extra candidate—the first valid time after the last candidate with both predictions ≥85%.
    if regulation in ["FDA", "ANVISA"]:
        expected_count = 8
        last_candidate = candidate[-1]
        post_times = [t for t in valid_times if t > last_candidate]
        extra_point = None
        for t in post_times:
            idx = int(np.where(valid_times == t)[0][0])
            if all_ref_pred[idx] >= 85 and all_test_pred[idx] >= 85:
                extra_point = t
                break
        if extra_point is None:
            raise ValueError("No valid extra candidate found for FDA mode.")
        candidate.append(extra_point)
        candidate = sorted(list(set(candidate)))
        if len(candidate) != expected_count:
            raise ValueError(f"Final candidate count ({len(candidate)}) does not equal expected count ({expected_count}) for FDA mode.")
    else:
        if len(candidate) != 7:
            raise ValueError(f"Non-FDA candidate count ({len(candidate)}) is not equal to 7.")
    
    # 8. Compute f2.
    # Use linear interpolation predictions at the candidate times.
    ref_vals = np.array([float(val) for val in interpolate_linear(ref_df, candidate)])
    test_vals = np.array([float(val) for val in interpolate_linear(test_df, candidate)])
    if candidate[0] == fixed_start:
        ref_vals[0] = 0.0
        test_vals[0] = 0.0
    diff = test_vals - ref_vals
    f2 = 50 * math.log10(100 / (1 + math.sqrt(np.mean(diff**2))))
    
    result = {
        'sequence': candidate,
        'f2': round(f2, 2),
        'compliant': True,
        'reasons': [],
        'ref_vals': ref_vals.tolist(),
        'test_vals': test_vals.tolist()
    }
    return [result], [result]

if check_time_points(test_df) and check_time_points(reference_df):
    print("Check 1: A minimum of three-time points - PASSED")
else:
    print("Check 1: A minimum of three-time points - FAILED, NO f2 Calculated")

if two_time_points(test_df) and two_time_points(reference_df):
    print("Check 2: 1,2 time points less than 85 percent dissoultion - PASSED")
else:
    print("Check 2: 1,2 time points less than 85 percent dissoultion- FAILED, NO f2 Calculated")

if min15__check(test_df) and min15__check(reference_df):
    print("Check 3: In 15 min their is greater than 85 percent dissolution - PASSED, NO f2 Calculated")
else:
    print("Check 4: In 15 min their is No greater than 85 percent dissolution - FAILED")

if check_same_time_points(test_df, reference_df):
    print("Check 4: The time points same for both products - PASSED")
else:
    print("Check 4: The time points same for both products - FAILED")

if check_sample_units(test_df) and check_sample_units(reference_df):
    print("Check 5: At least 12 individual sample units - PASSED")
else:
    print("Check 5: At least 12 individual sample units - FAILED")

# print(" ")
# dissolution_curve(reference_df,test_df)
# dissolution_curve_interval(reference_df, test_df)
print(" ")
print("Choose Market:")
print("Choose 1 for FDA")
print("Choose 2 for EMA/ICH/Canda/AUs")
print("Choose 3 for China")
print("Choose 4 for ASEAN")
print("Choose 5 for ANVISa")

input1 = int(input("Input number: "))

def check_both_85(reference_df, test_df):
    """Check if both reference and test meet the >= 85% criteria. Returns True if criteria is met, False otherwise."""
    ref_max = reference_df.iloc[:, 1:].mean(axis=1).max()
    test_max = test_df.iloc[:, 1:].mean(axis=1).max()
    return ref_max >= 85 and test_max >= 85

    
def check_either_85(reference_df, test_df):
    """Check if either reference or test meet the >= 85% criteria. Returns True if criteria is met, False otherwise."""
    ref_max = reference_df.iloc[:, 1:].mean(axis=1).max()
    test_max = test_df.iloc[:, 1:].mean(axis=1).max()
    return ref_max >= 85 or test_max >= 85
def f2_factor(ref_means: np.ndarray, test_means: np.ndarray) -> float:
    """
    Traditional similarity factor f2:
      f2 = 100 - 25 * log10( 1 + (1/P) * sum((test_means - ref_means)**2) )
    """
    if ref_means.shape != test_means.shape:
        raise ValueError("ref_means and test_means must have the same shape")
    mse = np.mean((test_means - ref_means) ** 2)
    return 100 - 25 * np.log10(1 + mse)
def calculate_f2_fda(reference_df, test_df):
    """
    FDA ≥ 85% rule:
      - Keep time = 0
      - Find the first timepoint (after t=0) where both reference AND test mean > 85%
      - Trim both DataFrames to include rows [0 .. that timepoint]
      - Compute:
          * f2_conv = conventional f2
          * exp_f2  = expected f2 using the variance‐inclusive formula:
            f2 = 100 - 25 * log10(1
                                 + (1/P) * Σ (μTi - μRi)²
                                 + (1/n) * Σ (S²Ti + S²Ri)
                                )
    Returns:
      f2_conv, exp_f2, trimmed_ref_df, trimmed_test_df
    """
    # 1) Extract times & means
    times      = reference_df.iloc[:, 0].astype(float).values
    ref_mean   = reference_df.iloc[:, 1:].mean(axis=1).values
    test_mean  = test_df.iloc[:, 1:].mean(axis=1).values

    # 2) Find first simultaneous >85% (after t=0)
    cutoff = next(
        (i for i in range(1, len(times))
         if ref_mean[i] > 85 and test_mean[i] > 85),
        None
    )
    if cutoff is None:
        raise ValueError("No timepoint found where both profiles exceed 85% simultaneously.")

    # 3) Trim to [0..cutoff]
    ref_trim  = reference_df.iloc[:cutoff+1].copy()
    test_trim = test_df.iloc[:cutoff+1].copy()

    # 4) Force zero at t=0
    if float(ref_trim.iloc[0, 0]) == 0.0:
        ref_trim.iloc[0, 1:]  = 0.0
        test_trim.iloc[0, 1:] = 0.0

    # 5) Prepare arrays
    ref_means_arr  = ref_trim.iloc[:, 1:].mean(axis=1).values
    test_means_arr = test_trim.iloc[:, 1:].mean(axis=1).values
    P = len(ref_means_arr)
    n = ref_trim.shape[1] - 1  # number of replicates per timepoint

    # 6a) Conventional f2
    f2_conv = f2_factor(ref_means_arr, test_means_arr)

    # 6b) Expected f2 with variance term
    #   (1/P) * sum(diff^2)
    mse_term = np.mean((test_means_arr - ref_means_arr) ** 2)
    #   (1/n) * sum(S²Ti + S²Ri)
    var_ref = ref_trim.iloc[:, 1:].var(axis=1, ddof=1).values
    var_test = test_trim.iloc[:, 1:].var(axis=1, ddof=1).values
    variance_term = (np.sum(var_ref + var_test) / n)

    exp_f2 = 100 - 25 * np.log10(1 + mse_term + variance_term)

    return f2_conv, exp_f2, ref_trim, test_trim

if input1 == 1:
    print("According to FDA ≥ 85% guidelines\n")

    # 1) Check criteria & CV
    both_85   = check_both_85(reference_df, test_df)
    # either_85 = check_either_85(reference_df, test_df)
    print("Both ≥ 85% criterion met:  ", both_85)
    # print("Either ≥ 85% criterion met:", either_85)

    cv_check = check_cv(test_df) and check_cv(reference_df)
    print("\nCV <20 at first non-zero, <10 thereafter:", cv_check, "\n")

    #(0)Timepoint drop
    reference_df = reference_df[reference_df.iloc[:, 0] != 0].reset_index(drop=True)
    test_df = test_df[test_df.iloc[:, 0] != 0].reset_index(drop=True)

    # 2) Verify time alignment
    if not check_same_time_points(reference_df, test_df):
        print("Error: Time points between test and reference do not match.")
        print("Calculations cannot be performed.")
    else:
        try:
            # 3) Compute f2s & trimmed data
            f2_conv, f2_exp, ref_trim, test_trim = calculate_f2_fda(reference_df, test_df)

            # # 4) Show trimmed times & means
            times      = ref_trim.iloc[:, 0].astype(float).tolist()
            ref_means  = ref_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()
            test_means = test_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()

            print("Trimmed times:       ", times)
            print("Reference means (%): ", ref_means)
            print("Test means      (%): ", test_means, "\n")

            # 5) Print f2 results
            print(f"Conventional f2: {f2_conv:.2f}")
            print(f"Expected    f2: {f2_exp:.2f}")
            if not cv_check:
                print("\nWarning: CV requirements not met; interpret with caution.")

            # --- Graph 1: Simple dissolution curves ---
            plt.figure(figsize=(12, 6))
            plt.plot(times, ref_means,  marker='o', linestyle='-',  label='Reference')
            plt.plot(times, test_means, marker='o', linestyle='--', label='Test')
            plt.xlabel('Time')
            plt.ylabel('Dissolution (%)')
            plt.title('Dissolution Curves')
            plt.grid(True)
            ax = plt.gca()
            ax.yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
            ax.set_yticks(range(0, 101, 5))
            plt.legend(loc='lower right')
            plt.tight_layout()
            plt.savefig('fda_dissolution_curves.png', dpi=300)
            plt.show()
            plt.close()

            # --- Graph 2: Curves with min/max intervals ---
            ref_min = ref_trim.iloc[:,1:].min(axis=1)
            ref_max = ref_trim.iloc[:,1:].max(axis=1)
            tst_min = test_trim.iloc[:,1:].min(axis=1)
            tst_max = test_trim.iloc[:,1:].max(axis=1)

            plt.figure(figsize=(12, 6))
            plt.errorbar(
                times, ref_means,
                yerr=[ [rm - mi for rm, mi in zip(ref_means, ref_min)],
                      [ma - rm for ma, rm in zip(ref_max, ref_means)] ],
                fmt='o-', label='Reference Mean'
            )
            plt.errorbar(
                times, test_means,
                yerr=[ [tm - mi for tm, mi in zip(test_means, tst_min)],
                      [ma - tm for ma, tm in zip(tst_max, test_means)] ],
                fmt='s--', label='Test Mean'
            )
            # caps
            for t, mn, mx in zip(times, ref_min, ref_max):
                plt.hlines([mn, mx], t-0.2, t+0.2, linestyles='--', alpha=0.5)
            for t, mn, mx in zip(times, tst_min, tst_max):
                plt.hlines([mn, mx], t-0.2, t+0.2, linestyles='--', alpha=0.5)

            plt.xlabel('Time')
            plt.ylabel('Dissolution (%)')
            plt.title('Dissolution Curves with Intervals')
            plt.grid(True)
            ax = plt.gca()
            ax.yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
            ax.set_yticks(range(0, 101, 5))
            plt.legend(loc='lower right')
            plt.tight_layout()
            plt.savefig('fda_dissolution_curves_with_intervals.png', dpi=300)
            plt.show()
            plt.close()

        except ValueError as e:
            print("❌", e)
            print("Cannot perform FDA‑rule f2 calculation.")
        
    run_predictive = input("\nDo you want to run predictive optimal combination analysis? (yes/no): ")
    if run_predictive.lower() == 'yes':
        # Determine candidate window.
        window_min, window_max = determine_candidate_window(
            reference_mean_df,
            test_mean_df,
            step=5,
            initial_threshold=10
        )
        # Map regulation input.
        regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
        selected_regulation = regulation_map.get(input1, "FDA")
        print(f"\nCandidate window for combination search: {window_min} to {window_max}")
        
        # Run deterministic predictive analysis.
        results, all_results = predictive_optimal_combinations_advanced(
            reference_mean_df,
            test_mean_df,
            regulation=selected_regulation,
            window_min=window_min,
            window_max=window_max,
            diff_threshold=None,
            interp_method='linear',
            points_per_stratum=None
        )
        
        # Convert candidate time points to integers.
        for cand in results:
            cand['sequence'] = [int(t) for t in cand['sequence']]
        overall_best = results[0] if results else None
        if overall_best:
            print("\n=== Optimal Predictive Combination ===")
            print(f"Time Points (best candidate): {overall_best['sequence']}")
            print(f"Length: {len(overall_best['sequence'])}")
            print(f"Predicted f2 Score: {overall_best['f2']}")
            if overall_best['reasons']:
                print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
            else:
                print("Regulatory Compliance: Passed")
            # Plot predicted dissolution curves.
            import matplotlib.pyplot as plt
            plt.figure(figsize=(12, 6))
            time_points = overall_best['sequence']
            ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(time_points), method='linear')
            test_diss = interpolate_dissolution_curve(test_mean_df, np.array(time_points), method='linear')
            plt.plot(time_points, ref_diss, 'bo-', label='Reference')
            plt.plot(time_points, test_diss, 'r*--', label='Test')
            plt.title(f"Optimal Profile: Predicted Dissolution (f2 = {overall_best['f2']})")
            plt.xlabel('Time (min)')
            plt.ylabel('Dissolution (%)')
            plt.legend()
            plt.grid(True)
            plt.show()
            print("\nPredicted Reference Dissolution Percentages:")
            for t, d in zip(time_points, ref_diss):
                print(f"Time {t} min: {d:.2f}%")
            print("\nPredicted Test Dissolution Percentages:")
            for t, d in zip(time_points, test_diss):
                print(f"Time {t} min: {d:.2f}%")
        else:
            print("❌ No candidate sequence was generated.")
        print("\n=== All Candidate Combination (Diverse) ===")
        for idx, cand in enumerate(results):
            seq_print = [int(t) for t in cand['sequence']]
            print(f"{idx+1:3d}. | Points: {seq_print} | Length: {len(seq_print)} | f2: {cand['f2']} | Compliant: {cand['compliant']}")

elif input1 == 2:
    print("According to EMA/ICH/Canada/Australia guidelines")

    # 1) CV check
    cv_check = check_cv(test_df) and check_cv(reference_df)
    print("CV <20 at first non-zero, <10 thereafter:", cv_check)
    
    #(0)Timepoint drop
    reference_df = reference_df[reference_df.iloc[:, 0] != 0].reset_index(drop=True)
    test_df = test_df[test_df.iloc[:, 0] != 0].reset_index(drop=True)

    # 2) Verify time alignment
    if not check_same_time_points(test_df, reference_df):
        print("Error: Time points between test and reference do not match.")
    else:
        print("\nAnalysis based on ‘either >85%’ dissolution criterion:")
        try:
            # Compute f2s and get trimmed DataFrames
            f2_conv, f2_exp, ref_trim, test_trim = calculate_f2_either85(reference_df, test_df)

            # Print results
            print(f"Conventional f2: {f2_conv:.2f}")
            print(f"Expected    f2: {f2_exp:.2f}")
            # 4) Show trimmed times & means
            times      = ref_trim.iloc[:, 0].astype(float).tolist()
            ref_means  = ref_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()
            test_means = test_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()
            print("Trimmed times:       ", times)
            print("Reference means (%): ", ref_means)
            print("Test means      (%): ", test_means, "\n")

            if not cv_check:
                print("\nWarning: CV requirements not met; interpret with caution.")

            # --- Graph 1: Simple dissolution curves ---
            times      = ref_trim.iloc[:, 0].astype(float)
            ref_means  = ref_trim.iloc[:, 1:].mean(axis=1)
            test_means = test_trim.iloc[:, 1:].mean(axis=1)

            plt.figure(figsize=(12, 6))
            plt.plot(times, ref_means,  marker='o', linestyle='-',  label='Reference')
            plt.plot(times, test_means, marker='o', linestyle='--', label='Test')
            plt.xlabel('Time')
            plt.ylabel('Dissolution (%)')
            plt.title('Dissolution Curves')
            plt.grid(True)
            ax = plt.gca()
            ax.yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
            ax.set_yticks(range(0, 101, 5))
            plt.legend(loc='lower right')
            plt.tight_layout()
            plt.savefig('dissolution_curves.png', dpi=300)
            plt.show()
            plt.close()

            # --- Graph 2: Curves with min/max intervals ---
            ref_min  = ref_trim.iloc[:, 1:].min(axis=1)
            ref_max  = ref_trim.iloc[:, 1:].max(axis=1)
            test_min = test_trim.iloc[:, 1:].min(axis=1)
            test_max = test_trim.iloc[:, 1:].max(axis=1)

            plt.figure(figsize=(12, 6))
            # Reference with error bars
            plt.errorbar(
                times,
                ref_means,
                yerr=[ref_means - ref_min, ref_max - ref_means],
                fmt='o-',
                label='Reference Mean'
            )
            # Test with error bars
            plt.errorbar(
                times,
                test_means,
                yerr=[test_means - test_min, test_max - test_means],
                fmt='s--',
                label='Test Mean'
            )
            # Horizontal caps for min/max
            for t, mn, mx in zip(times, ref_min, ref_max):
                plt.hlines([mn, mx], t-0.2, t+0.2, linestyles='--', alpha=0.5)
            for t, mn, mx in zip(times, test_min, test_max):
                plt.hlines([mn, mx], t-0.2, t+0.2, linestyles='--', alpha=0.5)

            plt.xlabel('Time')
            plt.ylabel('Dissolution (%)')
            plt.title('Dissolution Curves with Intervals')
            plt.grid(True)
            ax = plt.gca()
            ax.yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
            ax.set_yticks(range(0, 101, 5))
            plt.legend(loc='lower right')
            plt.tight_layout()
            plt.savefig('dissolution_curves_with_intervals.png', dpi=300)
            plt.show()
            plt.close()

        except ValueError as e:
            print("❌", e)
            print("Cannot perform ‘either >85%’–based f2 calculation.")
        
    run_predictive = input("\nDo you want to run predictive optimal combination analysis? (yes/no): ")
    if run_predictive.lower() == 'yes':
        # Determine candidate window.
        window_min, window_max = determine_candidate_window(
            reference_mean_df,
            test_mean_df,
            step=5,
            initial_threshold=10
        )
        # Map regulation input.
        regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
        selected_regulation = regulation_map.get(input1, "FDA")
        print(f"\nCandidate window for combination search: {window_min} to {window_max}")
        
        # Run deterministic predictive analysis.
        results, all_results = predictive_optimal_combinations_advanced(
            reference_mean_df,
            test_mean_df,
            regulation=selected_regulation,
            window_min=window_min,
            window_max=window_max,
            diff_threshold=None,
            interp_method='linear',
            points_per_stratum=None
        )
        
        # Convert candidate time points to integers.
        for cand in results:
            cand['sequence'] = [int(t) for t in cand['sequence']]
        overall_best = results[0] if results else None
        if overall_best:
            print("\n=== Optimal Predictive Combination ===")
            print(f"Time Points (best candidate): {overall_best['sequence']}")
            print(f"Length: {len(overall_best['sequence'])}")
            print(f"Predicted f2 Score: {overall_best['f2']}")
            if overall_best['reasons']:
                print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
            else:
                print("Regulatory Compliance: Passed")
            # Plot predicted dissolution curves.
            import matplotlib.pyplot as plt
            plt.figure(figsize=(12, 6))
            time_points = overall_best['sequence']
            ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(time_points), method='linear')
            test_diss = interpolate_dissolution_curve(test_mean_df, np.array(time_points), method='linear')
            plt.plot(time_points, ref_diss, 'bo-', label='Reference')
            plt.plot(time_points, test_diss, 'r*--', label='Test')
            plt.title(f"Optimal Profile: Predicted Dissolution (f2 = {overall_best['f2']})")
            plt.xlabel('Time (min)')
            plt.ylabel('Dissolution (%)')
            plt.legend()
            plt.grid(True)
            plt.show()
            print("\nPredicted Reference Dissolution Percentages:")
            for t, d in zip(time_points, ref_diss):
                print(f"Time {t} min: {d:.2f}%")
            print("\nPredicted Test Dissolution Percentages:")
            for t, d in zip(time_points, test_diss):
                print(f"Time {t} min: {d:.2f}%")
        else:
            print("❌ No candidate sequence was generated.")
        print("\n=== All Candidate Combination (Diverse) ===")
        for idx, cand in enumerate(results):
            seq_print = [int(t) for t in cand['sequence']]
            print(f"{idx+1:3d}. | Points: {seq_print} | Length: {len(seq_print)} | f2: {cand['f2']} | Compliant: {cand['compliant']}")
        
elif input1 == 3:
    print("According to China's guidelines")
    print("CV check results:")
    cv_check = check_cv(test_df) and check_cv(reference_df)
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products -", cv_check)

    # Remove time point 0 if present
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df = reference_df.reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    # Check time points alignment
    time_points_check = check_same_time_points(test_df, reference_df)
    if time_points_check:
        # Always perform f2 calculations using either > 85% criterion
        either_85_criteria = check_either_85(reference_df, test_df)
        if either_85_criteria:
            print("\nAnalysis based on China's guidelines (either > 85%):")
            changed_data_either85_f2s(reference_df, test_df)
            if not cv_check:
                print("\nWarning: CV requirements not met. Results may not be valid for regulatory purposes.")
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
        else:
            print("\nWarning: 'Either > 85' criteria are not met.")
            print("Performing f2 calculations anyway for reference:")
            changed_data_either85_f2s(reference_df, test_df)
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
    else:
        print("Error: Time points between test and reference datasets do not match.")
        print("Calculations cannot be performed.")
    run_predictive = input("\nDo you want to run predictive optimal combination analysis? (yes/no): ")
    if run_predictive.lower() == 'yes':
        # Determine candidate window.
        window_min, window_max = determine_candidate_window(
            reference_mean_df,
            test_mean_df,
            step=5,
            initial_threshold=10
        )
        # Map regulation input.
        regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
        selected_regulation = regulation_map.get(input1, "FDA")
        print(f"\nCandidate window for combination search: {window_min} to {window_max}")
        
        # Run deterministic predictive analysis.
        results, all_results = predictive_optimal_combinations_advanced(
            reference_mean_df,
            test_mean_df,
            regulation=selected_regulation,
            window_min=window_min,
            window_max=window_max,
            diff_threshold=None,
            interp_method='linear',
            points_per_stratum=None
        )
        
        # Convert candidate time points to integers.
        for cand in results:
            cand['sequence'] = [int(t) for t in cand['sequence']]
        overall_best = results[0] if results else None
        if overall_best:
            print("\n=== Optimal Predictive Combination ===")
            print(f"Time Points (best candidate): {overall_best['sequence']}")
            print(f"Length: {len(overall_best['sequence'])}")
            print(f"Predicted f2 Score: {overall_best['f2']}")
            if overall_best['reasons']:
                print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
            else:
                print("Regulatory Compliance: Passed")
            # Plot predicted dissolution curves.
            import matplotlib.pyplot as plt
            plt.figure(figsize=(12, 6))
            time_points = overall_best['sequence']
            ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(time_points), method='linear')
            test_diss = interpolate_dissolution_curve(test_mean_df, np.array(time_points), method='linear')
            plt.plot(time_points, ref_diss, 'bo-', label='Reference')
            plt.plot(time_points, test_diss, 'r*--', label='Test')
            plt.title(f"Optimal Profile: Predicted Dissolution (f2 = {overall_best['f2']})")
            plt.xlabel('Time (min)')
            plt.ylabel('Dissolution (%)')
            plt.legend()
            plt.grid(True)
            plt.show()
            print("\nPredicted Reference Dissolution Percentages:")
            for t, d in zip(time_points, ref_diss):
                print(f"Time {t} min: {d:.2f}%")
            print("\nPredicted Test Dissolution Percentages:")
            for t, d in zip(time_points, test_diss):
                print(f"Time {t} min: {d:.2f}%")
        else:
            print("❌ No candidate sequence was generated.")
        print("\n=== All Candidate Combination (Diverse) ===")
        for idx, cand in enumerate(results):
            seq_print = [int(t) for t in cand['sequence']]
            print(f"{idx+1:3d}. | Points: {seq_print} | Length: {len(seq_print)} | f2: {cand['f2']} | Compliant: {cand['compliant']}")
                
elif input1 == 4:
    print("According to ASEAN guidelines")
    print("CV check results:")
    cv_check = check_cv(test_df) and check_cv(reference_df)
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products -", cv_check)

    # Remove time point 0 if present
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df = reference_df.reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    # Check time points alignment
    time_points_check = check_same_time_points(test_df, reference_df)
    if time_points_check:
        # Always perform f2 calculations using either > 85% criterion
        either_85_criteria = check_either_85(reference_df, test_df)
        if either_85_criteria:
            print("\nAnalysis based on ASEAN guidelines (either > 85%):")
            changed_data_either85_f2s(reference_df, test_df)
            if not cv_check:
                print("\nWarning: CV requirements not met. Results may not be valid for regulatory purposes.")
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
        else:
            print("\nWarning: 'Either > 85' criteria are not met.")
            print("Performing f2 calculations anyway for reference:")
            changed_data_either85_f2s(reference_df, test_df)
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
    else:
        print("Error: Time points between test and reference datasets do not match.")
        print("Calculations cannot be performed.")
    run_predictive = input("\nDo you want to run predictive optimal combination analysis? (yes/no): ")
    if run_predictive.lower() == 'yes':
        # Determine candidate window.
        window_min, window_max = determine_candidate_window(
            reference_mean_df,
            test_mean_df,
            step=5,
            initial_threshold=10
        )
        # Map regulation input.
        regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
        selected_regulation = regulation_map.get(input1, "FDA")
        print(f"\nCandidate window for combination search: {window_min} to {window_max}")
        
        # Run deterministic predictive analysis.
        results, all_results = predictive_optimal_combinations_advanced(
            reference_mean_df,
            test_mean_df,
            regulation=selected_regulation,
            window_min=window_min,
            window_max=window_max,
            diff_threshold=None,
            interp_method='linear',
            points_per_stratum=None
        )
        
        # Convert candidate time points to integers.
        for cand in results:
            cand['sequence'] = [int(t) for t in cand['sequence']]
        overall_best = results[0] if results else None
        if overall_best:
            print("\n=== Optimal Predictive Combination ===")
            print(f"Time Points (best candidate): {overall_best['sequence']}")
            print(f"Length: {len(overall_best['sequence'])}")
            print(f"Predicted f2 Score: {overall_best['f2']}")
            if overall_best['reasons']:
                print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
            else:
                print("Regulatory Compliance: Passed")
            # Plot predicted dissolution curves.
            import matplotlib.pyplot as plt
            plt.figure(figsize=(12, 6))
            time_points = overall_best['sequence']
            ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(time_points), method='linear')
            test_diss = interpolate_dissolution_curve(test_mean_df, np.array(time_points), method='linear')
            plt.plot(time_points, ref_diss, 'bo-', label='Reference')
            plt.plot(time_points, test_diss, 'r*--', label='Test')
            plt.title(f"Optimal Profile: Predicted Dissolution (f2 = {overall_best['f2']})")
            plt.xlabel('Time (min)')
            plt.ylabel('Dissolution (%)')
            plt.legend()
            plt.grid(True)
            plt.show()
            print("\nPredicted Reference Dissolution Percentages:")
            for t, d in zip(time_points, ref_diss):
                print(f"Time {t} min: {d:.2f}%")
            print("\nPredicted Test Dissolution Percentages:")
            for t, d in zip(time_points, test_diss):
                print(f"Time {t} min: {d:.2f}%")
        else:
            print("❌ No candidate sequence was generated.")
        print("\n=== All Candidate Combination (Diverse) ===")
        for idx, cand in enumerate(results):
            seq_print = [int(t) for t in cand['sequence']]
            print(f"{idx+1:3d}. | Points: {seq_print} | Length: {len(seq_print)} | f2: {cand['f2']} | Compliant: {cand['compliant']}")

elif input1 == 5:
    print("According to ANVISA guidelines")
    print("CV check results:")
    cv_check = check_cv(test_df) and check_cv(reference_df)
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products -", cv_check)

    # Remove time point 0 if present
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df = reference_df.reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    # Check time points alignment
    time_points_check = check_same_time_points(test_df, reference_df)
    if time_points_check:
        # Always perform f2 calculations using either > 85% criterion
        either_85_criteria = check_either_85(reference_df, test_df)
        if either_85_criteria:
            print("\nAnalysis based on ANVISA guidelines (either > 85%):")
            changed_data_either85_f2s(reference_df, test_df)
            if not cv_check:
                print("\nWarning: CV requirements not met. Results may not be valid for regulatory purposes.")
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
        else:
            print("\nWarning: 'Either > 85' criteria are not met.")
            print("Performing f2 calculations anyway for reference:")
            changed_data_either85_f2s(reference_df, test_df)
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
    else:
        print("Error: Time points between test and reference datasets do not match.")
        print("Calculations cannot be performed.")
    run_predictive = input("\nDo you want to run predictive optimal combination analysis? (yes/no): ")
    if run_predictive.lower() == 'yes':
        # Determine candidate window.
        window_min, window_max = determine_candidate_window(
            reference_mean_df,
            test_mean_df,
            step=5,
            initial_threshold=10
        )
        # Map regulation input.
        regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
        selected_regulation = regulation_map.get(input1, "FDA")
        print(f"\nCandidate window for combination search: {window_min} to {window_max}")
        
        # Run deterministic predictive analysis.
        results, all_results = predictive_optimal_combinations_advanced(
            reference_mean_df,
            test_mean_df,
            regulation=selected_regulation,
            window_min=window_min,
            window_max=window_max,
            diff_threshold=None,
            interp_method='linear',
            points_per_stratum=None
        )
        
        # Convert candidate time points to integers.
        for cand in results:
            cand['sequence'] = [int(t) for t in cand['sequence']]
        overall_best = results[0] if results else None
        if overall_best:
            print("\n=== Optimal Predictive Combination ===")
            print(f"Time Points (best candidate): {overall_best['sequence']}")
            print(f"Length: {len(overall_best['sequence'])}")
            print(f"Predicted f2 Score: {overall_best['f2']}")
            if overall_best['reasons']:
                print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
            else:
                print("Regulatory Compliance: Passed")
            # Plot predicted dissolution curves.
            import matplotlib.pyplot as plt
            plt.figure(figsize=(12, 6))
            time_points = overall_best['sequence']
            ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(time_points), method='linear')
            test_diss = interpolate_dissolution_curve(test_mean_df, np.array(time_points), method='linear')
            plt.plot(time_points, ref_diss, 'bo-', label='Reference')
            plt.plot(time_points, test_diss, 'r*--', label='Test')
            plt.title(f"Optimal Profile: Predicted Dissolution (f2 = {overall_best['f2']})")
            plt.xlabel('Time (min)')
            plt.ylabel('Dissolution (%)')
            plt.legend()
            plt.grid(True)
            plt.show()
            print("\nPredicted Reference Dissolution Percentages:")
            for t, d in zip(time_points, ref_diss):
                print(f"Time {t} min: {d:.2f}%")
            print("\nPredicted Test Dissolution Percentages:")
            for t, d in zip(time_points, test_diss):
                print(f"Time {t} min: {d:.2f}%")
        else:
            print("❌ No candidate sequence was generated.")
        print("\n=== All Candidate Combination (Diverse) ===")
        for idx, cand in enumerate(results):
            seq_print = [int(t) for t in cand['sequence']]
            print(f"{idx+1:3d}. | Points: {seq_print} | Length: {len(seq_print)} | f2: {cand['f2']} | Compliant: {cand['compliant']}")

