#utils.py
import pandas as pd
import numpy as np
from summit.domain import Domain, ContinuousVariable
from typing import List, Dict , Any
from summit.strategies import TSEMO,Random, SNOBFIT
from summit.utils.dataset import DataSet

def HPLC_data_read_csv(file_path: str) -> np.ndarray:
    """
    Reads HPLC data from a CSV file.
    Assumes the CSV has 'Area' and 'RT' columns.
    Returns a NumPy array with [Area, RT].
    """
    try:
        data = pd.read_csv(file_path)
        data_final = pd.DataFrame()
        data_final['Peak_Area'] = data['Area']
        data_final['RT'] = data['RT']
        return data_final.to_numpy()
    except Exception as e:
        print(f"Error reading HPLC CSV file {file_path}: {e}")
        return np.array([]) # Return empty array on error

def impurity_response_csv(data_np: np.ndarray, IminRT: float, ImaxRT: float, areaISO: float) -> float:
    """
    Calculates impurity response based on peak area within a given RT range.
    data_np: NumPy array with [Area, RT]
    IminRT, ImaxRT: Retention time range for the impurity
    areaISO: Isocratic peak area for normalization
    """
    areaB = 0
    for i in range(data_np.shape[0]):
        if IminRT <= data_np[i, 1] <= ImaxRT:
            areaB += data_np[i, 0]
    return areaB / areaISO if areaISO != 0 else 0.0

def response_HPLC_csv(
    data_np: np.ndarray,
    YminRT: float, YmaxRT: float,
    IminRT_list: List[float], ImaxRT_list: List[float],
    minRTISO: float, maxRTISO: float,
    nobj: int
) -> List[float]:
    """
    Computes yield and impurity responses from HPLC data.
    data_np: NumPy array with [Area, RT]
    YminRT, YmaxRT: RT range for the main product (yield)
    IminRT_list, ImaxRT_list: Lists of RT ranges for multiple impurities
    minRTISO, maxRTISO: RT range for the internal standard (isocratic)
    nobj: Number of objectives (including yield and impurities)
    """
    if data_np.size == 0:
        # Return default values if data is empty
        return [float('inf')] * nobj

    areaA = 0
    for i in range(data_np.shape[0]):
        if YminRT <= data_np[i, 1] <= YmaxRT:
            areaA += data_np[i, 0]

    areaISO = 0
    for i in range(data_np.shape[0]):
        if minRTISO <= data_np[i, 1] <= maxRTISO:
            areaISO += data_np[i, 0]

    response = []
    # Yield calculation (negative log for minimization)
    yield_result = areaA / areaISO if areaISO != 0 else 0.0
    response.append(-np.log(yield_result) if yield_result > 0 else float('inf')) # Handle log(0)

    # Impurities calculation
    # nobj includes yield, so iterate for nobj-1 impurities
    for i in range(nobj - 1):
        if i < len(IminRT_list) and i < len(ImaxRT_list):
            impurities_result = impurity_response_csv(data_np, IminRT_list[i], ImaxRT_list[i], areaISO)
            response.append(impurities_result)
        else:
            response.append(0.0) # Default if impurity range is missing

    return response

def monitor_folder_creation1_csv(
    file_path: str,
    nobj: List,
    YminRT: float, YmaxRT: float,
    IminRT: float, ImaxRT: float,
    minRTISO: float, maxRTISO: float
) -> Dict[str, Dict[str, List[float]]]:
    """
    Processes a single HPLC CSV file and returns the calculated responses.
    This function wraps the HPLC data reading and response calculation.
    filepath = new file path (excel file path)
    nobj = Objective data objectives names 
    """
    data = HPLC_data_read_csv(file_path)
    response = response_HPLC_csv(data,YminRT,YmaxRT,IminRT,ImaxRT,minRTISO,maxRTISO,nobj)
    return response

def process_hplc_and_fill(filename: str, data_np: np.ndarray,lhs: pd.DataFrame,objectives: list,nobj,hplc_params: dict):
    """Process HPLC data and update results."""
    nobj = max(1, len(objectives))
    # Calculate response
    resp = response_HPLC_csv(
        data_np,
        hplc_params.get("YminRT", 2.0),
        hplc_params.get("YmaxRT", 4.0),
        hplc_params.get("IminRT_list", [0.5]),
        hplc_params.get("ImaxRT_list", [1.0]),
        hplc_params.get("minRTISO", 10.0),
        hplc_params.get("maxRTISO", 12.0),
        nobj
    )

    return resp

def process_uploaded_csv_file(full_path_uploaded_csv,
                              df,
                              minRTISO,
                              maxRTISO,
                              YminRT,
                              YmaxRT,
                              IminRT_list,
                              ImaxRT_list,
                              transformed_objectives,
                              lhs ):
    if 'Area' in df.columns and 'RT' in df.columns:
            data_np = df[['Area', 'RT']].to_numpy()
    else:
        cols_lower = {c.lower(): c for c in df.columns}
        if 'area' in cols_lower and 'rt' in cols_lower:
            data_np = df[[cols_lower['area'], cols_lower['rt']]].to_numpy()
    hplc_params = {}
    if YminRT is not None: hplc_params['YminRT'] = YminRT
    if YmaxRT is not None: hplc_params['YmaxRT'] = YmaxRT
    if IminRT_list is not None:
        hplc_params['IminRT_list'] = [float(x) for x in IminRT_list if str(x).strip() != '']

    if ImaxRT_list is not None:
        hplc_params['ImaxRT_list'] = [float(x) for x in ImaxRT_list if str(x).strip() != '']

    if minRTISO is not None: hplc_params['minRTISO'] = float(minRTISO)
    if maxRTISO is not None: hplc_params['maxRTISO'] = float(maxRTISO)

    resp = process_hplc_and_fill(filename = full_path_uploaded_csv, 
                                 data_np = data_np,
                                 lhs = lhs,
                                 objectives = transformed_objectives,
                                 nobj = len(transformed_objectives),
                                 hplc_params = hplc_params)
    return resp

# def build_domain_from_df(df: pd.DataFrame, objectives: List[Dict[str, Any]]):
#     """Build optimization domain from DataFrame."""
#     numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
#     domain = Domain()
#     if len(numeric_cols) == 0:
#         raise ValueError("No numeric columns to build domain.")

#     for col in numeric_cols[:5]:
#         sanitized_name = str(col).replace(" ", "_")  # Replace spaces with underscores
#         lb = float(df[col].min())
#         ub = float(df[col].max())
#         if lb == ub:
#             lb -= 1e-6
#             ub += 1e-6
#         domain += ContinuousVariable(name=sanitized_name, description=str(col), bounds=[lb, ub])

#     # Add objectives
#     for obj in objectives:
#         obj_name = obj.get("name", "obj").replace(" ", "_")  # Sanitize objective name
#         maximize = bool(obj.get("maximize", False))
#         domain += ContinuousVariable(
#             name=obj_name,
#             description=obj_name,
#             bounds=[0, 100],
#             is_objective=True,
#             maximize=maximize
#         )
#     return domain


# def run_summit_optimization(domain, lhs_df: pd.DataFrame, nobj: int):
#     """Run Summit optimization."""
    
#     if nobj > 1:
#         strat = TSEMO(domain, random_rate=0.00, n_spectral_points=4000)
#         print(lhs_df.columns.nlevels)
#         lhs_df.columns = pd.MultiIndex.from_tuples(
#             [(col[0], f"{col[1]}_lhs") for col in lhs_df.columns],
#             names=lhs_df.columns.names
#         )
#         print(lhs_df,'__________________________')

#         lhs_ds = DataSet.from_df(lhs_df)
#         print(lhs_ds)
#         out = strat.suggest_experiments(1, lhs_ds, use_spectral_sample=True, pop_size=100, iterations=100)
#     else:
#         strat = SNOBFIT(domain)
#         lhs_ds = DataSet.from_df(lhs_df)
#         out = strat.suggest_experiments(1, lhs_ds)
    
#     try:
#         out.columns = [col[0] for col in out.columns]
#     except Exception:
#         pass
    
#     if "strategy" in out.columns:
#         out = out.drop(columns={"strategy"})
    
#     common = [c for c in lhs_df.columns if c in out.columns]
#     if len(common) > 0:
#         out = out[common]
#     return out


# def suggest_experiments_and_append(num_suggestions: int, objectives: List[Dict[str, Any]],lhs):
#     try:
#         print("In Domain")
#         domain = build_domain_from_df(lhs, objectives)
#         print("Out Domain")
#     except Exception as e:
#         raise RuntimeError(f"Domain build failed: {e}")
#     suggestions = []
#     for _ in range(num_suggestions):
#         print("in run optimization")
#         out = run_summit_optimization(domain, lhs, len(objectives) or 1)
#         print("out run optimization")
#         if isinstance(out, pd.DataFrame) and out.shape[0] >= 1:
#             suggested = out.iloc[0].to_dict()

#         new_row = {c: (np.nan if c not in suggested else suggested[c]) for c in lhs.columns}
#         suggestions.append(new_row)
#         lhs = pd.concat([lhs, pd.DataFrame([new_row])], ignore_index=True)
#     return lhs


def build_domain_from_df(df: pd.DataFrame, objectives: List[Dict[str, Any]]):
    """Build optimization domain from DataFrame."""
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    domain = Domain()
    if len(numeric_cols) == 0:
        raise ValueError("No numeric columns to build domain.")

    # Get objective column names to exclude them from input variables
    objective_names = [obj.get("name", "").replace(" ", "_") for obj in objectives]
    
    # Add ALL numeric columns as input variables (except objectives)
    for col in numeric_cols:
        sanitized_name = str(col).replace(" ", "_")  # Replace spaces with underscores
        
        # Skip if this column is an objective
        if sanitized_name in objective_names:
            continue
            
        # Calculate bounds from existing data
        col_data = df[col].dropna()  # Remove NaN values for bound calculation
        if len(col_data) > 0:
            lb = float(col_data.min())
            ub = float(col_data.max())
            if lb == ub:
                lb -= 1e-6
                ub += 1e-6
        else:
            # Default bounds if no data available
            lb, ub = 0.0, 1.0
            
        domain += ContinuousVariable(name=sanitized_name, description=str(col), bounds=[lb, ub])

    # Add objectives
    for obj in objectives:
        obj_name = obj.get("name", "obj").replace(" ", "_")  # Sanitize objective name
        maximize = bool(obj.get("maximize", False))
        domain += ContinuousVariable(
            name=obj_name,
            description=obj_name,
            bounds=[0, 100],
            is_objective=True,
            maximize=maximize
        )
    return domain




def run_summit_optimization(domain, lhs_df: pd.DataFrame, nobj: int):
    """Run Summit optimization."""
    
    print(f"Starting optimization with {len(lhs_df)} existing rows")
    print(f"Domain has {len([v for v in domain.variables if not v.is_objective])} input vars, {len([v for v in domain.variables if v.is_objective])} objectives")
    
    # Prepare DataFrame for Summit - ensure proper column alignment
    domain_var_names = [var.name for var in domain.variables if not var.is_objective]
    domain_obj_names = [var.name for var in domain.variables if var.is_objective]
    
    print(f"Domain input variables: {domain_var_names}")
    print(f"Domain objectives: {domain_obj_names}")
    print(f"Original DataFrame columns: {list(lhs_df.columns)}")
    
    # Create a clean DataFrame with only the columns that match domain variables
    lhs_clean = pd.DataFrame()
    
    # Map input variables
    for var_name in domain_var_names:
        # Try to find matching column in original DataFrame
        found = False
        for col in lhs_df.columns:
            col_clean = str(col).replace(" ", "_").replace("(", "").replace(")", "").replace("-", "_")
            if col_clean == var_name or str(col) == var_name:
                lhs_clean[var_name] = lhs_df[col]
                found = True
                print(f"Mapped {col} -> {var_name}")
                break
        
        if not found:
            print(f"WARNING: Could not find column for variable {var_name}")
            # If no matching column found, fill with NaN (this shouldn't happen with proper domain building)
            lhs_clean[var_name] = np.nan
    
    # Map objective variables  
    for obj_name in domain_obj_names:
        found = False
        for col in lhs_df.columns:
            col_clean = str(col).replace(" ", "_").replace("(", "").replace(")", "").replace("-", "_")
            if col_clean == obj_name or str(col) == obj_name:
                lhs_clean[obj_name] = lhs_df[col]
                found = True
                print(f"Mapped objective {col} -> {obj_name}")
                break
        
        if not found:
            # For objectives, we might not have data yet
            lhs_clean[obj_name] = np.nan
            print(f"No data for objective {obj_name}, using NaN")
    
    print(f"Clean DataFrame shape: {lhs_clean.shape}")
    print(f"Clean DataFrame columns: {list(lhs_clean.columns)}")
    
    # Remove rows where ALL input variables are NaN (these can't be used for optimization)
    input_vars_df = lhs_clean[domain_var_names]
    valid_rows = ~input_vars_df.isna().all(axis=1)
    lhs_clean_valid = lhs_clean[valid_rows].copy()
    
    print(f"Valid rows for optimization: {len(lhs_clean_valid)}")
    
    # Ensure we have some valid data
    if len(lhs_clean_valid) == 0:
        print("WARNING: No valid data rows found for optimization - generating random suggestion")
        # Return a random suggestion based on domain bounds
        out = pd.DataFrame()
        for var in domain.variables:
            if not var.is_objective:
                # Generate random value within bounds
                low, high = var.bounds
                random_val = np.random.uniform(low, high)
                out[var.name] = [random_val]
                print(f"Random suggestion for {var.name}: {random_val} (bounds: {low}-{high})")
        print(f"Generated random DataFrame shape: {out.shape}")
        return out
    
    # Create DataSet for Summit
    try:
        print("Creating DataSet...")
        lhs_ds = DataSet.from_df(lhs_clean_valid)
        print(f"DataSet created successfully with {len(lhs_ds)} rows")
    except Exception as e:
        print(f"Failed to create DataSet: {e}")
        print("Generating random fallback...")
        # Fallback to random suggestion
        out = pd.DataFrame()
        for var in domain.variables:
            if not var.is_objective:
                low, high = var.bounds
                random_val = np.random.uniform(low, high)
                out[var.name] = [random_val]
        return out
    
    # Run optimization
    if nobj > 1:
        try:
            print("Trying TSEMO optimization...")
            strat = TSEMO(domain, random_rate=0.00, n_spectral_points=min(4000, len(lhs_clean_valid) * 100))
            out = strat.suggest_experiments(1, lhs_ds, use_spectral_sample=True, pop_size=50, iterations=50)
            print("TSEMO optimization successful")
        except Exception as e:
            print(f"TSEMO failed with error: {e}")
            print("Falling back to SNOBFIT...")
            try:
                strat = SNOBFIT(domain)
                out = strat.suggest_experiments(1, lhs_ds)
                print("SNOBFIT optimization successful")
            except Exception as e2:
                print(f"SNOBFIT also failed: {e2}")
                print("Using random fallback...")
                # Final fallback to random
                out = pd.DataFrame()
                for var in domain.variables:
                    if not var.is_objective:
                        low, high = var.bounds
                        random_val = np.random.uniform(low, high)
                        out[var.name] = [random_val]
                print(f"Random fallback generated: {out.to_dict('records')[0] if len(out) > 0 else 'EMPTY'}")
                return out
    else:
        try:
            print("Using SNOBFIT optimization...")
            strat = SNOBFIT(domain)
            out = strat.suggest_experiments(1, lhs_ds)
            print("SNOBFIT optimization successful")
        except Exception as e:
            print(f"SNOBFIT failed: {e}")
            print("Using random fallback...")
            # Fallback to random
            out = pd.DataFrame()
            for var in domain.variables:
                if not var.is_objective:
                    low, high = var.bounds
                    random_val = np.random.uniform(low, high)
                    out[var.name] = [random_val]
            return out
    
    # Clean up output
    if "strategy" in out.columns:
        out = out.drop(columns=["strategy"])
    
    print(f"Final optimization output shape: {out.shape}")
    print(f"Final optimization output columns: {list(out.columns)}")
    
    # Handle MultiIndex columns if present
    if isinstance(out.columns, pd.MultiIndex):
        print("MultiIndex columns detected, flattening...")
        # Flatten MultiIndex columns by taking the first level or combining levels
        new_columns = []
        for col in out.columns:
            if isinstance(col, tuple):
                # Take the first non-empty element from the tuple
                name = next((str(x) for x in col if str(x).strip() and str(x) != 'nan'), str(col[0]))
                new_columns.append(name)
            else:
                new_columns.append(str(col))
        out.columns = new_columns
        print(f"Flattened columns: {list(out.columns)}")
    
    # Show a sample of the actual data
    if len(out) > 0:
        sample_dict = {}
        for col in out.columns[:5]:  # Show first 5 columns as sample
            sample_dict[col] = out[col].iloc[0]
        print(f"Sample output data: {sample_dict}")
    
    return out


def suggest_experiments_and_append(num_suggestions: int, objectives: List[Dict[str, Any]], lhs):
    try:
        print("Building domain...")
        domain = build_domain_from_df(lhs, objectives)
        print(f"Domain created successfully")
        print(f"Domain has {len([v for v in domain.variables if not v.is_objective])} input variables and {len([v for v in domain.variables if v.is_objective])} objectives")
        
        # Print domain details
        input_vars = [v.name for v in domain.variables if not v.is_objective]
        obj_vars = [v.name for v in domain.variables if v.is_objective]
        print(f"Input variables: {input_vars}")
        print(f"Objective variables: {obj_vars}")
        
    except Exception as e:
        raise RuntimeError(f"Domain build failed: {e}")
    
    suggestions = []
    for i in range(num_suggestions):
        print(f"\n=== Generating suggestion {i+1}/{num_suggestions} ===")
        try:
            out = run_summit_optimization(domain, lhs, len(objectives) or 1)
            print(f"Optimization returned: {type(out)}, shape: {out.shape if hasattr(out, 'shape') else 'N/A'}")
            
            if isinstance(out, pd.DataFrame) and out.shape[0] >= 1:
                print(f"Optimization output shape: {out.shape}")
                print(f"Optimization output columns: {list(out.columns)}")
                
                # Convert to dictionary - handle potential MultiIndex columns
                try:
                    suggested = out.iloc[0].to_dict()
                    print(f"Raw suggestions from optimization: {dict(list(suggested.items())[:5])}...")  # Show first 5
                except Exception as e:
                    print(f"Error converting output to dict: {e}")
                    suggested = {}
            else:
                suggested = {}
                print("WARNING: No suggestions returned from optimization, using empty dict")

            # Create new row with proper column mapping
            new_row = {}
            print("Mapping suggestions to original columns:")
            
            for col in lhs.columns:
                # Check if we have a suggestion for this column
                col_clean = str(col).replace(" ", "_").replace("(", "").replace(")", "").replace("-", "_")
                
                if col_clean in suggested:
                    new_row[col] = suggested[col_clean]
                    print(f"  {col} <- {col_clean} = {suggested[col_clean]}")
                elif col in suggested:
                    new_row[col] = suggested[col]
                    print(f"  {col} = {suggested[col]}")
                else:
                    # For objective columns or unmapped columns, use NaN
                    new_row[col] = np.nan
                    print(f"  {col} = NaN (no suggestion)")
            
            print(f"Final new row: {new_row}")
            suggestions.append(new_row)
            lhs = pd.concat([lhs, pd.DataFrame([new_row])], ignore_index=True)
            print(f"Successfully added suggestion {i+1}, total rows: {len(lhs)}")
            
        except Exception as e:
            print(f"ERROR generating suggestion {i+1}: {e}")
            import traceback
            traceback.print_exc()
            
            # Create a row with all NaN values as fallback
            new_row = {c: np.nan for c in lhs.columns}
            print(f"Using fallback row with all NaN: {new_row}")
            suggestions.append(new_row)
            lhs = pd.concat([lhs, pd.DataFrame([new_row])], ignore_index=True)
    
    print(f"\n=== Final result: {len(suggestions)} suggestions added ===")
    return lhs
######################################
from fastapi import FastAPI, Depends, HTTPException, Request, status, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import OAuth2PasswordBearer
from datetime import datetime, timedelta
from fastapi.responses import JSONResponse
from jose import jwt, JWTError
from pymongo import MongoClient
from bson import ObjectId, json_util
from bson.errors import InvalidId
from typing import Optional
import config, csv, redis, os, uvicorn, utils, json
from openpyxl import Workbook
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import asyncio
import pandas as pd
import time
# JWT Configuration
SECRET_KEY = "myFAVsecretKEY"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 200
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="login")

# MongoDB Setup
try:
    client = MongoClient("mongodb://localhost:27017", serverSelectionTimeoutMS=5000)
    client.server_info()
    print("✅ MongoDB is reachable")
except Exception as e:
    print("❌ MongoDB connection failed:", e)

db = client["self_optimization_reactions"]
collection = db["self_optimization_reactions"]

# Redis Setup
try:
    redis_client = redis.Redis(host='localhost', port=6379, db=12, decode_responses=True)
    redis_client.ping()
    print("✅ Redis is reachable")
except Exception as e:
    print("❌ Redis connection failed:", e)

# FastAPI App Initialization
app = FastAPI(
    title="Self Optimization Reactions (SOR) Server",
    redoc_url=None,
    root_path="/sor/api",
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Assets Directory
ASSETS_DIR = "assets"
os.makedirs(ASSETS_DIR, exist_ok=True)

# JWT Token Generator
def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

# JWT Token Validator
async def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username = payload.get("sub")
        if username is None:
            raise HTTPException(status_code=401, detail="Invalid token")
        return username
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

# Login Endpoint
@app.post("/login")
async def login(user_info: dict = Depends(config.check_ldap_auth)):
    if not user_info:
        raise HTTPException(status_code=400, detail="Invalid Credentials")
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user_info["first_name"]},
        expires_delta=access_token_expires
    )
    login_record = {
        "first_name": user_info["first_name"],
        "last_name": user_info.get("last_name"),
        "email": user_info.get("email"),
        "login_time": datetime.utcnow(),
        "access_token": access_token,
        "type": "login_event",
        "sor_iterations":1,
        "completed":1,

    }
    result = collection.insert_one(login_record)
    return {
        "access_token": access_token,
        "token_type": "Bearer",
        "firstname": user_info["first_name"],
        "_id": str(result.inserted_id)
    }

@app.get("/fetch_by_firstname/{firstname}")
async def fetch_by_firstname(firstname: str, user: str = Depends(get_current_user)):
    try:
        if not firstname:
            raise HTTPException(status_code=400, detail="Firstname is required")

        # Query MongoDB for matching documents
        results = list(collection.find({"first_name": firstname}))

        if not results:
            raise HTTPException(status_code=404, detail=f"No records found for first_name: {firstname}")

        # Convert ObjectId and datetime fields to strings
        for doc in results:
            doc["_id"] = str(doc["_id"])
            if "login_time" in doc and isinstance(doc["login_time"], datetime):
                doc["login_time"] = doc["login_time"].isoformat()

        return JSONResponse(content={"results": results})

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

@app.delete("/delete_by_id/{id}")
async def delete_by_id(id: str, user: str = Depends(get_current_user)):
    try:
        object_id = ObjectId(id)
        result = collection.delete_one({"_id": object_id})

        if result.deleted_count == 0:
            raise HTTPException(status_code=404, detail="Document not found")

        return {"message": "Document deleted successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

# Update Experiment Endpoint
@app.post("/update/{id}")
async def update_experiment(id: str, request: Request, user: str = Depends(get_current_user)):
    try:
        data = await request.json()
        object_id = ObjectId(id)
        key = list(data.keys())[0]
        value = data[key]
        result = collection.update_one({"_id": object_id}, {"$set": {key: value}})
        if result.matched_count == 0:
            raise HTTPException(status_code=404, detail="Experiment not found")
        if key == "optimization_target":
            experiment = collection.find_one({"_id": object_id})
            lhs_response_data = experiment.get("lhs_response")
            objectives = value.get("objectives", {}).get("Objectives", [])
            lhs_table = lhs_response_data.get("table") if isinstance(lhs_response_data, dict) else []
            if lhs_table and isinstance(lhs_table, list):
                input_headers = lhs_table[0]
                data_rows = lhs_table[1:]
                merged_headers = input_headers + objectives
                merged_rows = [row + [""] * len(objectives) for row in data_rows]
                final_result = [merged_headers] + merged_rows
                hplc_path = value.get("hplcPath")
                if hplc_path:
                    excel_path = os.path.join(hplc_path, "optimization.xlsx")
                    try:
                        wb = Workbook()
                        ws = wb.active
                        ws.title = "Optimization Result"
                        for row in final_result:
                            ws.append(row)
                        wb.save(excel_path)
                    except Exception as e:
                        raise HTTPException(status_code=500, detail=f"Failed to write Excel file: {str(e)}")
                collection.update_one({"_id": object_id}, {"$set": {"final_result": final_result}})
        return {"message": "updated successfully"}
    except InvalidId:
        raise HTTPException(status_code=400, detail="Invalid MongoDB ID format")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Get Result Endpoint
@app.get("/result/{id}")
async def get_lhs_response(id: str, user: str = Depends(get_current_user)):
    try:
        if not ObjectId.is_valid(id):
            raise HTTPException(status_code=400, detail="Invalid ID format")
        experiment = collection.find_one({"_id": ObjectId(id)})
        if experiment is None:
            raise HTTPException(status_code=404, detail="Experiment not found")
        serialized_experiment = json.loads(json_util.dumps(experiment))
        return {"data": serialized_experiment}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal Server Error: {str(e)}")

# Get Experiment Data
@app.get("/experiment/{id}")
async def get_experiment(id: str, user: str = Depends(get_current_user)):
    try:
        object_id = ObjectId(id)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid MongoDB ID format")
    experiment = collection.find_one({"_id": object_id})
    if not experiment:
        raise HTTPException(status_code=404, detail="Experiment not found")
    experiment["_id"] = str(experiment["_id"])
    return {"source": "mongodb", "data": experiment}

# Function to merge and save final result
def merge_and_save_final_result(full_path, object_id, response):
    experiment = collection.find_one({"_id": ObjectId(object_id)})
    if not experiment:
        raise ValueError(f"No document found with _id: {object_id}")
    final_result = experiment.get("final_result")
    no_of_LHS = int(experiment.get("lhs_response", {}).get("no_of_LHS", 0))
    completed = int(experiment.get("completed", 0))
    if not isinstance(final_result, list) or len(final_result) <= completed:
        raise ValueError("final_result must contain enough rows to update")

    row = final_result[completed]
    empty_indices = [i for i, val in enumerate(row) if val == '']
    for i, val in zip(empty_indices[-len(response):], response):
        row[i] = val
    final_result[completed] = row
    df = pd.DataFrame(final_result[1:], columns=final_result[0])
    df.to_excel(full_path, index=False)
    collection.update_one(
        {"_id": ObjectId(object_id)},
        {
            "$set": {"final_result": final_result},
            "$inc": {"completed": 1}
        }
    )
    print(f"Row {completed} updated. Excel saved to {full_path} and MongoDB updated.")
    return df

# WebSocket Clients and Watcher
websocket_clients = []
watcher_started = False

# WebSocket Notifier Class with on_created method
class WebSocketNotifier(FileSystemEventHandler):
    def __init__(self, websocket_clients, loop, id):
        self.websocket_clients = websocket_clients
        self.loop = loop
        self.id = id  # Store the id

    def on_created(self,event):
        if not event.is_directory:
            full_path_uploaded_csv = event.src_path# C:\Users\T00009590\Documents\Current Applications\cloud_sor\Report01.csv
            object_id = ObjectId(self.id)
            experiment = collection.find_one({"_id": object_id})
            ## Get Objectives 
            optimization = experiment.get("optimization_target", {})
            raw_objectives = optimization.get("objectives", {})
            lhs = experiment.get('final_result')
            df = pd.DataFrame(lhs[1:], columns=lhs[0])
            llm_response_with_objective_colmns = df.apply(pd.to_numeric, errors='coerce')
            # Transform objectives dynamically
            transformed_objectives = []
            for i, name in enumerate(raw_objectives.get("Objectives", [])):
                obj = {
                    "name": name,
                    "maximize": raw_objectives.get("Condition", [])[i].lower() == "maximize"
                }
                if i < len(raw_objectives.get("Property", [])):
                    obj["Property"] = raw_objectives["Property"][i]
                transformed_objectives.append(obj)

            ## Fetch updated CSV 
            df = pd.read_csv(full_path_uploaded_csv)
            ## Process File
            objectives_rtmin = raw_objectives.get('RTMin')
            objectives_rtmax = raw_objectives.get('RTMax')
            minRTISO = float(optimization.get("rtMinIso", 0))
            maxRTISO = float(optimization.get("rtMaxIso", 0))
            YminRT = float(objectives_rtmin[0])
            YmaxRT = float(objectives_rtmax[0])
            IminRT_list = [float(x) for x in objectives_rtmin[1:]]
            ImaxRT_list = [float(x) for x in objectives_rtmax[1:]]
            full_path = optimization.get("hplcPath", "")
            completed_experiments = experiment.get('completed')
            count_of_lhs = experiment.get('lhs_response').get('no_of_LHS')
            print(completed_experiments)
            print(count_of_lhs)
            if (int(completed_experiments) < int(count_of_lhs)) :
                resp = utils.process_uploaded_csv_file(full_path_uploaded_csv,
                                                    df,
                                                    minRTISO,
                                                    maxRTISO,
                                                    YminRT,
                                                    YmaxRT,
                                                    IminRT_list,
                                                    ImaxRT_list,
                                                    transformed_objectives,
                                                    llm_response_with_objective_colmns)
                full_path = os.path.join(full_path, "optimization.xlsx")
                merge_and_save_final_result(full_path,object_id,resp)
                message = "Processing LHS. Waiting for new file"

            elif (int(completed_experiments) == int(count_of_lhs)):
                resp = utils.process_uploaded_csv_file(full_path_uploaded_csv,
                                                    df,
                                                    minRTISO,
                                                    maxRTISO,
                                                    YminRT,
                                                    YmaxRT,
                                                    IminRT_list,
                                                    ImaxRT_list,
                                                    transformed_objectives,
                                                    llm_response_with_objective_colmns)
                full_path = os.path.join(full_path, "optimization.xlsx")
                res = merge_and_save_final_result(full_path,object_id,resp)
                sor_iterations = experiment.get('optimization_target').get('iterations')
                sor_response_new_line = utils.suggest_experiments_and_append(sor_iterations, transformed_objectives,res)
                collection.update_one({"_id": object_id}, {"$inc": {"sor_iterations": 1}})
                message = "Processing SOR. Waiting for new file"
            else :
                user_sor_iterations = experiment.get('optimization_target').get('iterations')
                sys_sor_iterations = experiment.get('sor_iterations')
                lhs = experiment.get('final_result')
                df = pd.DataFrame(lhs[1:], columns=lhs[0])
                llm_response_with_objective_colmns = df.apply(pd.to_numeric, errors='coerce')
                if (int(user_sor_iterations) < int(sys_sor_iterations)):
                    sor_response_new_line = utils.suggest_experiments_and_append(sor_iterations, transformed_objectives,llm_response_with_objective_colmns)
                    collection.update_one(
                        {"_id": object_id},
                        {
                            "$inc": {"sor_iterations": 1}
                        }
                        )
                    message = "Processing SOR. Waiting for new file"
                elif (int(user_sor_iterations) == int(sys_sor_iterations)):
                    sor_iterations = experiment.get('optimization_target').get('iterations')
                    sor_response_new_line = utils.suggest_experiments_and_append(sor_iterations, transformed_objectives,llm_response_with_objective_colmns)
                    # sor_response_new_line.to_excel(full_path, index=False)
                    final_result  = [sor_response_new_line.columns.tolist()] + sor_response_new_line.values.tolist()
                    collection.update_one(
                        {"_id": object_id},
                            {   "$set": {"current_phase": "done","final_result":final_result},
                                "$inc": {"sor_iterations": 1}
                            }
                        )
                    message = "File Process Completed. Waiting for new file"
            for ws, _ in self.websocket_clients:
                asyncio.run_coroutine_threadsafe(ws.send_text(message), self.loop)


# Function to start watcher
def start_watcher(path, websocket_clients, loop, id):
    event_handler = WebSocketNotifier(websocket_clients, loop, id)
    observer = Observer()
    observer.schedule(event_handler, path=path, recursive=True)
    observer.start()
    return observer

@app.websocket("/ws/file-uploads/{id}")
async def websocket_endpoint(websocket: WebSocket, id: str):
    global watcher_started  # Declare global at the start
    await websocket.accept()
    # Fetch path from database
    object_id = ObjectId(id)
    experiment = collection.find_one({"_id": object_id})
    hplc_path = experiment.get("optimization_target").get("hplcPath")
    websocket_clients.append((websocket, hplc_path))
    loop = asyncio.get_event_loop()
    if not watcher_started:
        observer = start_watcher(hplc_path, websocket_clients, loop, id)
        watcher_started = True
    print(f"WebSocket connected for ID {id} with path: {hplc_path}")
    try:
        while True:
            await websocket.receive_text()
    except Exception as e:
        print(f"WebSocket error: {e}")
    finally:
        websocket_clients.remove((websocket, hplc_path))
        if not websocket_clients:
            observer.stop()
            observer.join()
            watcher_started = False

# Run server
if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000)
