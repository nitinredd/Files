import os
import numpy as np
import pandas as pd
import json
from typing import List
from sklearn.cluster import BisectingKMeans
from sklearn.metrics import silhouette_score, silhouette_samples
from openai import AzureOpenAI
import tiktoken
from tqdm import tqdm

class TopicAnalyzer:
    def __init__(self, azure_endpoint, azure_api_key, azure_deployment, api_version="2024-02-15-preview"):
        """
        Initialize the TopicAnalyzer with Azure OpenAI credentials
        """
        self.client = AzureOpenAI(
            azure_endpoint=azure_endpoint,
            api_key=azure_api_key,
            api_version=api_version,
        )
        self.deployment = azure_deployment
        self.encoding = tiktoken.get_encoding("cl100k_base")

    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        Get embeddings for a list of texts using Azure OpenAI
        """
        embeddings = []
        for text in tqdm(texts, desc="Getting embeddings"):
            response = self.client.embeddings.create(
                input=text,
                model=self.deployment
            )
            embeddings.append(response.data[0].embedding)
        return np.array(embeddings)

    def classify(self, docs: List[str], topics: List[str]) -> np.ndarray:
        """
        Classify documents against topics using embeddings
        """
        doc_embed = self.get_embeddings(docs)
        topic_embed = self.get_embeddings(topics)
        return np.dot(doc_embed, topic_embed.T)

    def cluster(self, docs: List[str], n: int = 20) -> dict:
        """
        Cluster documents using BisectingKMeans
        """
        doc_embed = self.get_embeddings(docs)
        cluster_model = BisectingKMeans(
            init='k-means++',
            n_clusters=n,
            n_init=10,
            max_iter=300
        )
        cluster_model.fit(doc_embed)
        distances = np.linalg.norm(
            doc_embed[:, np.newaxis] - cluster_model.cluster_centers_.T,
            axis=2
        )
        return {
            "label": cluster_model.labels_,
            "score": silhouette_score(doc_embed, cluster_model.labels_),
            "scores": silhouette_samples(doc_embed, cluster_model.labels_),
            "centroid": np.argmin(distances, axis=0),
        }

    def generate_topics(self, clusters: List[List[str]], n_topics: int) -> tuple:
        """
        Generate topics and subtopics using Azure OpenAI
        """
        # Generate subtopics
        subtopic_prompt = f"""Here are {len(clusters)} clusters of documents.
        Suggest 2-4 word topic names for each cluster.
        Return only a JSON array of strings, nothing else.
        Here are the clusters: {json.dumps(clusters)}"""

        subtopic_response = self.client.chat.completions.create(
            model=self.deployment,
            messages=[{"role": "user", "content": subtopic_prompt}],
            temperature=0
        )
        subtopics = json.loads(subtopic_response.choices[0].message.content)

        # Generate main topics
        topic_prompt = f"""Cluster these topics into {n_topics} groups.
        Return only a JSON object with keys as 2-4 word group names and values as arrays of topics.
        Ensure at least 2 topics per group.
        Here are the topics: {json.dumps(subtopics)}"""

        topic_response = self.client.chat.completions.create(
            model=self.deployment,
            messages=[{"role": "user", "content": topic_prompt}],
            temperature=0
        )
        topics = json.loads(topic_response.choices[0].message.content)

        return subtopics, topics

def main():
    # Configuration
    config = {
        "azure_endpoint": "YOUR_AZURE_ENDPOINT",  # e.g., "https://your-resource.openai.azure.com/"
        "azure_api_key": "YOUR_AZURE_API_KEY",
        "azure_deployment": "YOUR_DEPLOYMENT_NAME",
        "filename": "docs.xlsx",
        "text_column": "para",
        "n_subtopics": 25,
        "min_similarity": 0.40
    }
    
    # Initialize analyzer
    analyzer = TopicAnalyzer(
        azure_endpoint=config["azure_endpoint"],
        azure_api_key=config["azure_api_key"],
        azure_deployment=config["azure_deployment"]
    )

    # Read documents
    print("Reading documents...")
    docs = pd.read_excel(config["filename"]).dropna(subset=[config["text_column"]]).fillna('').astype(str)
    
    # Cluster documents
    print("Clustering documents...")
    result = analyzer.cluster(docs[config["text_column"]].tolist(), n=config["n_subtopics"])
    docs['cluster'] = result['label']
    docs['score'] = result['scores']
    
    # Get representative documents for each cluster
    clusters = (
        docs.groupby('cluster')
        .apply(lambda group: group.nlargest(3, 'score')[config["text_column"]].tolist())
        .tolist()
    )

    # Generate topics
    print("Generating topics...")
    n_topics = int(config["n_subtopics"] ** 0.5)
    subtopics, topic_groups = analyzer.generate_topics(clusters, n_topics)
    
    # Create topics DataFrame
    topics = pd.DataFrame([
        {'topic': topic, 'subtopic': subtopic}
        for topic, subtopics in topic_groups.items()
        for subtopic in subtopics
    ])

    # Calculate similarities
    print("Calculating similarities...")
    data = {
        'docs': docs.to_dict(orient='records'),
        'topics': topics.to_dict(orient='records'),
    }
    
    matches = []
    similarity = analyzer.classify(
        [row[config["text_column"]] for row in data['docs']],
        [row['subtopic'] for row in data['topics']]
    )

    for row in range(len(similarity)):
        for col in range(len(similarity[row])):
            if similarity[row][col] > config["min_similarity"]:
                matches.append({
                    'doc': row,
                    'topic': col,
                    'similarity': float(similarity[row][col])
                })
    
    data['matches'] = matches

    # Save results
    print("Saving results...")
    with pd.ExcelWriter('docexplore.xlsx') as writer:
        docs.to_excel(writer, sheet_name='docs', index=False)
        topics.to_excel(writer, sheet_name='topics', index=False)
        
        grid = pd.DataFrame(matches).pivot_table(
            index='doc',
            columns='topic',
            values='similarity'
        )
        grid.index = pd.Series(grid.index).replace(dict(enumerate(docs[config["text_column"]].tolist())))
        grid.columns = pd.Series(grid.columns).replace(dict(enumerate(topics['subtopic'].tolist())))
        grid.index.name = config["text_column"]
        grid.reset_index().to_excel(writer, sheet_name='matches', index=False)

    with open("docexplore.json", "w") as handle:
        handle.write(json.dumps(data, indent=2))

    print("Process complete!")
    print("Results saved to:")
    print("1. docexplore.xlsx (sheets: docs, topics, matches)")
    print("2. docexplore.json")

if __name__ == "__main__":
    main()
