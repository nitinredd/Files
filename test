import os
import json
from langchain_openai import AzureChatOpenAI
from langchain.schema import HumanMessage
filename = r"C:\Users\\Desktop\WORK\RA\SOP Documents\Chunks\Sop_Test.xlsx"
output_dir = "DRL_annual_report_output"
os.makedirs(output_dir, exist_ok=True)
text_column = 'para'
# n_subtopics = 10
n_subtopics = 10
print("Number of subtopics:", n_subtopics)

# OPTIONAL: Pick the rough number of topics you want
# n_topics = int(n_subtopics ** 0.5)
n_topics = 30
print("Number of topics:", n_topics)

# OPTIONAL: Ignore any similarities less than min_similarity.
# 0.75 is a good setting for text-embedding-ada-002
# 0.40 is a good setting for text-embedding-3-small/large
min_similarity = 0.75

base_url=""
api_version=""

api_key=""
deployment_name="GPT4o"
model_name="GPT4o"

import os
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore

from langchain_openai import AzureOpenAIEmbeddings

file_store = LocalFileStore('langchain-embeddings')

base = AzureOpenAIEmbeddings(
    model="",
    api_version="",
    # base_url=base_url,
    azure_endpoint="",
    # openai_api_type="azure"
    api_key="",
    azure_deployment=""
)

cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# Define classify() and cluster() functions
import numpy as np
from typing import List
from sklearn.cluster import BisectingKMeans
from sklearn.metrics import silhouette_score, silhouette_samples


def classify(docs: List[str], topics: List[str], **kwargs):
    doc_embed = np.array(cached_embeddings.embed_documents(docs))
    topic_embed = np.array(cached_embeddings.embed_documents(topics))
    return np.dot(doc_embed, topic_embed.T)


def cluster(docs: List[str], n: int = 20, **kwargs):
    # Cluster the documents
    cluster_model = BisectingKMeans(init='k-means++', n_clusters=n, n_init=10, max_iter=1000)
    doc_embed = np.array(cached_embeddings.embed_documents(docs))
    cluster_model.fit(doc_embed)
    # Calculate the distance from each point to each centroid
    distances = np.linalg.norm(doc_embed[:, np.newaxis] - cluster_model.cluster_centers_, axis=2)
    return {
        "label": cluster_model.labels_,
        "score": silhouette_score(doc_embed, cluster_model.labels_),
        "scores": silhouette_samples(doc_embed, cluster_model.labels_),
        "centroid": np.argmin(distances, axis=0),
    }

# Create topics by clustering
import pandas as pd

docs = pd.read_excel(filename).dropna(subset=text_column).fillna('').astype(str)
result = cluster(docs[text_column].tolist(), n=n_subtopics)
docs['cluster'] = result['label']
docs['score'] = result['scores']
clusters = (
    docs.groupby('cluster')
    .apply(lambda group: group.nlargest(3, 'score')[text_column].tolist())
    .tolist()
)

chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    # base_url=base_url,
    api_key=api_key,
    azure_endpoint=base_url
)
messages = [
    HumanMessage(
        content=f'''Here are {len(clusters)} clusters of documents.
Suggest 2-4 word topic names for each cluster.
Return a JSON string array of length {len(clusters)}.

{json.dumps(clusters, indent=2)}'''
    )
]

subtopic_response = chat_model.invoke(messages)

import re

match = re.search(r'```json(.*?)```', subtopic_response.content, re.DOTALL)
subtopics = json.loads(match.group(1) if match else subtopic_response.content)
subtopics

# Create higher-level topic groups
messages = [
    HumanMessage(
        content=f'''Cluster these topics into {n_topics} groups.
            Return a JSON object with keys as a 2-4 word group name and values as arrays of topics.
            Ensure at least 2 topics per group.
        {json.dumps(subtopics, indent=2)}'''
            )
        ]

topic_response = chat_model.invoke(messages)

match = re.search(r'```json(.*?)```', topic_response.content, re.DOTALL)
topics = json.loads(match.group(1) if match else topic_response.content)
topics = pd.DataFrame([
    {'topic': topic, 'subtopic': subtopic}
    for topic, subtopics in topics.items()
    for subtopic in subtopics
])

data = {
    'docs': docs.to_dict(orient='records'),
    'topics': topics.to_dict(orient='records'),
}


matches = data['matches'] = []
similarity = classify(
    [row[text_column] for row in data['docs']],
    [row['subtopic'] for row in data['topics']]
)

for row in range(len(similarity)):
    for col in range(len(similarity[row])):
        if similarity[row][col] > min_similarity:
            matches.append({'doc': row, 'topic': col, 'similarity': similarity[row][col]})

# Save as Excel
with pd.ExcelWriter('docexplore.xlsx') as writer:
    docs.to_excel(writer, sheet_name='docs', index=False)
    topics.to_excel(writer, sheet_name='topics', index=False)
    grid = pd.DataFrame(matches).pivot_table(index='doc', columns='topic', values='similarity')
    grid.index = pd.Series(grid.index).replace(dict(enumerate(docs[text_column].tolist())))
    grid.columns = pd.Series(grid.columns).replace(dict(enumerate(topics['subtopic'].tolist())))
    grid.index.name = text_column
    grid.reset_index().to_excel(writer, sheet_name='matches', index=False)

with open(f"{output_dir}/docexplore.json", "w") as handle:
    handle.write(json.dumps(data, indent=2))

