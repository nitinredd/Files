import os
import io
import faiss
import torch
import clip
import camelot
import tabula
import pandas as pd
import streamlit as st
from PIL import Image
from PyPDF2 import PdfReader
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain.schema import Document
from langchain.retrievers import VectorStoreRetriever

# --- CONFIG & CREDENTIALS ---------------------------------------------------

st.set_page_config(page_title="Reaction Database AI", layout="wide")

AZURE_ENDPOINT   = os.getenv("AZURE_ENDPOINT")
AZURE_API_KEY    = os.getenv("AZURE_API_KEY")
CHAT_DEPLOYMENT  = "GPT4o"
EMBED_DEPLOYMENT = "Def_data_qa"
EMBED_MODEL      = "text-embedding-ada-002"

# folders keyed by reaction class
REACTION_FOLDERS = {
    "C-N Bond Formation": r"/path/to/C-N_Bond_Formation",
    # add your others...
}

# --- MODELS -----------------------------------------------------------------

@st.cache_resource(show_spinner=False)
def load_clip():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)
    return model, preprocess, device

clip_model, clip_preprocess, clip_device = load_clip()

@st.cache_resource(show_spinner=False)
def get_text_embedder():
    fs = LocalFileStore("langchain-embeddings")
    base = AzureOpenAIEmbeddings(
        model=EMBED_MODEL,
        api_version="2023-07-01-preview",
        azure_endpoint=AZURE_ENDPOINT,
        api_key=AZURE_API_KEY,
        azure_deployment=EMBED_DEPLOYMENT
    )
    return CacheBackedEmbeddings.from_bytes_store(base, fs, namespace=base.model)

text_embedder = get_text_embedder()

@st.cache_resource(show_spinner=False)
def get_chat_model():
    return AzureChatOpenAI(
        azure_deployment=CHAT_DEPLOYMENT,
        model="GPT4o",
        api_version="2024-02-15-preview",
        api_key=AZURE_API_KEY,
        azure_endpoint=AZURE_ENDPOINT
    )

chat_model = get_chat_model()

# --- UTILITIES --------------------------------------------------------------

def chunk_text(text, max_chars=5000):
    """Simple splitter by characters to avoid giant GPT prompts."""
    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]

def extract_tables_from_pdf(pdf_path):
    """Run both Camelot and Tabula on each page, return list of DataFrames."""
    tables = []
    try:
        for page in camelot.read_pdf(pdf_path, pages="all", flavor="stream"):
            tables.append(page.df)
    except Exception:
        pass
    try:
        df_list = tabula.read_pdf(pdf_path, pages="all", multiple_tables=True)
        tables.extend(df_list)
    except Exception:
        pass
    return tables

def extract_images_and_schemes(pdf_path):
    """Pulled all images + detect 'Scheme' in nearby text blocks."""
    imgs = []
    doc = fitz.open(pdf_path)
    text_by_page = [p.get_text("text") for p in doc]
    for pageno in range(len(doc)):
        blocks = doc[pageno].get_text("blocks")
        # detect "Scheme" caption
        for b in blocks:
            if "scheme" in b[4].lower():
                # mark this page as scheme page
                imgs.append(("caption", pageno, None))
        # extract each image
        for img_idx, img_obj in enumerate(doc[pageno].get_images(full=True)):
            xref = img_obj[0]
            img_bytes = doc.extract_image(xref)["image"]
            img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
            # compute CLIP embedding
            inp = clip_preprocess(img).unsqueeze(0).to(clip_device)
            with torch.no_grad():
                emb = clip_model.encode_image(inp).cpu().numpy().reshape(-1)
            imgs.append(("image", pageno, img, emb))
    return imgs

def build_faiss_index(embeddings: list, dim: int):
    index = faiss.IndexFlatL2(dim)
    arr = np.vstack(embeddings).astype("float32")
    index.add(arr)
    return index

# --- BUILD VECTOR STORES ----------------------------------------------------

@st.cache_data(show_spinner=True)
def initialize_stores():
    text_stores = {}
    image_stores = {}
    for name, folder in REACTION_FOLDERS.items():
        text_docs, text_embeds = [], []
        img_meta, img_embeds = [], []

        for fname in os.listdir(folder):
            if not fname.lower().endswith(".pdf"):
                continue
            path = os.path.join(folder, fname)

            # TEXT + TABLES
            reader = PdfReader(path)
            full_text = "\n".join(p.extract_text() or "" for p in reader.pages)
            chunks = chunk_text(full_text)
            for i, c in enumerate(chunks):
                text_docs.append(Document(page_content=c, metadata={"source": fname, "chunk": i}))
            # embed text
            emb = text_embedder.embed_documents([d.page_content for d in text_docs[-len(chunks):]])
            text_embeds.extend(emb)

            # IMAGES + SCHEMES
            items = extract_images_and_schemes(path)
            for it in items:
                if it[0]=="image":
                    _, pg, img, e = it
                    img_meta.append({"source": fname, "page": pg})
                    img_embeds.append(e)
                else:
                    # caption placeholder
                    _, pg, _ = it
                    img_meta.append({"source": fname, "page": pg, "caption":"Synthetic Scheme here"})
                    img_embeds.append(np.zeros_like(img_embeds[0]) if img_embeds else np.zeros(512))

        # build FAISS vector indices
        txt_index = build_faiss_index(text_embeds, len(text_embeds[0]))
        img_index = build_faiss_index(img_embeds, len(img_embeds[0]))

        text_stores[name]  = {"index": txt_index, "docs": text_docs}
        image_stores[name] = {"index": img_index,  "meta": img_meta}

    return text_stores, image_stores

text_stores, image_stores = initialize_stores()


# --- RETRIEVAL & RENDERING --------------------------------------------------

def retrieve_text(name, query, k=5):
    q_emb = text_embedder.embed_query(query)
    D, I = text_stores[name]["index"].search(q_emb.reshape(1,-1), k)
    return [ text_stores[name]["docs"][i] for i in I[0] ]

def retrieve_images(name, query, k=3):
    # use CLIP text encoder to generate image-retrieval embedding
    text_token = clip.tokenize([query]).to(clip_device)
    with torch.no_grad():
        txt_e = clip_model.encode_text(text_token).cpu().numpy().reshape(-1)
    D, I = image_stores[name]["index"].search(txt_e.reshape(1,-1), k)
    return [ image_stores[name]["meta"][i] for i in I[0] ]

# --- UI ---------------------------------------------------------------------

st.title("üî¨ Reaction Database AI")

react_type = st.selectbox("Choose Reaction Class", list(REACTION_FOLDERS.keys()))
user_q     = st.text_input("Ask about this reaction:")

if user_q:
    # 1. retrieve text
    hits = retrieve_text(react_type, user_q, k=5)
    st.subheader("üìù Procedure, Yield & Tabular Data")
    for doc in hits:
        st.markdown(f"**Source:** {doc.metadata['source']} (chunk {doc.metadata['chunk']})")
        # if looks like a table chunk, try parsing as DataFrame
        if "\n" in doc.page_content and doc.page_content.strip().startswith("|"):
            df = pd.read_csv(io.StringIO(doc.page_content), sep="|", engine="python")
            st.dataframe(df)
        else:
            st.text_area("", doc.page_content, height=200)

    # 2. retrieve images
    ims = retrieve_images(react_type, user_q, k=3)
    st.subheader("üîé Synthetic Schemes & Figures")
    for m in ims:
        # display captions
        if "caption" in m:
            st.markdown(f"*Scheme detected on page {m['page']} of {m['source']}*")
        else:
            # load actual image from PDF again
            pdf = fitz.open(os.path.join(REACTION_FOLDERS[react_type], m["source"]))
            xref = pdf[m["page"]-1].get_images(full=True)[0][0]
            img = Image.open(io.BytesIO(pdf.extract_image(xref)["image"]))
            st.image(img, caption=f"{m['source']} ‚Äî page {m['page']}", use_column_width=True)

    # 3. answer via ChatGPT for summary / linking
    system = SystemMessage(content="You are an expert synthetic chemist. Summarize and correlate the retrieved content.")
    human  = HumanMessage(content=user_q)
    answer = chat_model([system, human])
    st.subheader("ü§ñ AI Summary")
    st.write(answer.content)
