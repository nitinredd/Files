import os
import json
import pandas as pd
import streamlit as st
from langchain_openai import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from gtts import gTTS
import speech_recognition as sr
import io
import uuid

recognizer = sr.Recognizer()

# Azure GPT Configuration
base_url = ""
api_version = "2024-02-15-preview"
api_key = ""
deployment_name = "GPT4o"
model_name = "GPT4o"

# Paths to embedded JSON files
embedded_files = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Test_1.json",
    'data_2': None,
    'data_3': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Formulas.json",
}

# Initialize embeddings & chat_model
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment=""
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# Voice helpers
def recognize_google_stt():
    with sr.Microphone() as source:
        st.info("üéôÔ∏è Listening‚Ä¶ speak now")
        audio = recognizer.listen(source, phrase_time_limit=5)
        st.success("üîç Processing‚Ä¶")
        try:
            return recognizer.recognize_google(audio)
        except sr.UnknownValueError:
            return "Sorry, I couldn't understand your voice."
        except sr.RequestError as e:
            return f"Error: {e}"

def tts_stream(text: str):
    buf = io.BytesIO()
    gTTS(text).write_to_fp(buf)
    buf.seek(0)
    return buf

# Multi-Agent classes (unchanged core logic except the no-results message)
class ChildAgent:
    def __init__(self, name, retriever):
        self.name = name
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True
        )
    def ask(self, query):
        response = self.chain.invoke({"query": query})
        result = response.get("result", "")
        return result

class CoordinatorAgent:
    def __init__(self, child_agents):
        self.children = child_agents

    def coordinate(self, query):
        # Return only the first child that gives a non-"not found" answer
        for child in self.children:
            try:
                resp = child.ask(query)
                if resp and 'not found' not in resp.lower():
                    return resp
            except:
                continue
        return None

class OversightAgent:
    def validate(self, answer):
        return answer  # pass through

class LearningAgent:
    def __init__(self):
        self.logs = []
    def log(self, user_query, response):
        self.logs.append({'query': user_query, 'response': response})

class AgentManager:
    def __init__(self, agents):
        self.coordinator = CoordinatorAgent(agents)
        self.oversight = OversightAgent()
        self.learning = LearningAgent()

    def handle_query(self, query):
        raw = self.coordinator.coordinate(query)
        if not raw:
            answer = "Oops! No relevant information found."
        else:
            answer = raw
        self.learning.log(query, answer)
        return answer

# Data loading & vectorstore
def load_json_data(file_paths):
    dfs = {}
    for name, path in file_paths.items():
        if not path:
            continue
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if isinstance(data, list):
                records = []
                for item in data:
                    content = json.dumps(item, sort_keys=True)
                    records.append({'content': content})
                dfs[name] = pd.DataFrame(records)
            elif isinstance(data, dict):
                content = json.dumps({**{'source': name}, **data}, sort_keys=True)
                dfs[name] = pd.DataFrame([{'content': content}])
        except Exception as e:
            st.error(f"Error loading JSON '{name}': {e}")
    return dfs

def build_vectorstores(dfs):
    child_agents = []
    for key, df in dfs.items():
        docs = [Document(page_content=row['content'], metadata={'source': key}) for _, row in df.iterrows()]
        if not docs:
            continue
        store = FAISS.from_documents(docs, cached_embeddings)
        retriever = store.as_retriever(search_kwargs={"k": 5})
        child_agents.append(ChildAgent(name=key, retriever=retriever))
    return child_agents

# Streamlit UI
st.set_page_config(page_title="AI Chatbot", layout="wide")
st.title("AI Chatbot")

# Load data & agents once
with st.spinner("Initializing..."):
    dfs = load_json_data(embedded_files)
    agents = build_vectorstores(dfs)
    manager = AgentManager(agents)

# Initialize session state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = [("bot", "Hi! I'm your AI Chatbot.")]

# Display chat history
for sender, msg in st.session_state.chat_history:
    if sender == "user":
        st.chat_message("user").markdown(f"**You:** {msg}")
    else:
        st.chat_message("assistant").markdown(f"**Bot:** {msg}")
        if sender == "bot":
            if st.button("üîä Read Aloud", key=f"read_{uuid.uuid4()}"):
                st.audio(tts_stream(msg), format="audio/mp3")

# Sample-prompt tiles (7 empty tiles for you to populate)
st.markdown("#### Sample Prompts")
cols = st.columns(7)
# A list-of-lists where you'll fill in your 7 prompt groups
sample_prompt_lists = [[] for _ in range(7)]
for i, col in enumerate(cols):
    with col:
        if st.button(f"Prompt {i+1}", key=f"tile_{i}"):
            selected = st.selectbox(
                "Choose a question‚Ä¶", 
                sample_prompt_lists[i], 
                key=f"select_{i}"
            )
            if selected:
                st.session_state.chat_history.append(("user", selected))
                response = manager.handle_query(selected)
                st.session_state.chat_history.append(("bot", response))
                # clear the widget state
                st.experimental_rerun()

# Main input form
with st.form(key="query_form", clear_on_submit=True):
    text = st.text_input("Your question:", key="input_query")
    mic, submit = st.columns([0.85, 0.15])
    with mic:
        st.write("")  # spacer
    with submit:
        submit_pressed = st.form_submit_button("Send")
    if submit_pressed:
        if not text.strip():
            st.warning("Please enter a question or use üéôÔ∏è to speak.")
        else:
            st.session_state.chat_history.append(("user", text))
            answer = manager.handle_query(text)
            st.session_state.chat_history.append(("bot", answer))
            st.experimental_rerun()

# Microphone outside the form
if st.button("üéôÔ∏è Speak"):
    voice_q = recognize_google_stt()
    if voice_q:
        st.session_state.chat_history.append(("user", voice_q))
        answer = manager.handle_query(voice_q)
        st.session_state.chat_history.append(("bot", answer))
        st.experimental_rerun()
