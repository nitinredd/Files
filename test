# batch_similarity_analyzer_market_autoscan_with_profiles.py
import os
import math
import streamlit as st
import pandas as pd
import numpy as np
from zipfile import ZipFile
from scipy.stats import skew, kurtosis, norm, probplot
from scipy import stats
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ----------------------------------------
# Required Helper Functions (unchanged / improved)
# ----------------------------------------

def prepare_data(reference_df, test_df):
    """Remove time zero if present and reset index"""
    if reference_df.shape[0] > 0 and (reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0'):
        reference_df = reference_df.iloc[1:].reset_index(drop=True)
    if test_df.shape[0] > 0 and (test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0'):
        test_df = test_df.iloc[1:].reset_index(drop=True)
    return reference_df, test_df

def conventional_f2(ref_means, test_means):
    """Calculate conventional f2 from 1-D arrays (means per timepoint)."""
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    p = len(ref_means)
    if p == 0:
        return None
    return 100 - 25 * np.log10(1 + (1/p) * sum_sq_diff)

def expected_f2(ref_df, test_df):
    """Calculate expected f2 from full dataframes (first col time, rest units)"""
    # Means per timepoint
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    
    # Conventional f2 component
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    
    # Variance components
    ref_var = row_variance(ref_df)
    test_var = row_variance(test_df)
    sum_var = (ref_var + test_var).sum()
    
    n = ref_df.shape[1] - 1  # Number of units per time point
    p = len(ref_means)
    
    if n == 0 or p == 0:
        return None
    
    adjustment = (1/n) * sum_var
    return 100 - 25 * np.log10(1 + (1/p) * (sum_sq_diff + adjustment))

def bias_corrected_f2(ref_df, test_df):
    """Calculate bias-corrected f2"""
    try:
        ref_means = ref_df.iloc[:, 1:].mean(axis=1)
        test_means = test_df.iloc[:, 1:].mean(axis=1)

        diff = test_means - ref_means
        sum_sq_diff = (diff ** 2).sum()

        ref_var = row_variance(ref_df)
        test_var = row_variance(test_df)
        sum_var = (ref_var + test_var).sum()

        n = ref_df.shape[1] - 1
        p = len(ref_means)

        if n == 0 or p == 0:
            return None

        adjustment = (1 / n) * sum_var
        right_side = sum_sq_diff + p

        if adjustment < right_side:
            adjusted_diff = sum_sq_diff - adjustment
            if adjusted_diff > 0:
                return 100 - 25 * np.log10(1 + (1 / p) * adjusted_diff)
            else:
                return None
        else:
            return None

    except Exception:
        return None

def row_variance(df):
    """Calculate row-wise variance across units (columns 1:)."""
    if df.shape[1] <= 1:
        return pd.Series([0]*df.shape[0])
    return df.iloc[:, 1:].var(axis=1, ddof=1)

def bootstrap_f2(ref_df, test_df, calc_func, n_iterations=10000):
    """Bootstrap f2 calculation (returns mean, median, skewness, and kurtosis)."""
    n_ref_units = max(0, ref_df.shape[1] - 1)
    n_test_units = max(0, test_df.shape[1] - 1)

    original_f2 = calc_func(ref_df, test_df)
    f2_values = []
    
    if n_ref_units == 0 or n_test_units == 0:
        return original_f2, None, None, None, None, None, None, []
    
    for _ in range(n_iterations):
        ref_sample_idx = np.random.choice(range(1, ref_df.shape[1]), n_ref_units, replace=True)
        test_sample_idx = np.random.choice(range(1, test_df.shape[1]), n_test_units, replace=True)

        ref_sample = ref_df.iloc[:, [0] + list(ref_sample_idx)].reset_index(drop=True)
        test_sample = test_df.iloc[:, [0] + list(test_sample_idx)].reset_index(drop=True)

        f2_val = calc_func(ref_sample, test_sample)

        if f2_val is not None and isinstance(f2_val, (int, float, np.floating, np.integer)):
            f2_values.append(float(f2_val))
    
    if not f2_values:
        return original_f2, None, None, None, None, None, None, []
    
    f2_values = np.array(f2_values)
    mean_f2 = np.mean(f2_values)
    median_f2 = np.median(f2_values)
    skewness_f2 = skew(f2_values)
    kurtosis_f2 = kurtosis(f2_values)
    lower_bound = np.percentile(f2_values, 5)
    upper_bound = np.percentile(f2_values, 95)

    return original_f2, lower_bound, upper_bound, mean_f2, median_f2, skewness_f2, kurtosis_f2, f2_values

def simulate_lilliefors_critical_value(n, alpha=0.05, num_simulations=1000):
    """Simulate Lilliefors critical value for confidence bands"""
    D_values = []
    for _ in range(num_simulations):
        sample = np.random.normal(loc=0, scale=1, size=n)
        sample_mean = np.mean(sample)
        sample_std = np.std(sample, ddof=1)
        z_scores = (sample - sample_mean) / sample_std
        z_scores.sort()
        ecdf_sim = np.arange(1, n + 1) / (n + 1)
        cdf = norm.cdf(z_scores)
        D = np.max(np.abs(ecdf_sim - cdf))
        D_values.append(D)
    return np.percentile(D_values, 100 * (1 - alpha))

def create_jmp_style_qq_plot(data, title, method_name, file_name):
    """Create JMP-like QQ plot with Lilliefors style bands."""
    if len(data) == 0:
        return None
    
    n = len(data)
    
    # Sort data and calculate plotting positions
    sorted_data = np.sort(data)
    plotting_positions = norm.ppf((np.arange(1, n + 1)) / (n + 2))
    
    # Fit line for normal distribution
    slope, intercept, r, _, _ = stats.linregress(plotting_positions, sorted_data)
    fit_line = intercept + slope * plotting_positions
    
    # Calculate empirical CDF
    ecdf = (np.arange(1, n + 1)) / (n + 1)
    
    # Get Lilliefors critical value for confidence bands
    D_critical = simulate_lilliefors_critical_value(n=n, alpha=0.05)
    
    # Calculate confidence bands using ECDF approach
    lower_quantile = norm.ppf(np.clip(ecdf - D_critical, 1e-10, 1 - 1e-10))
    upper_quantile = norm.ppf(np.clip(ecdf + D_critical, 1e-10, 1 - 1e-10))
    
    # Transform quantile bands to data space using the fitted line
    lower_band = intercept + slope * lower_quantile
    upper_band = intercept + slope * upper_quantile
    
    # Create the plot
    fig = go.Figure()
    
    # Add curved confidence bands first (so they appear behind)
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=upper_band,
        mode='lines',
        name='Upper Confidence Band',
        line=dict(color='red', width=1, dash='dot'),
        showlegend=False,
        hoverinfo='skip'
    ))
    
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=lower_band,
        mode='lines',
        name='Lower Confidence Band', 
        line=dict(color='red', width=1, dash='dot'),
        showlegend=False,
        hoverinfo='skip'
    ))
    
    # Add the main reference line (solid red)
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=fit_line,
        mode='lines',
        name='Normal Reference Line',
        line=dict(color='red', width=2, dash='solid'),
        showlegend=False,
        hovertemplate='Normal Reference Line<extra></extra>'
    ))
    
    # Add the data points (black circles)
    fig.add_trace(go.Scatter(
        x=plotting_positions,
        y=sorted_data,
        mode='markers',
        name='Data Points',
        marker=dict(
            color='black',
            size=6,
            symbol='circle',
            line=dict(width=0)
        ),
        showlegend=False,
        hovertemplate=
        '<b>Normal Quantile:</b> %{x:.3f}<br>' +
        '<b>Sample Quantile:</b> %{y:.3f}<br>' +
        '<extra></extra>'
    ))
    
    # Calculate nice axis ranges
    x_range = max(plotting_positions) - min(plotting_positions)
    y_range = max(sorted_data) - min(sorted_data)
    x_margin = x_range * 0.1
    y_margin = y_range * 0.1
    
    # Set up axis ticks similar to JMP
    x_ticks = [-2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]
    x_ticks = [tick for tick in x_ticks if min(plotting_positions)-x_margin <= tick <= max(plotting_positions)+x_margin]
    
    # Calculate statistics for annotation
    mean_val = np.mean(data)
    std_val = np.std(data)
    skewness_val = skew(data)
    kurtosis_val = kurtosis(data)
    
    
    fig.update_layout(
        title=dict(
            text=f'<b>Normal Quantile Plot</b><br><span style="font-size: 12px;">{method_name} - {file_name.replace(".xlsx", "")}</span>',
            x=0.5,
            y=0.95,
            font=dict(size=14, color='black', family='Arial')
        ),
        xaxis=dict(
            title=dict(
                text='<b>Normal Quantile</b>',
                font=dict(size=12, color='black', family='Arial')
            ),
            range=[min(plotting_positions)-x_margin, max(plotting_positions)+x_margin],
            tickvals=x_ticks,
            tickmode='array',
            gridcolor='lightgray',
            gridwidth=0.5,
            showgrid=True,
            zeroline=True,
            zerolinecolor='lightgray',
            zerolinewidth=0.8,
            tickfont=dict(size=10, color='black', family='Arial'),
            linecolor='black',
            linewidth=1,
            mirror=True
        ),
        yaxis=dict(
            title=dict(
                text='<b>Sample Quantile</b>',
                font=dict(size=12, color='black', family='Arial')
            ),
            range=[min(sorted_data)-y_margin, max(sorted_data)+y_margin],
            gridcolor='lightgray',
            gridwidth=0.5,
            showgrid=True,
            zeroline=True,
            zerolinecolor='lightgray',
            zerolinewidth=0.8,
            tickfont=dict(size=10, color='black', family='Arial'),
            linecolor='black',
            linewidth=1,
            mirror=True
        ),
        plot_bgcolor='white',
        paper_bgcolor='white',
        font=dict(family="Arial, sans-serif"),
        hovermode='closest',
        width=600,
        height=500,
        margin=dict(l=80, r=100, t=80, b=60),
        showlegend=False
    )
    
    
    stats_text = (
        f'<b>Statistics:</b><br>'
        f'Mean: {mean_val:.4f}<br>'
        f'Std Dev: {std_val:.4f}<br>'
        f'Skewness: {skewness_val:.4f}<br>'
        f'Kurtosis: {kurtosis_val:.4f}<br>'
        f'R¬≤: {r**2:.4f}<br>'
        f'N: {n:,}'
    )
    
    fig.add_annotation(
        x=0.98,
        y=0.98,
        xref="paper",
        yref="paper",
        text=stats_text,
        showarrow=False,
        align="left",
        bgcolor="white",
        bordercolor="black",
        borderwidth=1,
        borderpad=6,
        font=dict(size=9, color='black', family='Arial')
    )
    
    return fig

def create_combined_qq_plots(bootstrap_results, file_name):
    """Create a combined view of all QQ plots for a file"""
    methods_with_data = [(method, data) for method, data in bootstrap_results.items() if data and len(data) > 0]
    
    if not methods_with_data:
        return None
    
    n_methods = len(methods_with_data)
    cols = min(3, n_methods)
    rows = (n_methods + cols - 1) // cols
    
    subplot_titles = [method for method, _ in methods_with_data]
    
    fig = make_subplots(
        rows=rows,
        cols=cols,
        subplot_titles=subplot_titles,
        vertical_spacing=0.12,
        horizontal_spacing=0.1
    )
    
    colors = ['#3498DB', '#E74C3C', '#2ECC71', '#F39C12', '#9B59B6', '#1ABC9C']
    
    for idx, (method, data) in enumerate(methods_with_data):
        row = idx // cols + 1
        col = idx % cols + 1
        color = colors[idx % len(colors)]
        
        # Calculate QQ plot data
        (osm, osr), (slope, intercept, r) = probplot(np.array(data), dist="norm", plot=None)
        line_x = np.linspace(osm.min(), osm.max(), 100)
        line_y = slope * line_x + intercept
        
        # Add reference line
        fig.add_trace(
            go.Scatter(
                x=line_x,
                y=line_y,
                mode='lines',
                line=dict(color='#E74C3C', width=2),
                showlegend=False,
                hoverinfo='skip'
            ),
            row=row, col=col
        )
        
        # Add data points
        fig.add_trace(
            go.Scatter(
                x=osm,
                y=osr,
                mode='markers',
                marker=dict(
                    color=color,
                    size=6,
                    opacity=0.7,
                    line=dict(width=1, color=color)
                ),
                showlegend=False,
                hovertemplate=f'<b>{method}</b><br>Theoretical: %{{x:.3f}}<br>Sample: %{{y:.3f}}<extra></extra>'
            ),
            row=row, col=col
        )
    
    fig.update_layout(
        title=dict(
            text=f'<b>Normal Quantile Plots Comparison</b><br><sub>{file_name}</sub>',
            x=0.5,
            font=dict(size=16, color='#2C3E50')
        ),
        plot_bgcolor='white',
        paper_bgcolor='white',
        font=dict(family="Arial, sans-serif"),
        height=300 * rows,
        margin=dict(l=60, r=40, t=80, b=40)
    )
    
    fig.update_xaxes(
        title_text="Normal Quantile",
        gridcolor='#ECF0F1',
        showgrid=True,
        tickfont=dict(size=9)
    )
    
    fig.update_yaxes(
        title_text="f2 Value",
        gridcolor='#ECF0F1',
        showgrid=True,
        tickfont=dict(size=9)
    )
    
    return fig

# ----------------------------------------
# Market-specific CV & trimming rules (unchanged)
# ----------------------------------------

def compute_cv_per_timepoint(df):
    """Compute CV (%) per timepoint for a dataframe (columns: time, unit1, unit2, ...)."""
    if df.shape[1] <= 1:
        return pd.Series([np.nan]*df.shape[0])
    means = df.iloc[:, 1:].mean(axis=1)
    stds = df.iloc[:, 1:].std(axis=1, ddof=1)
    # avoid division by zero
    with np.errstate(divide='ignore', invalid='ignore'):
        cvs = (stds / means) * 100.0
    cvs = cvs.replace([np.inf, -np.inf], np.nan)
    return cvs.fillna(999.0)  # very large CV if mean is 0 (signals failure)

def check_cv_criteria(ref_df, test_df, market):
    """
    Return True if CV criteria PASS for given market.
    """
    m = (market or "").strip().upper()
    if ref_df.shape[0] == 0:
        return False
    times = pd.to_numeric(ref_df.iloc[:, 0], errors='coerce')
    if times.isnull().all():
        return False

    ref_cvs = compute_cv_per_timepoint(ref_df)
    test_cvs = compute_cv_per_timepoint(test_df)

    pass_criteria = True

    if m in ['EMA', 'CHINA', 'ASEAN', 'CANADA', 'WHO']:
        early_mask = times <= 10
        for i in range(len(times)):
            if early_mask.iloc[i]:
                if ref_cvs.iloc[i] > 20 or test_cvs.iloc[i] > 20:
                    pass_criteria = False
                    break
            else:
                if ref_cvs.iloc[i] > 10 or test_cvs.iloc[i] > 10:
                    pass_criteria = False
                    break

    elif m in ['USFDA', 'FDA']:
        early_mask = times <= 15
        for i in range(len(times)):
            if early_mask.iloc[i]:
                if ref_cvs.iloc[i] > 20 or test_cvs.iloc[i] > 20:
                    pass_criteria = False
                    break
            else:
                if ref_cvs.iloc[i] > 10 or test_cvs.iloc[i] > 10:
                    pass_criteria = False
                    break

    elif m == 'ANVISA':
        p = len(times)
        num_early = max(1, int(math.ceil(0.4 * p)))
        for i in range(len(times)):
            if i < num_early:
                if ref_cvs.iloc[i] > 20 or test_cvs.iloc[i] > 20:
                    pass_criteria = False
                    break
            else:
                if ref_cvs.iloc[i] > 10 or test_cvs.iloc[i] > 10:
                    pass_criteria = False
                    break
    else:
        pass_criteria = False

    return pass_criteria

def trim_by_85_rule(ref_df, test_df, market):
    """
    Trim timepoints according to market rules for >85% truncation.
    """
    times = pd.to_numeric(ref_df.iloc[:, 0], errors='coerce')
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)

    both_mask = (ref_means >= 85) & (test_means >= 85)
    either_mask = (ref_means >= 85) | (test_means >= 85)

    idx_both = np.where(both_mask)[0]
    idx_either = np.where(either_mask)[0]

    trim_idx = None

    m = (market or "").strip().upper()

    if m in ['USFDA', 'FDA']:
        idx_time15 = np.where(times <= 15)[0]
        last_idx_time15 = idx_time15[-1] if len(idx_time15) > 0 else None
        if len(idx_both) > 0:
            first_both = int(idx_both[0])
            if last_idx_time15 is not None:
                trim_idx = min(first_both, last_idx_time15)
            else:
                trim_idx = first_both
        else:
            trim_idx = last_idx_time15

    elif m == 'ANVISA':
        if len(idx_both) > 0:
            trim_idx = int(idx_both[0])
        else:
            trim_idx = None

    elif m in ['EMA', 'CHINA', 'ASEAN']:
        if len(idx_either) > 0:
            trim_idx = int(idx_either[0])
        else:
            trim_idx = None
    else:
        trim_idx = None

    if trim_idx is None:
        return ref_df.copy().reset_index(drop=True), test_df.copy().reset_index(drop=True)
    else:
        ref_trim = ref_df.iloc[: trim_idx + 1].reset_index(drop=True)
        test_trim = test_df.iloc[: trim_idx + 1].reset_index(drop=True)
        return ref_trim, test_trim

# Wrappers that trim based on market then call the actual calculation

def calc_conventional_f2_for_market(ref_df, test_df, market):
    r_trim, t_trim = trim_by_85_rule(ref_df, test_df, market)
    if r_trim.shape[0] == 0:
        return None
    ref_means = r_trim.iloc[:, 1:].mean(axis=1)
    test_means = t_trim.iloc[:, 1:].mean(axis=1)
    return conventional_f2(ref_means, test_means)

def calc_expected_f2_for_market(ref_df, test_df, market):
    r_trim, t_trim = trim_by_85_rule(ref_df, test_df, market)
    return expected_f2(r_trim, t_trim)

def calc_bias_corrected_f2_for_market(ref_df, test_df, market):
    r_trim, t_trim = trim_by_85_rule(ref_df, test_df, market)
    return bias_corrected_f2(r_trim, t_trim)

# ----------------------------------------
# Helper: Profile plot (mean ¬± SD, annotate bootstrap CI)
# ----------------------------------------

def create_profile_plot(file_name, ref_trim, test_trim, bootstrap_summary):
    """
    Create a plot showing mean dissolution profiles and ¬±SD bands for ref and test.
    bootstrap_summary: dict of method -> {'lower':..., 'upper':..., 'mean':..., 'median':...}
    """
    # handle empty
    if ref_trim is None or test_trim is None or ref_trim.shape[0] == 0:
        return None

    times = pd.to_numeric(ref_trim.iloc[:, 0], errors='coerce')
    ref_means = ref_trim.iloc[:, 1:].mean(axis=1)
    ref_sd = ref_trim.iloc[:, 1:].std(axis=1, ddof=1).fillna(0)
    test_means = test_trim.iloc[:, 1:].mean(axis=1)
    test_sd = test_trim.iloc[:, 1:].std(axis=1, ddof=1).fillna(0)

    fig = go.Figure()

    # Reference band
    fig.add_trace(go.Scatter(
        x=np.concatenate([times, times[::-1]]),
        y=np.concatenate([ref_means + ref_sd, (ref_means - ref_sd)[::-1]]),
        fill='toself',
        fillcolor='rgba(52,152,219,0.2)',
        line=dict(color='rgba(255,255,255,0)'),
        hoverinfo='skip',
        showlegend=False,
        name='Ref ¬± SD'
    ))
    # Reference mean line
    fig.add_trace(go.Scatter(
        x=times,
        y=ref_means,
        mode='lines+markers',
        name='Reference Mean',
        line=dict(color='#3498DB'),
        marker=dict(size=6)
    ))

    # Test band
    fig.add_trace(go.Scatter(
        x=np.concatenate([times, times[::-1]]),
        y=np.concatenate([test_means + test_sd, (test_means - test_sd)[::-1]]),
        fill='toself',
        fillcolor='rgba(231,76,60,0.18)',
        line=dict(color='rgba(255,255,255,0)'),
        hoverinfo='skip',
        showlegend=False,
        name='Test ¬± SD'
    ))
    # Test mean line
    fig.add_trace(go.Scatter(
        x=times,
        y=test_means,
        mode='lines+markers',
        name='Test Mean',
        line=dict(color='#E74C3C'),
        marker=dict(size=6)
    ))

    # Annotations with bootstrap summaries
    ann_texts = []
    for method, stats_dict in bootstrap_summary.items():
        if stats_dict:
            lower = stats_dict.get('lower')
            upper = stats_dict.get('upper')
            mean = stats_dict.get('mean')
            median = stats_dict.get('median')
            if lower is not None and upper is not None:
                ann_texts.append(f"{method}: CI(5-95)={lower:.2f}-{upper:.2f}, mean={mean:.2f}")
            elif mean is not None:
                ann_texts.append(f"{method}: mean={mean:.2f}")

    ann_full = "<br>".join(ann_texts) if ann_texts else "No bootstrap run"

    fig.update_layout(
        title=dict(text=f"<b>Dissolution Profile ‚Äî {file_name.replace('.xlsx','')}</b>", x=0.5),
        xaxis_title="Time",
        yaxis_title="Dissolution (%)",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        plot_bgcolor='white',
        hovermode='closest',
        margin=dict(l=60, r=30, t=80, b=60),
        width=800,
        height=450
    )

    fig.add_annotation(
        x=1.02,
        y=1,
        xref="paper",
        yref="paper",
        text=ann_full,
        showarrow=False,
        align="left",
        bordercolor="black",
        borderwidth=1,
        bgcolor="white",
        font=dict(size=10)
    )

    return fig

# ----------------------------------------
# Batch Processing Functions (updated to produce profile figs)
# ----------------------------------------

def load_batch_data(folder_path):
    """Load test and reference data from multiple Excel files in a folder."""
    workbook_data = {}
    
    for file_name in os.listdir(folder_path):
        if file_name.endswith(".xlsx") or file_name.endswith(".xls"):
            file_path = os.path.join(folder_path, file_name)
            try:
                reference_df = pd.read_excel(file_path, sheet_name=0)
                test_df = pd.read_excel(file_path, sheet_name=1)
                workbook_data[file_name] = (reference_df, test_df)
            except Exception as e:
                st.warning(f"Skipping file `{file_name}` due to error: {e}")
    
    return workbook_data

def choose_methods_for_file(market, cv_pass):
    """
    Mapping to recommended methods depending on market and whether CV criteria passed.
    """
    m = (market or "").strip().upper()
    if cv_pass:
        if m in ['EMA', 'CHINA', 'ASEAN', 'CANADA', 'WHO']:
            return ['Expected']
        elif m in ['USFDA', 'FDA', 'ANVISA']:
            return ['Conventional']
        else:
            return ['Conventional', 'Expected']
    else:
        if m in ['EMA', 'CHINA', 'ASEAN', 'CANADA', 'WHO']:
            return ['Expected Bootstrap']
        elif m in ['USFDA', 'FDA', 'ANVISA']:
            return ['Conventional Bootstrap']
        else:
            return ['Conventional Bootstrap', 'Expected Bootstrap']

def process_batch(workbook_data, selected_methods_extra, market, use_recommended=True, n_iterations=10000):
    """
    Process multiple workbooks and calculate f2 metrics.
    Returns DataFrame, bootstrap_data (for QQ), profile_figs (for profile HTMLs).
    """
    all_results = []
    bootstrap_data = {}  # Store bootstrap arrays for QQ plots
    profile_figs = {}    # Store profile plotly figures per file
    
    for file_name, (reference_df, test_df) in workbook_data.items():
        try:
            ref_clean, test_clean = prepare_data(reference_df.copy(), test_df.copy())
            results = {"File Name": file_name}
            file_bootstrap_data = {}
            bootstrap_summary = {}  # method -> summary dict for annotation

            # Determine recommended methods
            cv_pass = check_cv_criteria(ref_clean, test_clean, market)
            methods_to_run = []
            if use_recommended:
                recommended = choose_methods_for_file(market, cv_pass)
                methods_to_run.extend(recommended)
                # If user supplied extras, add them
                if selected_methods_extra:
                    methods_to_run.extend(selected_methods_extra)
                # unify
                methods_to_run = list(dict.fromkeys(methods_to_run))
                results['Market'] = market
                results['CV Criteria Pass'] = cv_pass
                results['Recommended Methods'] = ", ".join(recommended)
            else:
                # honor selected_methods_extra only
                methods_to_run = selected_methods_extra

            # Also compute r_trim/t_trim once for profile plot and for non-bootstrap calculators
            r_trim, t_trim = trim_by_85_rule(ref_clean, test_clean, market)

            # Compute profile plot later after collecting bootstrap summaries
            # Loop methods and compute values
            for method in methods_to_run:
                if method == "Conventional":
                    val = calc_conventional_f2_for_market(ref_clean, test_clean, market)
                    results["Conventional f2"] = val

                elif method == "Expected":
                    val = calc_expected_f2_for_market(ref_clean, test_clean, market)
                    results["Expected f2"] = val

                elif method == "Bias Corrected":
                    val = calc_bias_corrected_f2_for_market(ref_clean, test_clean, market)
                    results["Bias Corrected f2"] = val

                elif method == "Conventional Bootstrap":
                    def conv_func(r, t):
                        return calc_conventional_f2_for_market(r.reset_index(drop=True), t.reset_index(drop=True), market)
                    orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                        ref_clean, test_clean, conv_func, n_iterations)
                    results["Conventional Bootstrap f2"] = orig
                    results["Conventional Bootstrap CI"] = f"{lower:.2f} - {upper:.2f}" if lower is not None and upper is not None else None
                    results["Conventional Bootstrap Mean"] = mean
                    results["Conventional Bootstrap Median"] = median
                    results["Conventional Bootstrap Skewness"] = skewness
                    results["Conventional Bootstrap Kurtosis"] = kurt
                    file_bootstrap_data["Conventional Bootstrap"] = list(vals)
                    bootstrap_summary["Conventional Bootstrap"] = {'lower': lower, 'upper': upper, 'mean': mean, 'median': median}

                elif method == "Expected Bootstrap":
                    def exp_func(r, t):
                        return calc_expected_f2_for_market(r.reset_index(drop=True), t.reset_index(drop=True), market)
                    orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                        ref_clean, test_clean, exp_func, n_iterations)
                    results["Expected Bootstrap f2"] = orig
                    results["Expected Bootstrap CI"] = f"{lower:.2f} - {upper:.2f}" if lower is not None and upper is not None else None
                    results["Expected Bootstrap Mean"] = mean
                    results["Expected Bootstrap Median"] = median
                    results["Expected Bootstrap Skewness"] = skewness
                    results["Expected Bootstrap Kurtosis"] = kurt
                    file_bootstrap_data["Expected Bootstrap"] = list(vals)
                    bootstrap_summary["Expected Bootstrap"] = {'lower': lower, 'upper': upper, 'mean': mean, 'median': median}

                elif method == "Bias Corrected Bootstrap":
                    def bc_func(r, t):
                        return calc_bias_corrected_f2_for_market(r.reset_index(drop=True), t.reset_index(drop=True), market)
                    orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                        ref_clean, test_clean, bc_func, n_iterations)
                    results["Bias Corrected Bootstrap f2"] = orig
                    results["Bias Corrected Bootstrap CI"] = f"{lower:.2f} - {upper:.2f}" if lower is not None and upper is not None else None
                    results["Bias Corrected Bootstrap Mean"] = mean
                    results["Bias Corrected Bootstrap Median"] = median
                    results["Bias Corrected Bootstrap Skewness"] = skewness
                    results["Bias Corrected Bootstrap Kurtosis"] = kurt
                    file_bootstrap_data["Bias Corrected Bootstrap"] = list(vals)
                    bootstrap_summary["Bias Corrected Bootstrap"] = {'lower': lower, 'upper': upper, 'mean': mean, 'median': median}

                else:
                    # If the user selected a string equal to "Conventional Bootstrap" etc. with some variation,
                    # currently we only accept exact strings in the main options list.
                    pass

            # produce profile figure (mean ¬± SD) and annotate bootstrap summaries
            profile_fig = create_profile_plot(file_name, r_trim, t_trim, bootstrap_summary)
            profile_figs[file_name] = profile_fig

            all_results.append(results)
            bootstrap_data[file_name] = file_bootstrap_data

        except Exception as e:
            st.warning(f"Error processing file `{file_name}`: {e}")
    
    if all_results:
        df = pd.DataFrame(all_results)
    else:
        df = pd.DataFrame(columns=["File Name"])
    return df, bootstrap_data, profile_figs

# ----------------------------------------
# ZIP & plotting helpers (include profile plots)
# ----------------------------------------

def create_zip_report(report_df, qq_plots_data=None, profile_plots=None):
    """Create a ZIP file containing the report CSV and QQ/profile plots."""
    report_file = "f2_similarities_report.csv"
    zip_file = "f2_similarities_report.zip"
    
    report_df.to_csv(report_file, index=False)
    
    with ZipFile(zip_file, "w") as zipf:
        zipf.write(report_file)
        
        # Add QQ plots if available
        if qq_plots_data:
            for file_name, plots in qq_plots_data.items():
                for method, fig in plots.get('individual', {}).items():
                    if fig:
                        plot_filename = f"QQ_Plot_{file_name.replace('.xlsx', '')}_{method.replace(' ', '_')}.html"
                        fig.write_html(plot_filename)
                        zipf.write(plot_filename)
                        os.remove(plot_filename)
                if plots.get('combined'):
                    combined_filename = f"QQ_Plot_Combined_{file_name.replace('.xlsx', '')}.html"
                    plots['combined'].write_html(combined_filename)
                    zipf.write(combined_filename)
                    os.remove(combined_filename)
        
        # Add profile plots
        if profile_plots:
            for file_name, fig in profile_plots.items():
                if fig:
                    profile_filename = f"Profile_{file_name.replace('.xlsx', '')}.html"
                    fig.write_html(profile_filename)
                    zipf.write(profile_filename)
                    os.remove(profile_filename)
    
    if os.path.exists(report_file):
        os.remove(report_file)
    
    return zip_file

def generate_qq_plots(bootstrap_data):
    """Generate all QQ plots and return them as a dictionary."""
    qq_plots_data = {}
    
    for file_name, file_bootstrap_data in bootstrap_data.items():
        plots = {
            'individual': {},
            'combined': None
        }
        
        for method, data in file_bootstrap_data.items():
            if data and len(data) > 0:
                fig = create_jmp_style_qq_plot(np.array(data), f"QQ Plot", method, file_name)
                plots['individual'][method] = fig
        
        combined_fig = create_combined_qq_plots(file_bootstrap_data, file_name)
        plots['combined'] = combined_fig
        
        qq_plots_data[file_name] = plots
    
    return qq_plots_data

# ----------------------------------------
# Streamlit: autoscan behaviour using session_state
# ----------------------------------------

def scan_folder_callback():
    """
    Callback when the user inputs folder path: load files and compute recommendations.
    Stores results into st.session_state for UI display.
    """
    path = st.session_state.get("folder_path_input", "")
    session = st.session_state
    session['scan_error'] = None
    session['workbook_data'] = {}
    session['recommendations'] = {}

    if not path:
        session['scan_error'] = "No path provided."
        return
    if not os.path.isdir(path):
        session['scan_error'] = "Invalid folder path."
        return

    workbook_data = load_batch_data(path)
    if not workbook_data:
        session['scan_error'] = "No valid Excel files found in folder."
        session['workbook_data'] = {}
        session['recommendations'] = {}
        return

    session['workbook_data'] = workbook_data
    # compute recommendations (CV pass/fail and recommended methods) for display
    rec_map = {}
    for file_name, (ref_df, test_df) in workbook_data.items():
        ref_clean, test_clean = prepare_data(ref_df.copy(), test_df.copy())
        cv_pass = check_cv_criteria(ref_clean, test_clean, session.get('selected_market', 'EMA'))
        recommended = choose_methods_for_file(session.get('selected_market', 'EMA'), cv_pass)
        rec_map[file_name] = {"cv_pass": cv_pass, "recommended": recommended}
    session['recommendations'] = rec_map

# ----------------------------------------
# Streamlit App Code
# ----------------------------------------

def main():
    st.set_page_config(page_title="Batch Similarity Analyzer (Auto-scan & Profiles)", layout="wide")
    st.title("Batch Similarity Analyzer ‚Äî Auto-scan + Profiles")
    st.markdown("""
    Paste a folder path containing Excel files (sheet0=reference, sheet1=test).
    The app will auto-scan and **recommend methods per file** based on market CV rules.
    You can then add extra methods from the dropdown and click *Calculate and Generate Report*.
    """)

    # Market selection first so scan callback can use it
    market_options = ['EMA', 'CHINA', 'ASEAN', 'USFDA', 'ANVISA', 'WHO', 'CANADA', 'FDA']
    selected_market = st.selectbox("Select target market (affects CV rules & recommendations):", market_options, index=0)
    st.session_state['selected_market'] = selected_market

    folder_path = st.text_input("Enter path to folder containing Excel files:", key="folder_path_input", on_change=scan_folder_callback)
    st.caption("The app will scan the folder and show recommendations as soon as the path is provided.")

    # Show scan results (if any)
    if st.session_state.get('scan_error'):
        st.error(st.session_state['scan_error'])
    elif st.session_state.get('workbook_data'):
        st.success(f"Found {len(st.session_state['workbook_data'])} Excel files. Recommendations ready.")
        rec_map = st.session_state.get('recommendations', {})
        st.subheader("Recommended methods per file (based on CV rules)")
        for fn, info in rec_map.items():
            cv_text = "PASS" if info['cv_pass'] else "FAIL"
            st.write(f"**{fn}** ‚Äî CV: {cv_text} ‚Äî Recommended: {', '.join(info['recommended'])}")

    # global extra methods user wants to add (applied to all files)
    options = ["Conventional", "Expected", "Bias Corrected",
               "Conventional Bootstrap", "Expected Bootstrap", "Bias Corrected Bootstrap"]
    additional_methods = st.multiselect("Add extra methods to run for all files (optional):", options)

    n_iterations = st.slider("Number of bootstrap iterations:", 1000, 50000, 10000, 1000)

    # Calculate & generate report button
    if st.button("Calculate and Generate Report"):
        if not st.session_state.get('workbook_data'):
            st.error("No workbook data loaded. Please provide a valid folder path first.")
        else:
            with st.spinner("Processing files..."):
                workbook_data = st.session_state['workbook_data']
                report_df, bootstrap_data, profile_figs = process_batch(
                    workbook_data, additional_methods, selected_market, use_recommended=True, n_iterations=n_iterations
                )

                st.subheader("üìä Results Preview")
                st.dataframe(report_df.head(50))

                # Generate QQ Plots for bootstrap methods
                qq_plots_data = None
                any_bootstrap = any(len(v) > 0 for file_v in bootstrap_data.values() for v in file_v.values())
                if any_bootstrap:
                    with st.spinner("Generating QQ plots..."):
                        qq_plots_data = generate_qq_plots(bootstrap_data)
                        st.success("‚úÖ QQ plots generated.")
                        # Show small summary
                        for file_name, plots in qq_plots_data.items():
                            indiv = len([p for p in plots['individual'].values() if p is not None]) if plots['individual'] else 0
                            combined = 1 if plots['combined'] is not None else 0
                            st.write(f"**{file_name}:** {indiv} QQ plots + {combined} combined")

                # Show profile plots inline (one tab per file)
                st.subheader("üìà Dissolution Profiles (Mean ¬± SD)")
                for file_name, fig in profile_figs.items():
                    if fig:
                        st.markdown(f"**{file_name}**")
                        st.plotly_chart(fig, use_container_width=True)

                # Create ZIP containing CSV + QQ + profile plots
                with st.spinner("Creating downloadable ZIP file..."):
                    zip_file_path = create_zip_report(report_df, qq_plots_data, profile_figs)

                with open(zip_file_path, "rb") as f:
                    file_content = f.read()
                if os.path.exists(zip_file_path):
                    os.remove(zip_file_path)

                st.download_button(
                    label="üì• Download Complete Report with Plots (ZIP)",
                    data=file_content,
                    file_name="f2_similarities_report_with_plots.zip",
                    mime="application/zip",
                    help="CSV report + QQ plot HTMLs + Profile plot HTMLs"
                )
                st.success("Report and plots are ready for download!")

    # Help/information
    with st.expander("‚ÑπÔ∏è Behaviour & rules implemented"):
        st.markdown("""
        - On pasting a folder path the app automatically scans the Excel files and recommends methods per file (based on the chosen market and CV rules).
        - You may add extra methods (global) via the dropdown before clicking **Calculate and Generate Report**.
        - Profile plots show mean ¬± SD bands for trimmed (by 85% rule) timepoints. Bootstrap f2 summaries (CI, mean) are annotated on the plot.
        - ZIP contains:
          - `f2_similarities_report.csv`
          - `QQ_Plot_[File]_[Method].html` (if bootstrap run)
          - `QQ_Plot_Combined_[File].html` (if applicable)
          - `Profile_[File].html`
        """)

if __name__ == "__main__":
    main()
