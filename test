import numpy as np
import matplotlib
from scipy.stats import norm
from sklearn.utils import resample
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import warnings
from matplotlib.ticker import MaxNLocator
warnings.filterwarnings('ignore')
import time
from tqdm import tqdm
#R_regulation
# import rpy2
# import rpy2.robjects as robjects
# from rpy2.robjects.packages import importr, data
# utils = importr('utils')
# base = importr('base')
# bootf2 = importr('bootf2')
# read = importr('readxl')
# open = importr('writexl')
#interpolation
import itertools
from itertools import combinations
from scipy.interpolate import interp1d
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel

file_path= r"C:\Users\p00095189\Desktop\WORK\Formulations\Similarity_Analyzer\SIMILARITY_ANALYZER\SIMILARITY_ANALYZER\Other_files\Input f2 files\File6.xlsx"
reference_df =pd.read_excel(file_path, sheet_name=0)
test_df=pd.read_excel(file_path, sheet_name=1)
arrayboot=[]

np.random.seed(306)
reference_mean = reference_df.iloc[:, 1:].mean(axis=1)
test_mean = test_df.iloc[:, 1:].mean(axis=1)
reference_mean_df = pd.DataFrame({
    reference_df.columns[0]: reference_df.iloc[:, 0],
    'Dissolution': reference_mean
})
test_mean_df = pd.DataFrame({
    test_df.columns[0]: test_df.iloc[:, 0],
    'Dissolution': test_mean
})


def dissolution_curve_interval(reference_df, test_df):
    # Print dataframes (optional for debugging)
    #print(reference_df)
    #print(test_df)

    # Calculate means, maxs, and mins for reference data
    ref_data_means = pd.DataFrame(reference_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Reference'])
    ref_data_max = pd.DataFrame(reference_df.iloc[:, 1:].max(axis=1), columns=['Max_Reference'])
    ref_data_min = pd.DataFrame(reference_df.iloc[:, 1:].min(axis=1), columns=['Min_Reference'])

    # Calculate means, maxs, and mins for test data
    test_data_means = pd.DataFrame(test_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Test'])
    test_data_max = pd.DataFrame(test_df.iloc[:, 1:].max(axis=1), columns=['Max_Test'])
    test_data_min = pd.DataFrame(test_df.iloc[:, 1:].min(axis=1), columns=['Min_Test'])

    # Add Time column
    ref_data_means.insert(0, 'Time', reference_df.iloc[:, 0])
    ref_data_max.insert(0, 'Time', reference_df.iloc[:, 0])
    ref_data_min.insert(0, 'Time', reference_df.iloc[:, 0])

    test_data_means.insert(0, 'Time', test_df.iloc[:, 0])
    test_data_max.insert(0, 'Time', test_df.iloc[:, 0])
    test_data_min.insert(0, 'Time', test_df.iloc[:, 0])

    # Create a figure and axis object
    plt.figure(figsize=(12, 6))

    # Plotting reference data with error bars
    plt.errorbar(ref_data_means['Time'], ref_data_means['Mean_Reference'],
                 yerr=[ref_data_means['Mean_Reference'] - ref_data_min['Min_Reference'],
                       ref_data_max['Max_Reference'] - ref_data_means['Mean_Reference']],
                 fmt='o', label='Reference Mean', color='blue', linestyle='-')

    # Plotting test data with error bars
    plt.errorbar(test_data_means['Time'], test_data_means['Mean_Test'],
                 yerr=[test_data_means['Mean_Test'] - test_data_min['Min_Test'],
                       test_data_max['Max_Test'] - test_data_means['Mean_Test']],
                 fmt='o', label='Test Mean', color='green', linestyle='--')

    # Adding horizontal segments for min and max values for reference data
    for time, min_val, max_val in zip(ref_data_means['Time'], ref_data_min['Min_Reference'], ref_data_max['Max_Reference']):
        plt.hlines(min_val, time - 0.2, time + 0.2, colors='blue', linestyles='--', alpha=0.5)
        plt.hlines(max_val, time - 0.2, time + 0.2, colors='blue', linestyles='--', alpha=0.5)

    # Adding horizontal segments for min and max values for test data
    for time, min_val, max_val in zip(test_data_means['Time'], test_data_min['Min_Test'], test_data_max['Max_Test']):
        plt.hlines(min_val, time - 0.2, time + 0.2, colors='green', linestyles='--', alpha=0.5)
        plt.hlines(max_val, time - 0.2, time + 0.2, colors='green', linestyles='--', alpha=0.5)

    # Add labels and title
    plt.xlabel('Time')
    plt.ylabel('Dissolution (%)')
    plt.title('Dissolution Curves with Intervals')
    plt.grid(True)
    plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
    plt.gca().set_yticks(range(0, 101, 5))
    plt.legend(loc='lower right')
    
    #Save the plot as an image file
    plt.savefig('dissolution_curves_with_intervals.png', format='png', dpi=300)

    #Optionally, display the plot (uncomment if you want to see it as well)
    plt.show()

    # Close the plot to free up memory
    plt.close()


def dissolution_curve(reference_df,test_df):
    ref_data_means=pd.DataFrame(reference_df.iloc[:,1:].mean(axis=1),columns=['Mean_Reference'])
    test_data_means=pd.DataFrame(test_df.iloc[:,1:].mean(axis=1),columns=['Mean_Test'])


    ref_data_means.insert(0, 'Time', reference_df.iloc[:, 0])
    test_data_means.insert(0, 'Time', test_df.iloc[:, 0])



    # Create a figure and axis object
    plt.figure(figsize=(12, 6))

    # Plotting reference data
    for column in ref_data_means.columns[1:]:  # Skip the first column which is 'Time'
        plt.plot(ref_data_means.iloc[:, 0], ref_data_means[column], label=f'Reference {column}',marker='o')

    # Plotting test data
    for column in test_data_means.columns[1:]:  # Skip the first column which is 'Time'
        plt.plot(test_data_means.iloc[:, 0], test_data_means[column], label=f'Test {column}', marker='o',linestyle='--')

    # Add labels and title
    plt.xlabel('Time')
    plt.ylabel('Dissolution (%)')
    plt.title('Dissolution Curves')
    plt.grid(True)
    plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))
    plt.gca().yaxis.set_major_locator(MaxNLocator(nbins='auto', integer=True))
    plt.gca().set_yticks(range(0, 101, 5))
    plt.legend(loc='lower right')
    #Save the plot as an image file
    plt.savefig('dissolution_curves.png', format='png', dpi=300)

    #Optionally, display the plot (uncomment if you want to see it as well)
    plt.show()

    # Close the plot to free up memory
    plt.close()

# Check 1: A minimum of three-time points (time zero excluded) is considered for both products
def check_time_points(df):

    if df.iloc[0, 0] == 0 or df.iloc[0, 0]=='0':  # Check if the first time point is zero
        return len(df.iloc[:,0].values)-1 >=3 # Exclude the zero time point 
    else:
        return len(df.iloc[:,0].values) >=3 
    
# Check 2: 1,2 time points > 85%? If > 85%, No f2 calculations
def two_time_points(df):
    mean_values = df.iloc[:, 1:].mean(axis=1)  # Exclude the Time Points column
    if mean_values[0]>85 or mean_values[1]>85:
         return False
    else:
         return True

#Check 4: If in Min check if in 15 min > 85% the no f2 required
def min15__check(df):
     time_string=df.columns[0]
     mean_values = df.iloc[:, 1:].mean(axis=1) 
     
     if "min" in time_string.lower() or "minutes" in time_string.lower():
          condition = df.iloc[:, 0] <= 15
          indices = df.index[condition]
          filtered_df = mean_values.loc[indices]
          condition_other_columns = filtered_df > 85
          final_indices = filtered_df.index[condition_other_columns].tolist()
          if len(final_indices)>0:
               return True
          else:
               return False
     else:
          return False
          
# The coefficient of variation (CV) of both product should be less than 20% at the first (non-zero) time point and less than 10% at the following time points
def check_cv(df):
    if df.iloc[0, 0] == 0:  # Check if the first time point is zero
        cv_values = df.iloc[1:, 1:].std(axis=1) / df.iloc[1:, 1:].mean(axis=1) * 100  # Exclude the zero time point and Time Points column
        return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()
    else:
        cv_values = df.iloc[:, 1:].std(axis=1) / df.iloc[:, 1:].mean(axis=1) * 100 # Calculate CV for all time points
        #print(df)
        #print(cv_values)
        return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()



def row_variance(df):
    return df.iloc[:,1:].var(axis=1,ddof=1)

def stand(p,sum_diff_df_sqr):
    f2_v1 = 100 - 25 * np.log10((1 + (1 / p) * sum_diff_df_sqr))  # correct F2_DRL
    return f2_v1

def expected(p,sum_diff_df_sqr,left_side):
    f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) + (left_side)))))
    return f2
    
def BiasCor(p,sum_diff_df_sqr,left_side):
    # f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
    # print("Bias corrected f2", f2)
    Right_side=sum_diff_df_sqr+p
    if left_side >= Right_side:
        f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
        return round(f2,3)
    else:
        f2="Baised Corrected f2 can not be calculated"
        return f2
    return f2



def f2s(ref_data,test_data):   
    ref_data_means=ref_data.iloc[:,1:].mean(axis=1)
    test_data_means=test_data.iloc[:,1:].mean(axis=1)
    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    diff_df=test_data_df-ref_data_df
    diff_df_sqr=diff_df**2
    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    ref_data_var_df=pd.DataFrame(row_variance(ref_data),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(test_data),columns=['Unbiased Variance'])
    addition_df=test_data_var_df+ref_data_var_df
    sum_addition_df=addition_df['Unbiased Variance'].sum()
    n_r,n_c=ref_data.shape
    n=n_c-1
    p=len(ref_data.iloc[:,0])
    left_side=(1/n)*(sum_addition_df)
    f2=stand(p,sum_diff_df_sqr)
    print("Conventional f2 :",round(f2,3))
    expf2=expected(p,sum_diff_df_sqr,left_side)
    print("Expected f2     :", round(expf2,3))
    BiCf2=BiasCor(p,sum_diff_df_sqr,left_side)
    print("BiasCorrected f2:", BiCf2)



def changed_data_either85_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)



def changed_data_both85_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    # print("mean ref",mean_values_reference)
    # print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    # print(ref_index)
    # print(test_index)

    if ref_index>test_index:
        final_index=ref_index
    elif test_index>ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    # print(final_index)
    # print(final_index+1)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)

def changed_data_either85_FDA_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)




def cal_f2(resampled_test,resampled_reference):
    #print(resampled_reference.iloc[:,1:])
    ref_data_means=resampled_reference.iloc[:,1:].mean(axis=1)
    test_data_means=resampled_test.iloc[:,1:].mean(axis=1)

    #print(ref_data_means)
    #print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    #print(ref_data_df)
    #print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)

    p=len(resampled_reference.iloc[:,0])
    #print('Time points',p)

    f2_v1 = 100 - 25 * np.log10((1 + (1 / p) * sum_diff_df_sqr))  # correct F2_DRL
    #print("f2 score is: ",f2_v1)
    arrayboot.append(f2_v1)
    return f2_v1

def bca(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=cal_f2(test_data,ref_data)
    n_iterations=12 # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_f2(dft,dfr)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for f2: [{lower_percentile}, {upper_percentile}]")

def bca_exp_f2(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=cal_exp_f2(test_data,ref_data)
    n_iterations=12  # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_exp_f2(dft,dfr)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for expected f2: [{lower_percentile}, {upper_percentile}]")

def plots(arrayboot,plot):
    plt.figure(figsize=(10, 6))
    sns.histplot(arrayboot, kde=True, bins=10, color='blue')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    tiltename=f"histogram_plot_{plot}"
    plt.title(tiltename)
    filename=f"histogram_plot_{plot}.png"
    plt.savefig(filename)  # Save the plot as a PNG file
    plt.close()  # Close the figure to avoid displaying it inline
    plt.figure(figsize=(8, 8))
    stats.probplot(arrayboot, dist="norm", plot=plt)
    tiltename=f"qq_plot_{plot}"
    plt.title(tiltename)
    filename=f"qq_plot_{plot}.png"
    plt.savefig(filename)  # Save the plot as a PNG file
    plt.close()  # Close the figure to avoid displaying it inline


def changed_data_either85_bca(reference_df,test_df):
    arrayboot.clear()
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index
    
    if final_index==0:
        final_index=change_reference_df.index[-1]
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("f2 Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    #print("Len1",len(arrayboot))
    plot="f2"
    plots(arrayboot,plot)

    print(" ")
    bca(change_test_df,change_reference_df)
    #print("Len2",len(arrayboot))
    plot="f2_bca"
    plots(arrayboot,plot)


    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    #print("Len3",len(arrayboot))
    plot="expected_f2"
    plots(arrayboot,plot)

    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    #print("Len4",len(arrayboot))
    plot="expected_f2_bca"
    plots(arrayboot,plot)

    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Baised Corrected f2 can not be calculated")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        #print("Len5",len(arrayboot))
        plot="baiscorrected_f2"
        plots(arrayboot,plot)

        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)
        #print("Len6",len(arrayboot))
        plot="baiscorrected_f2_bca"
        plots(arrayboot,plot)



def changed_data_both85_bca(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    # print(ref_index)
    # print(test_index)

    if ref_index>test_index:
        final_index=ref_index
    elif test_index>ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    print(" ")
    bca(change_test_df,change_reference_df)
    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Cannot caclulate f2")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)


def changed_data_either85_FDA_bca(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    print(" ")
    bca(change_test_df,change_reference_df)
    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Cannot caclulate f2")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)




def f2_bootstral_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=12 # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_f2(dft,dfr)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

# Check 3: The time points at which the dissolutions are measured are the same for both products
def check_same_time_points(df1, df2):
    return df1.iloc[:,0].equals(df2.iloc[:,0])

# Check 4: At least 12 individual dosage units are used for both products
def check_sample_units(df):
    return df.shape[1] - 1 >= 12  # Exclude the Time Points column


def cal_exp_f2(resampled_test,resampled_reference):
    ref_data_means=resampled_reference.iloc[:,1:].mean(axis=1)
    test_data_means=resampled_test.iloc[:,1:].mean(axis=1)

    #print(ref_data_means)
    #print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    #print(ref_data_df)
    #print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)

    ref_data_var_df=pd.DataFrame(row_variance(resampled_reference),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(resampled_test),columns=['Unbiased Variance'])

    #print(ref_data_var_df)
    #print(test_data_var_df)

    addition_df=test_data_var_df+ref_data_var_df
    #print(addition_df)

    sum_addition_df=addition_df['Unbiased Variance'].sum()
    #print(sum_addition_df)

    n_r,n_c=resampled_test.shape
    n=n_c-1
    #print(n)

    p=len(resampled_test.iloc[:,0])
    #print('Time points',p)

    left_side=(1/n)*(sum_addition_df)
    #print(left_side)

    f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) + (left_side)))))
    #print("Expected f2", f2)
    arrayboot.append(f2)
    return f2

def f2_bootstra_expf2_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=12 # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_exp_f2(dft,dfr)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

def biascorrectedf2(ref_data,test_data):
    ref_data_means=ref_data.iloc[:,1:].mean(axis=1)
    test_data_means=test_data.iloc[:,1:].mean(axis=1)

    # print(ref_data_means)
    # print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    # print(ref_data_df)
    # print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)


    ref_data_var_df=pd.DataFrame(row_variance(ref_data),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(test_data),columns=['Unbiased Variance'])

    # print(ref_data_var_df)
    # print(test_data_var_df)

    addition_df=test_data_var_df+ref_data_var_df
    # print(addition_df)

    sum_addition_df=addition_df['Unbiased Variance'].sum()
    # print(sum_addition_df)

    n_r,n_c=ref_data.shape
    n=n_c-1
    # print(n)

    p=len(ref_data.iloc[:,0])
    # print('Time points',p)

    left_side=(1/n)*(sum_addition_df)
    # print(left_side)

    # f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
    # print("Bias corrected f2", f2)
    Right_side=sum_diff_df_sqr+p

    if left_side >= Right_side:
        with warnings.catch_warnings():
            warnings.simplefilter("error", RuntimeWarning)
            try:
                f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
            except RuntimeWarning as e:
                print(f"Warning occurred: {e}")
                f2=0
                return f2
        #print("Baised Corrected f2", f2)
        return f2
    else:
        f2=0
        #print("No f2",f2)
        return f2

def f2_bootstratp_bias_f2_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=12 # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=biascorrectedf2(dfr,dft)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

def bca_bias_f2(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=f2_bootstratp_bias_f2_tile(ref_data,test_data)
    n_iterations=12 # n_iterations=10000
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=f2_bootstratp_bias_f2_tile(dfr,dft)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for expected f2: [{lower_percentile}, {upper_percentile}]")


def R_Regulation(file_path):
    dr=read.read_excel(file_path,sheet='Sheet1')
    dt=read.read_excel(file_path,sheet='Sheet2')
    print("Enter Name of Regulation either EMA, FDA, WHO, Canada, ANVISA:")
    reg=input()
    b=bootf2.bootf2(dt,dr,file_out='test',regulation=reg)
    print(b.rx2('boot.summary'))
    print("Please check file created for full report")

def R_all_Regulation(file_path):
    dr=read.read_excel(file_path,sheet='Sheet1')
    dt=read.read_excel(file_path,sheet='Sheet2')
    b=bootf2.bootf2(dt,dr,file_out='EMA_Results',regulation="EMA")
    print("EMA Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='FDA_Results',regulation="FDA")
    print("FDA Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='WHO_Results',regulation="WHO")
    print("WHO Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='Canada_Results',regulation="Canada")
    print("Canada Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='ANVISA_Results',regulation="ANVISA")
    print("ANVISA Summary:")
    print(b.rx2('boot.summary'))
    print("Please check files created for full report")
def interpolate_linear(df, new_times):
    """
    Simple linear interpolation (used for determining the candidate window).
    """
    df_sorted = df.sort_values(by=df.columns[0])
    known_times = df_sorted.iloc[:, 0].values.astype(float)
    known_values = df_sorted.iloc[:, 1].values
    f = interp1d(known_times, known_values, kind='linear', fill_value=(known_values[0], known_values[-1]))
    return f(new_times)

def determine_candidate_window(ref_df, test_df, step=1, initial_threshold=10):
    """
    Identify candidate window based on the actual time range in the data.
    """
    # Get the actual maximum time from the union of reference and test data
    max_ref_time = ref_df.iloc[:, 0].max()
    max_test_time = test_df.iloc[:, 0].max()
    fixed_max = max(max_ref_time, max_test_time)
    fixed_min = 0

    times = np.arange(fixed_min, fixed_max + 1, step)
    ref_vals = interpolate_linear(ref_df, times)
    test_vals = interpolate_linear(test_df, times)
    diff = np.abs(ref_vals - test_vals)
    
    valid_times = times[diff <= initial_threshold]
    if len(valid_times) == 0:
        print(f"No time points found within {initial_threshold}% difference; trying threshold=20.")
        valid_times = times[diff <= 20]
        if len(valid_times) == 0:
            print("No candidate window found even with 20% threshold. Using full range.")
            return fixed_min, fixed_max
        else:
            window_max = valid_times[-1]
            print(f"Candidate window determined (threshold 20): {fixed_min} to {window_max}")
            return fixed_min, window_max
    else:
        window_max = valid_times[-1]
        print(f"Candidate window determined (threshold {initial_threshold}): {fixed_min} to {window_max}")
        return fixed_min, window_max

# ======================== STAGE 2: Helper & Predictive Analysis Functions ========================
def generate_all_time_combinations(min_time, max_time, step=1):
    """
    Generate all possible time point sequences within the interval [min_time, max_time],
    ensuring that 0 is fixed at the start and that there are at least 3 points.
    """
    times = list(range(min_time, max_time + 1, step))
    if 0 not in times:
        times.insert(0, 0)
    all_combinations = []
    for r in range(2, len(times)):
        for combo in itertools.combinations(times[1:], r):
            seq = [0] + list(combo)
            all_combinations.append(sorted(seq))
    return list(all_combinations)

def interpolate_dissolution_curve(df, new_times, method='gpr'):
    """
    Predict dissolution values at new time points, clamping to observed time range.
    """
    df_sorted = df.sort_values(by=df.columns[0])
    known_times = df_sorted.iloc[:, 0].values.astype(float)
    known_values = df_sorted.iloc[:, 1].values
    
    # Clamp new_times to the observed range [min(known_times), max(known_times)]
    new_times_clamped = np.clip(new_times, known_times.min(), known_times.max())
    
    if method == 'gpr':
        # Define kernel: constant * RBF + white noise.
        kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=10.0, length_scale_bounds=(1e-2, 1e2)) + \
                 WhiteKernel(noise_level=1, noise_level_bounds=(1e-5, 1e1))
        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5)
        # CORRECTED: Use known_times and known_values instead of undefined X/y
        gp.fit(known_times.reshape(-1, 1), known_values)
        y_pred = gp.predict(new_times_clamped.reshape(-1, 1))
        return y_pred
    else:
        if len(known_times) < 2:
            return np.full(len(new_times_clamped), known_values[0]) if len(known_values) > 0 else np.zeros(len(new_times_clamped))
        if method == 'cubic' and len(known_times) >= 4:
            f = interp1d(known_times, known_values, kind='cubic', fill_value='extrapolate')
        else:
            f = interp1d(known_times, known_values, kind='linear', fill_value=(known_values[0], known_values[-1]))
        return f(new_times_clamped)
    

def check_regulatory_compliance(seq, regulation, ref_means, test_means):
    """
    Check if a candidate sequence satisfies the selected regulatory rules.
    Rules implemented:
      - FDA: Any ≥85% dissolution must appear only at the final time.
      - EMA: All time points must be below 85% and at least 4 points are required.
      - China: All reference values must be below 85%.
      - ASEAN: The first 3 time points for both reference and test must be below 85%.
      - ANVISA: The first time point must be 0.
    Returns (compliant, reasons).
    """
    compliant = True
    reasons = []
    
    if regulation == "FDA":
        over_85_ref = [t for t in seq if ref_means[t] >= 85]
        over_85_test = [t for t in seq if test_means[t] >= 85]
        if over_85_ref or over_85_test:
            last_t = seq[-1]
            if not (last_t in over_85_ref or last_t in over_85_test):
                compliant = False
                reasons.append("85%+ dissolution not at last time")
    elif regulation == "EMA":
        if any(ref_means[t] >= 85 or test_means[t] >= 85 for t in seq):
            compliant = False
            reasons.append("Dissolution ≥85% detected")
        if len(seq) < 4:
            compliant = False
            reasons.append("Insufficient time points (<4)")
    elif regulation == "China":
        if any(ref_means[t] >= 85 for t in seq):
            compliant = False
            reasons.append("Reference dissolution ≥85%")
    elif regulation == "ASEAN":
        first_three = seq[:3] if len(seq) >= 3 else seq
        if any(ref_means[t] >= 85 for t in first_three):
            compliant = False
            reasons.append("Reference ≥85% in first 3 points")
        if any(test_means[t] >= 85 for t in first_three):
            compliant = False
            reasons.append("Test ≥85% in first 3 points")
    elif regulation == "ANVISA":
        if seq[0] != 0:
            compliant = False
            reasons.append("Missing 0 as first time point")
    
    return (compliant, reasons)

def predictive_optimal_combinations_advanced(ref_df, test_df, regulation, 
                                            window_min, window_max, diff_threshold=None,
                                            interp_method='linear', num_samples=1000000):
    """Final robust version with proper array reshaping and error handling"""
    # ===== 1. Initialization & Validation =====
    results = []
    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=10.0) + WhiteKernel()
    
    # Convert to numpy arrays and handle NaNs
    ref_times = ref_df.iloc[:, 0].values.astype(float)
    ref_diss = ref_df.iloc[:, 1].values.astype(float)
    test_times = test_df.iloc[:, 0].values.astype(float)
    test_diss = test_df.iloc[:, 1].values.astype(float)

    # Remove NaN values
    ref_mask = ~np.isnan(ref_times) & ~np.isnan(ref_diss)
    test_mask = ~np.isnan(test_times) & ~np.isnan(test_diss)
    
    # ===== 2. Interpolation Setup =====
    if interp_method == 'gpr':
        def safe_gp_interpolator(x, y):
            gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)
            valid_mask = ~np.isnan(x) & ~np.isnan(y)
            X = x[valid_mask].reshape(-1, 1)  # Ensure 2D array
            gp.fit(X, y[valid_mask])
            return gp
            
        ref_model = safe_gp_interpolator(ref_times, ref_diss)
        test_model = safe_gp_interpolator(test_times, test_diss)
        
        # Proper reshaping for prediction
        def ref_interp(x):
            return ref_model.predict(np.array(x).reshape(-1, 1))
        
        def test_interp(x):
            return test_model.predict(np.array(x).reshape(-1, 1))
    else:
        valid_methods = ['linear', 'nearest', 'slinear', 'quadratic', 'cubic']
        interp_method = interp_method if interp_method in valid_methods else 'linear'
        
        ref_interp = interp1d(ref_times[ref_mask], ref_diss[ref_mask], 
                            kind=interp_method, bounds_error=False, fill_value=np.nan)
        test_interp = interp1d(test_times[test_mask], test_diss[test_mask],
                             kind=interp_method, bounds_error=False, fill_value=np.nan)

    # ===== 3. Time Points Generation =====
    time_grid = np.arange(window_min, window_max + 1).reshape(-1, 1)  # 2D array for GPR
    
    # Handle both interpolation types
    try:
        max_ref = np.nanmax(ref_interp(time_grid))
        max_test = np.nanmax(test_interp(time_grid))
    except ValueError:
        max_ref = np.nanmax(ref_interp(time_grid.flatten()))
        max_test = np.nanmax(test_interp(time_grid.flatten()))
    
    max_diss = max(max_ref, max_test)
    
    if max_diss <= 60:
        valid_times = np.unique(np.concatenate([
            np.arange(0, window_max+1, 3),
            np.arange(0, window_max+1, 5)
        ])).astype(int)
    else:
        valid_times = np.arange(0, window_max+1, 5).astype(int)
    
    valid_times = valid_times[(valid_times >= window_min) & (valid_times <= window_max)]

    # ===== 4. Combination Generation =====
    start_time = time.time()
    seq_lengths = np.random.randint(3, 7, num_samples)
    
    # Pre-allocate homogeneous array
    all_seqs = np.full((num_samples, 6), -1, dtype=int)
    f2_scores = np.full(num_samples, np.nan)
    compliance_status = np.full(num_samples, False)
    compliance_reasons = [''] * num_samples
    
    with tqdm(total=num_samples, desc="Processing combinations") as pbar:
        for i in range(num_samples):
            try:
                # Generate valid sequence with error checking
                available_points = valid_times[(valid_times > 0) & (valid_times < window_max)]
                if len(available_points) < (seq_lengths[i]-2):
                    raise ValueError("Not enough valid time points")
                
                mid_points = np.random.choice(
                    available_points, 
                    size=seq_lengths[i]-2,
                    replace=False
                )
                seq = np.sort(np.concatenate([[0], mid_points, [window_max]]))
                seq = seq.astype(int)
                
                # Store in homogeneous array
                seq_len = len(seq)
                if seq_len > 6:
                    raise ValueError("Sequence too long")
                all_seqs[i, :seq_len] = seq
                
                # Get dissolution values with proper reshaping
                if interp_method == 'gpr':
                    seq_2d = seq.reshape(-1, 1)  # Ensure 2D for GPR
                    ref_vals = ref_interp(seq_2d)
                    test_vals = test_interp(seq_2d)
                else:
                    ref_vals = ref_interp(seq)
                    test_vals = test_interp(seq)
                
                # Validate values
                if np.isnan(ref_vals).any() or np.isnan(test_vals).any():
                    raise ValueError("NaN in dissolution values")
                
                # Calculate F2
                diff = test_vals - ref_vals
                sum_sq = np.sum(diff**2)
                p = len(seq)
                f2 = 100 - 25 * np.log10(1 + (sum_sq/p))
                f2_scores[i] = f2
                
                # Check compliance
                compliant, reasons = check_regulatory_compliance(
                    seq, regulation,
                    dict(zip(seq, ref_vals)),
                    dict(zip(seq, test_vals))
                )
                compliance_status[i] = compliant
                compliance_reasons[i] = ', '.join(reasons)

            except (ValueError, IndexError) as e:
                f2_scores[i] = np.nan
                compliance_reasons[i] = str(e)
                
            pbar.update(1)

    # ===== 5. Results Processing =====
    valid_mask = ~np.isnan(f2_scores)
    results = []
    for i in np.where(valid_mask)[0]:
        seq = all_seqs[i][all_seqs[i] != -1].tolist()
        results.append({
            'sequence': seq,
            'f2': round(f2_scores[i], 2),
            'compliant': compliance_status[i],
            'reasons': compliance_reasons[i],
            'length': len(seq)
        })
    
    # Sort by descending F2 score
    results.sort(key=lambda x: -x['f2'])
    
    print(f"\nProcessed {num_samples} combinations in {time.time()-start_time:.2f}s")
    return results[:500], results  # Return top 500 and all results

if check_time_points(test_df) and check_time_points(reference_df):
    print("Check 1: A minimum of three-time points - PASSED")
else:
    print("Check 1: A minimum of three-time points - FAILED, NO f2 Calculated")

if two_time_points(test_df) and two_time_points(reference_df):
    print("Check 2: 1,2 time points less than 85 percent dissoultion - PASSED")
else:
    print("Check 2: 1,2 time points less than 85 percent dissoultion- FAILED, NO f2 Calculated")

if min15__check(test_df) and min15__check(reference_df):
    print("Check 3: In 15 min their is greater than 85 percent dissolution - PASSED, NO f2 Calculated")
else:
    print("Check 4: In 15 min their is No greater than 85 percent dissolution - FAILED")

if check_same_time_points(test_df, reference_df):
    print("Check 4: The time points same for both products - PASSED")
else:
    print("Check 4: The time points same for both products - FAILED")

if check_sample_units(test_df) and check_sample_units(reference_df):
    print("Check 5: At least 12 individual sample units - PASSED")
else:
    print("Check 5: At least 12 individual sample units - FAILED")

print(" ")
dissolution_curve(reference_df,test_df)
dissolution_curve_interval(reference_df, test_df)
print(" ")
print("Choose Market:")
print("Choose 1 for FDA")
print("Choose 2 for EMA/ICH/Canda/AUs")
print("Choose 3 for China")
print("Choose 4 for ASEAN")
print("Choose 5 for ANVISa")

input1 = int(input("Input number: "))

def check_both_85(reference_df, test_df):
    """Check if both reference and test meet the >= 85% criteria. Returns True if criteria is met, False otherwise."""
    ref_max = reference_df.iloc[:, 1:].mean(axis=1).max()
    test_max = test_df.iloc[:, 1:].mean(axis=1).max()
    return ref_max >= 85 and test_max >= 85

def check_either_85(reference_df, test_df):
    """Check if either reference or test meet the >= 85% criteria. Returns True if criteria is met, False otherwise."""
    ref_max = reference_df.iloc[:, 1:].mean(axis=1).max()
    test_max = test_df.iloc[:, 1:].mean(axis=1).max()
    return ref_max >= 85 or test_max >= 85

if input1 == 1:
    print("According to FDA >= 85 guidelines")
    print(" ")
    both_85_criteria = check_both_85(reference_df, test_df)
    either_85_criteria = check_either_85(reference_df, test_df)

    print("CV check results:")
    cv_check = check_cv(test_df) and check_cv(reference_df)
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products -", cv_check)
    print(" ")

    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df = reference_df.reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    if check_same_time_points(test_df, reference_df):
        if both_85_criteria:
            print("\nAnalysis based on FDA Both >= 85 criteria:")
            changed_data_both85_f2s(reference_df, test_df)
            if not cv_check:
                print("\nWarning: CV requirements not met. Results may not be valid for regulatory purposes.")
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_both85_bca(reference_df, test_df)
        elif either_85_criteria:
            print("\nAnalysis based on FDA Either >= 85 criteria:")
            changed_data_either85_FDA_f2s(reference_df, test_df)
            if not cv_check:
                print("\nWarning: CV requirements not met. Results may not be valid for regulatory purposes.")
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_FDA_bca(reference_df, test_df)
        else:
            print("\nWarning: Neither 'Both >= 85' nor 'Either >= 85' criteria are met.")
            print("Performing f2 calculations anyway for reference:")
            print("\nResults using Both >= 85 criteria:")
            changed_data_both85_f2s(reference_df, test_df)
            print("\nResults using Either >= 85 criteria:")
            changed_data_either85_FDA_f2s(reference_df, test_df)
    else:
        print("Error: Time points between test and reference datasets do not match.")
        print("Calculations cannot be performed.")
    run_predictive = input("\nDo you want to run predictive optimal combination analysis? (yes/no): ")
if run_predictive.lower() == 'yes':
    # Determine candidate window
    window_min, window_max = determine_candidate_window(
        reference_mean_df, 
        test_mean_df,
        step=5,
        initial_threshold=10
    )
    
    # Map regulation for predictive analysis
    regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
    selected_regulation = regulation_map.get(input1, "FDA")
    
    # Run predictive analysis under different conditions
    conditions = [("Diff threshold 10", 10), 
                 ("Diff threshold 20", 20), 
                 ("No diff check", None)]
    
    all_candidates = []
    overall_best = None
    overall_best_f2 = -np.inf

    print(f"\nCandidate window for combination search: {window_min} to {window_max}")
    
    for cond_label, threshold in conditions:
        print(f"\nProcessing candidates for condition: {cond_label}")
        
        # Get results for this condition
        top_results, all_results = predictive_optimal_combinations_advanced(
            reference_mean_df,
            test_mean_df,
            regulation=selected_regulation,
            window_min=window_min,
            window_max=window_max,
            diff_threshold=threshold,
            num_samples=1000000
        )
        
        # Add condition label to results
        for res in top_results:
            res['condition'] = cond_label
        all_candidates.extend(top_results)
        
        # Track overall best
        if top_results:
            current_best = max(top_results, key=lambda x: x['f2'])
            if current_best['f2'] > overall_best_f2:
                overall_best = current_best
                overall_best_f2 = current_best['f2']

    # Display final results
    if overall_best:
        print("\n=== Optimal Predictive Combination ===")
        print(f"Condition: {overall_best.get('condition','N/A')}")
        print(f"\n Optimal Time Points: {overall_best['sequence']}")
        print(f"Predicted f2 Score: {overall_best['f2']}")
        if overall_best['reasons']:
            print(f"Compliance Issues: {', '.join_overall_best['reasons']}")
        else:
            print("Regulatory Compliance: Passed")
    
        plt.figure(figsize=(12, 6))
        plt.plot(overall_best['sequence'],
                     interpolate_dissolution_curve(reference_mean_df, overall_best['sequence'], method='gpr'),
                     'bo-', label='Reference')
        plt.plot(overall_best['sequence'],
                     interpolate_dissolution_curve(test_mean_df, overall_best['sequence'], method='gpr'),
                     'r*--', label='Test')
        plt.title(f"Optimal Predictive Dissolution Profile (f2 = {overall_best['f2']})")
        plt.xlabel('Time (min)')
        plt.ylabel('Dissolution (%)')
        plt.legend()
        plt.grid(True)
        plt.show()
    else:
          print("❌ No overall optimal candidate could be determined.")

elif input1 == 2:
    print("According to EMA/ICH/Canada/Australia guidelines")
    print("CV check results:")
    cv_check = check_cv(test_df) and check_cv(reference_df)
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products -", cv_check)

    # Remove time point 0 if present
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df = reference_df.reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    # Check time points alignment
    time_points_check = check_same_time_points(test_df, reference_df)
    if time_points_check:
        # Always perform f2 calculations using either > 85% criterion
        either_85_criteria = check_either_85(reference_df, test_df)
        if either_85_criteria:
            print("\nAnalysis based on EMA/ICH/Canada/Australia guidelines (either > 85%):")
            changed_data_either85_f2s(reference_df, test_df)
            if not cv_check:
                print("\nWarning: CV requirements not met. Results may not be valid for regulatory purposes.")
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
        else:
            print("\nWarning: 'Either > 85' criteria are not met.")
            print("Performing f2 calculations anyway for reference:")
            changed_data_either85_f2s(reference_df, test_df)
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
    else:
        print("Error: Time points between test and reference datasets do not match.")
        print("Calculations cannot be performed.")
    run_predictive = input("\nDo you want to run predictive optimal combination analysis? (yes/no): ")
    if run_predictive.lower() == 'yes':
        # Determine candidate window
        window_min, window_max = determine_candidate_window(
            reference_mean_df, test_mean_df, 
             step=5, initial_threshold=10
        )
        
        # Map regulation for predictive analysis
        regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
        selected_regulation = regulation_map.get(input1, "FDA")
        
        # Run predictive analysis under different conditions
        conditions = [("Diff threshold 10", 10), ("Diff threshold 20", 20), ("No diff check", None)]
        all_candidates = []
        overall_best = None
        overall_best_f2 = -np.inf
        
        print("\nCandidate window for combination search: from", window_min, "to", window_max)
        for cond_label, threshold in conditions:
            print(f"\nProcessing candidates for condition: {cond_label}")
            best_result, candidates = predictive_optimal_combinations_advanced(
                reference_mean_df, test_mean_df,
                regulation=selected_regulation,
                window_min=window_min,
                window_max=window_max,
                diff_threshold=threshold,
                interp_method='gpr'
            )
            # Tag each candidate with the condition label
            for cand in candidates:
                cand['condition'] = cond_label
            all_candidates.extend(candidates)
            
            if best_result is not None and best_result["f2"] > overall_best_f2:
                overall_best_f2 = best_result["f2"]
                overall_best = best_result.copy()
                overall_best['condition'] = cond_label
        
        # Sort all candidates by f2 descending
        all_candidates_sorted = sorted(all_candidates, key=lambda x: -x["f2"])
        
        # -------------------------- STEP 5: OUTPUT RESULTS --------------------------
        print("\n=== All Compliant Candidate Sequences ===")
        if all_candidates_sorted:
            for idx, cand in enumerate(all_candidates_sorted):
                seq = cand['sequence']
                f2 = cand['f2']
                comp = "Yes" if cand['compliant'] else "No"
                reasons = ", ".join(cand['reasons']) if cand['reasons'] else "None"
                cond = cand.get('condition', "N/A")
                print(f"{idx+1:3d}. Condition: {cond} | Sequence: {seq} | f2 Score: {f2} | Compliance: {comp} | Reasons: {reasons}")
        else:
            print("No compliant candidate sequences were found under any condition.")
        
        if overall_best:
            print("\n=== Optimal Compliant Predictive Combination ===")
            print(f"Condition: {overall_best.get('condition','N/A')}")
            print(f"Time Points: {overall_best['sequence']}")
            print(f"f2 Score: {overall_best['f2']}")
            if overall_best['reasons']:
                print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
            else:
                print("Compliance: Yes")
            
            # Plot the optimal dissolution curves
            plt.figure(figsize=(12, 6))
            plt.plot(overall_best['sequence'],
                     interpolate_dissolution_curve(reference_mean_df, overall_best['sequence'], method='gpr'),
                     'bo-', label='Reference')
            plt.plot(overall_best['sequence'],
                     interpolate_dissolution_curve(test_mean_df, overall_best['sequence'], method='gpr'),
                     'r*--', label='Test')
            plt.title(f"Optimal Predictive Dissolution Profile (f2 = {overall_best['f2']})")
            plt.xlabel('Time (min)')
            plt.ylabel('Dissolution (%)')
            plt.legend()
            plt.grid(True)
            plt.show()
        else:
            print("No overall optimal candidate could be determined.")
        
elif input1 == 3:
    print("According to China's guidelines")
    print("CV check results:")
    cv_check = check_cv(test_df) and check_cv(reference_df)
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products -", cv_check)

    # Remove time point 0 if present
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df = reference_df.reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    # Check time points alignment
    time_points_check = check_same_time_points(test_df, reference_df)
    if time_points_check:
        # Always perform f2 calculations using either > 85% criterion
        either_85_criteria = check_either_85(reference_df, test_df)
        if either_85_criteria:
            print("\nAnalysis based on China's guidelines (either > 85%):")
            changed_data_either85_f2s(reference_df, test_df)
            if not cv_check:
                print("\nWarning: CV requirements not met. Results may not be valid for regulatory purposes.")
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
        else:
            print("\nWarning: 'Either > 85' criteria are not met.")
            print("Performing f2 calculations anyway for reference:")
            changed_data_either85_f2s(reference_df, test_df)
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
    else:
        print("Error: Time points between test and reference datasets do not match.")
        print("Calculations cannot be performed.")
    run_predictive = input("\nDo you want to run predictive optimal combination analysis? (yes/no): ")
    if run_predictive.lower() == 'yes':
        # Determine candidate window
        window_min, window_max = determine_candidate_window(
            reference_mean_df, test_mean_df, 
             step=5, initial_threshold=10
        )
        
        # Map regulation for predictive analysis
        regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
        selected_regulation = regulation_map.get(input1, "FDA")
        
        # Run predictive analysis under different conditions
        conditions = [("Diff threshold 10", 10), ("Diff threshold 20", 20), ("No diff check", None)]
        all_candidates = []
        overall_best = None
        overall_best_f2 = -np.inf
        
        print("\nCandidate window for combination search: from", window_min, "to", window_max)
        for cond_label, threshold in conditions:
            print(f"\nProcessing candidates for condition: {cond_label}")
            best_result, candidates = predictive_optimal_combinations_advanced(
                reference_mean_df, test_mean_df,
                regulation=selected_regulation,
                window_min=window_min,
                window_max=window_max,
                diff_threshold=threshold,
                interp_method='gpr'
            )
            # Tag each candidate with the condition label
            for cand in candidates:
                cand['condition'] = cond_label
            all_candidates.extend(candidates)
            
            if best_result is not None and best_result["f2"] > overall_best_f2:
                overall_best_f2 = best_result["f2"]
                overall_best = best_result.copy()
                overall_best['condition'] = cond_label
        
        # Sort all candidates by f2 descending
        all_candidates_sorted = sorted(all_candidates, key=lambda x: -x["f2"])
        
        # -------------------------- STEP 5: OUTPUT RESULTS --------------------------
        print("\n=== All Compliant Candidate Sequences ===")
        if all_candidates_sorted:
            for idx, cand in enumerate(all_candidates_sorted):
                seq = cand['sequence']
                f2 = cand['f2']
                comp = "Yes" if cand['compliant'] else "No"
                reasons = ", ".join(cand['reasons']) if cand['reasons'] else "None"
                cond = cand.get('condition', "N/A")
                print(f"{idx+1:3d}. Condition: {cond} | Sequence: {seq} | f2 Score: {f2} | Compliance: {comp} | Reasons: {reasons}")
        else:
            print("No compliant candidate sequences were found under any condition.")
        
        if overall_best:
            print("\n=== Optimal Compliant Predictive Combination ===")
            print(f"Condition: {overall_best.get('condition','N/A')}")
            print(f"Time Points: {overall_best['sequence']}")
            print(f"f2 Score: {overall_best['f2']}")
            if overall_best['reasons']:
                print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
            else:
                print("Compliance: Yes")
            
            # Plot the optimal dissolution curves
            plt.figure(figsize=(12, 6))
            plt.plot(overall_best['sequence'],
                     interpolate_dissolution_curve(reference_mean_df, overall_best['sequence'], method='gpr'),
                     'bo-', label='Reference')
            plt.plot(overall_best['sequence'],
                     interpolate_dissolution_curve(test_mean_df, overall_best['sequence'], method='gpr'),
                     'r*--', label='Test')
            plt.title(f"Optimal Predictive Dissolution Profile (f2 = {overall_best['f2']})")
            plt.xlabel('Time (min)')
            plt.ylabel('Dissolution (%)')
            plt.legend()
            plt.grid(True)
            plt.show()
        else:
            print("No overall optimal candidate could be determined.")
                
elif input1 == 4:
    print("According to ASEAN guidelines")
    print("CV check results:")
    cv_check = check_cv(test_df) and check_cv(reference_df)
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products -", cv_check)

    # Remove time point 0 if present
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df = reference_df.reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    # Check time points alignment
    time_points_check = check_same_time_points(test_df, reference_df)
    if time_points_check:
        # Always perform f2 calculations using either > 85% criterion
        either_85_criteria = check_either_85(reference_df, test_df)
        if either_85_criteria:
            print("\nAnalysis based on ASEAN guidelines (either > 85%):")
            changed_data_either85_f2s(reference_df, test_df)
            if not cv_check:
                print("\nWarning: CV requirements not met. Results may not be valid for regulatory purposes.")
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
        else:
            print("\nWarning: 'Either > 85' criteria are not met.")
            print("Performing f2 calculations anyway for reference:")
            changed_data_either85_f2s(reference_df, test_df)
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
    else:
        print("Error: Time points between test and reference datasets do not match.")
        print("Calculations cannot be performed.")
    run_predictive = input("\nDo you want to run predictive optimal combination analysis? (yes/no): ")
    if run_predictive.lower() == 'yes':
        # Determine candidate window
        window_min, window_max = determine_candidate_window(
            reference_mean_df, test_mean_df, 
             step=5, initial_threshold=10
        )
        
        # Map regulation for predictive analysis
        regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
        selected_regulation = regulation_map.get(input1, "FDA")
        
        # Run predictive analysis under different conditions
        conditions = [("Diff threshold 10", 10), ("Diff threshold 20", 20), ("No diff check", None)]
        all_candidates = []
        overall_best = None
        overall_best_f2 = -np.inf
        
        print("\nCandidate window for combination search: from", window_min, "to", window_max)
        for cond_label, threshold in conditions:
            print(f"\nProcessing candidates for condition: {cond_label}")
            best_result, candidates = predictive_optimal_combinations_advanced(
                reference_mean_df, test_mean_df,
                regulation=selected_regulation,
                window_min=window_min,
                window_max=window_max,
                diff_threshold=threshold,
                interp_method='gpr'
            )
            # Tag each candidate with the condition label
            for cand in candidates:
                cand['condition'] = cond_label
            all_candidates.extend(candidates)
            
            if best_result is not None and best_result["f2"] > overall_best_f2:
                overall_best_f2 = best_result["f2"]
                overall_best = best_result.copy()
                overall_best['condition'] = cond_label
        
        # Sort all candidates by f2 descending
        all_candidates_sorted = sorted(all_candidates, key=lambda x: -x["f2"])
        
        # -------------------------- STEP 5: OUTPUT RESULTS --------------------------
        print("\n=== All Compliant Candidate Sequences ===")
        if all_candidates_sorted:
            for idx, cand in enumerate(all_candidates_sorted):
                seq = cand['sequence']
                f2 = cand['f2']
                comp = "Yes" if cand['compliant'] else "No"
                reasons = ", ".join(cand['reasons']) if cand['reasons'] else "None"
                cond = cand.get('condition', "N/A")
                print(f"{idx+1:3d}. Condition: {cond} | Sequence: {seq} | f2 Score: {f2} | Compliance: {comp} | Reasons: {reasons}")
        else:
            print("No compliant candidate sequences were found under any condition.")
        
        if overall_best:
            print("\n=== Optimal Compliant Predictive Combination ===")
            print(f"Condition: {overall_best.get('condition','N/A')}")
            print(f"Time Points: {overall_best['sequence']}")
            print(f"f2 Score: {overall_best['f2']}")
            if overall_best['reasons']:
                print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
            else:
                print("Compliance: Yes")
            
            # Plot the optimal dissolution curves
            plt.figure(figsize=(12, 6))
            plt.plot(overall_best['sequence'],
                     interpolate_dissolution_curve(reference_mean_df, overall_best['sequence'], method='gpr'),
                     'bo-', label='Reference')
            plt.plot(overall_best['sequence'],
                     interpolate_dissolution_curve(test_mean_df, overall_best['sequence'], method='gpr'),
                     'r*--', label='Test')
            plt.title(f"Optimal Predictive Dissolution Profile (f2 = {overall_best['f2']})")
            plt.xlabel('Time (min)')
            plt.ylabel('Dissolution (%)')
            plt.legend()
            plt.grid(True)
            plt.show()
        else:
            print("No overall optimal candidate could be determined.")

elif input1 == 5:
    print("According to ANVISA guidelines")
    print("CV check results:")
    cv_check = check_cv(test_df) and check_cv(reference_df)
    print("CV should be less than 20 at the first (non-zero) time point and less than 10 at the following time points for both products -", cv_check)

    # Remove time point 0 if present
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.drop(reference_df.index[0])
        reference_df = reference_df.reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.drop(test_df.index[0])
        test_df = test_df.reset_index(drop=True)

    # Check time points alignment
    time_points_check = check_same_time_points(test_df, reference_df)
    if time_points_check:
        # Always perform f2 calculations using either > 85% criterion
        either_85_criteria = check_either_85(reference_df, test_df)
        if either_85_criteria:
            print("\nAnalysis based on ANVISA guidelines (either > 85%):")
            changed_data_either85_f2s(reference_df, test_df)
            if not cv_check:
                print("\nWarning: CV requirements not met. Results may not be valid for regulatory purposes.")
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
        else:
            print("\nWarning: 'Either > 85' criteria are not met.")
            print("Performing f2 calculations anyway for reference:")
            changed_data_either85_f2s(reference_df, test_df)
            print("Loading bootstrap percentile.. & Loading BCA...")
            changed_data_either85_bca(reference_df, test_df)
    else:
        print("Error: Time points between test and reference datasets do not match.")
        print("Calculations cannot be performed.")
    run_predictive = input("\nDo you want to run predictive optimal combination analysis? (yes/no): ")
    if run_predictive.lower() == 'yes':
        # Determine candidate window
        window_min, window_max = determine_candidate_window(
            reference_mean_df, test_mean_df, 
             step=5, initial_threshold=10
        )
        
        # Map regulation for predictive analysis
        regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
        selected_regulation = regulation_map.get(input1, "FDA")
        
        # Run predictive analysis under different conditions
        conditions = [("Diff threshold 10", 10), ("Diff threshold 20", 20), ("No diff check", None)]
        all_candidates = []
        overall_best = None
        overall_best_f2 = -np.inf
        
        print("\nCandidate window for combination search: from", window_min, "to", window_max)
        for cond_label, threshold in conditions:
            print(f"\nProcessing candidates for condition: {cond_label}")
            best_result, candidates = predictive_optimal_combinations_advanced(
                reference_mean_df, test_mean_df,
                regulation=selected_regulation,
                window_min=window_min,
                window_max=window_max,
                diff_threshold=threshold,
                interp_method='gpr'
            )
            # Tag each candidate with the condition label
            for cand in candidates:
                cand['condition'] = cond_label
            all_candidates.extend(candidates)
            
            if best_result is not None and best_result["f2"] > overall_best_f2:
                overall_best_f2 = best_result["f2"]
                overall_best = best_result.copy()
                overall_best['condition'] = cond_label
        
        # Sort all candidates by f2 descending
        all_candidates_sorted = sorted(all_candidates, key=lambda x: -x["f2"])
        
        # -------------------------- STEP 5: OUTPUT RESULTS --------------------------
        print("\n=== All Compliant Candidate Sequences ===")
        if all_candidates_sorted:
            for idx, cand in enumerate(all_candidates_sorted):
                seq = cand['sequence']
                f2 = cand['f2']
                comp = "Yes" if cand['compliant'] else "No"
                reasons = ", ".join(cand['reasons']) if cand['reasons'] else "None"
                cond = cand.get('condition', "N/A")
                print(f"{idx+1:3d}. Condition: {cond} | Sequence: {seq} | f2 Score: {f2} | Compliance: {comp} | Reasons: {reasons}")
        else:
            print("No compliant candidate sequences were found under any condition.")
        
        if overall_best:
            print("\n=== Optimal Compliant Predictive Combination ===")
            print(f"Condition: {overall_best.get('condition','N/A')}")
            print(f"Time Points: {overall_best['sequence']}")
            print(f"f2 Score: {overall_best['f2']}")
            if overall_best['reasons']:
                print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
            else:
                print("Compliance: Yes")
            
            # Plot the optimal dissolution curves
            plt.figure(figsize=(12, 6))
            plt.plot(overall_best['sequence'],
                     interpolate_dissolution_curve(reference_mean_df, overall_best['sequence'], method='gpr'),
                     'bo-', label='Reference')
            plt.plot(overall_best['sequence'],
                     interpolate_dissolution_curve(test_mean_df, overall_best['sequence'], method='gpr'),
                     'r*--', label='Test')
            plt.title(f"Optimal Predictive Dissolution Profile (f2 = {overall_best['f2']})")
            plt.xlabel('Time (min)')
            plt.ylabel('Dissolution (%)')
            plt.legend()
            plt.grid(True)
            plt.show()
        else:
            print("No overall optimal candidate could be determined.")

##################################################################################################################
##########################################SOR############################################################################

# Mainapp.py
import warnings
import utils.domain, utils.lhs, utils.flowrate, utils.hplcread, utils.optimization
import pandas as pd
from summit.strategies import TSEMO,Random, SNOBFIT
from tkinter import Tk
from tkinter.filedialog import askdirectory,askopenfilename

with warnings.catch_warnings():
    warnings.simplefilter("ignore")

print("Select master file through the dialog box")
Tk().withdraw()
lhs_path = askopenfilename(title="Select master file")
df_lhs1 = pd.read_excel(lhs_path,sheet_name="LHS")
df_objective1 = pd.read_excel(lhs_path,sheet_name="Objectives")
df_pump = pd.read_excel(lhs_path, sheet_name="Pump Details")
df_reactor = pd.read_excel(lhs_path, sheet_name="Reactor Details",header=None)

print("----------------------------Stage1: Initializing domain for SOR---------------------------------.\n")
domain = utils.domain.create_domain_X(df_lhs1)
domain = utils.domain.create_domain_y(domain, df_objective1)

print("Select path to save the results")
save_dir = askdirectory(title='Select Path')
print("-----------------------------Stage2: Running LHS--------------------------------------------------\n")
nexp = int(input("Enter no of LHS experiment"))
nobj = int(input("Enter no of objectives"))
lhs_exp = utils.lhs.LHSDesign(nexp,domain,nobj)
print("-----------------------------Writing LHS result to disk--------------------------------------------\n")
lhs_exp.to_excel(save_dir + "/SOR_result.xlsx")

print("Help for pump combination & reaction scheme")
print("Pump | Reaction Scheme | Identifier\n 3 | KSM+R1+R2 -> P | 1\n 3 | KSM+ R1 -> I1+QD -> P | 2\n 4 | KSM+R1->I1+R2->I2+QD->P | 1\n 4 | KSM+R1+R2 -> I1+QD -> P | 2")
npump = int(input("How many pumps are you using?"))
rscheme = int(input("Enter reaction scheme identifier"))
if npump == 3 and rscheme == 1:
    lhs_pump = utils.flowrate.pump3_flow_rate1(lhs_exp,df_pump,df_reactor)
elif npump == 3 and rscheme == 2:
    lhs_pump = utils.flowrate.pump3_flow_rate2(lhs_exp, df_pump, df_reactor)
elif npump == 4 and rscheme == 1:
    lhs_pump = utils.flowrate.pump4_flow_rate1(lhs_exp, df_pump, df_reactor)
elif npump == 4 and rscheme == 2:
    lhs_pump = utils.flowrate.pump4_flow_rate2(lhs_exp,df_pump,df_reactor)
print("......................Writing pump flowrates to disk.....................\n")
lhs_pump.to_excel(save_dir + "/SOR_result.xlsx")

print("-----------------------------------Stage3: Read HPLC data from directory--------------------------------\n")
# YminRT = 2.0
# YmaxRT = 2.35
# IminRT = 1.3
# ImaxRT = 1.98
# minRTISO = 3.9
# maxRTISO = 4.2
YminRT = float(input("Enter min RT value for Purity(Yield) : "))
YmaxRT = float(input("Enter max RT value for Purity(Yield) : "))
IminRT_ = []
ImaxRT_ = []
n = int(input("Enter number of Impurities : "))
for i in range(0, n):
    IminRT = float(input("Enter min RT value for Impurity : "))
    ImaxRT = float(input("Enter max RT value for Impurity : "))
    # adding the element    
    IminRT_.append(IminRT)
    ImaxRT_.append(ImaxRT)
minRTISO = float(input("Enter min RT value for standard : "))
maxRTISO = float(input("Enter max RT value for standard : "))
print("Select HPLC folder through the dialog box")
path = askdirectory(title='Select HPLC Folder') # shows dialog box and return the path
print(path)
monitor_generator = utils.hplcread.monitor_folder_creation1_csv(lhs_pump, path, nobj,YminRT,YmaxRT,IminRT_,ImaxRT_,minRTISO,maxRTISO)
i=1
print("------------------------------------Reading HPLC results-------------------------------\n")
while i <=nexp:
    try:
        result = next(monitor_generator)
        if result is not None:
            result.to_excel(save_dir + "/SOR_result.xlsx")
            print("HPLC results read for :",i)
            i+=1
    except StopIteration:
        print("Generator stopped early")
        break
print("------------------------------------HPLC data read & write to disk completed!!---------------------------------\n")

print("------------------------------------Stage4: Running Optimization----------------------------------------\n")
nsor = int(input("Enter number of SOR iterations"))
sor_in = result[result.columns.drop(list(result.filter(regex='Flow rate')))]
sor_in = sor_in.drop(columns={"Reaction time"})
sor_in, result = utils.optimization.run_optimization(domain,sor_in,nobj,npump,rscheme, lhs_exp, df_pump, df_reactor) 
print("Suggested for : 1"+"/"+str(nsor)+" experiments \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/") 
result.to_excel(save_dir + "/SOR_result.xlsx")
monitor_generator1 = utils.hplcread.monitor_folder_creation1_csv(sor_in, path, nobj,YminRT,YmaxRT,IminRT_,ImaxRT_,minRTISO,maxRTISO,use_dataframe_last_idx=True)
i=1
while i <= nsor:
    try:
        sor_in = next(monitor_generator1)
        if sor_in is not None:
            result = result.fillna(sor_in.iloc[:, -nobj:])
            result.to_excel(save_dir + "/SOR_result.xlsx")
            print("SOR_IN", sor_in)
            if i == nsor:
                break
            else:
                sor_in, result = utils.optimization.run_optimization(domain,sor_in,nobj,npump,rscheme, lhs_exp, df_pump, df_reactor)
                i+=1
                print("Suggested for : "+str(i)+"/"+str(nsor)+" experiments \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/")
                result.to_excel(save_dir + "/SOR_result.xlsx")
                monitor_generator1 = utils.hplcread.monitor_folder_creation1_csv(sor_in, path, nobj,YminRT,YmaxRT,IminRT_,ImaxRT_,minRTISO,maxRTISO,use_dataframe_last_idx=True)
            

    except StopIteration:
        print("Generator stopped early")
        break

#domain.py
from summit.domain import Domain, ContinuousVariable
import pandas as pd

def create_domain_X(df):
    
    """
    -----------------------------------INPUTS-------------------------------------------
    df : Initial dataframe from user input on GUI - sheet related to LHS - Internal Pass
    ------------------------------------------------------------------------------------
    """
    
    domain = Domain()
    for i in range(1,df.shape[1]):
        if df.iloc[:,i].notna().all():
            name = df.iloc[:,i].name.replace(" ","")
            desc = df.iloc[:1,i][0]
            lb = df.iloc[1:2,i][1]
            ub = df.iloc[2:3,i][2]
            domain += ContinuousVariable(name=name, description=desc, bounds=[lb, ub])
    return domain

def create_domain_y(domain,df):
    
    """
    -----------------------------------INPUTS--------------------------------------------
    df : Initial dataframe from user input on GUI - sheet related to LHS - Internal Pass
    -------------------------------------------------------------------------------------
    """
    for i in range(1, df.shape[1]):
        if df.iloc[:,i].notna().all():
            name = df.iloc[:,i].name.replace(" ","")
            desc = df.iloc[:1,i][0]
            maximize = True if(df.iloc[1:2,i][1] == "Maximize") else False
            domain += ContinuousVariable(name=name, description=desc, bounds=[0, 100], is_objective = True, maximize = maximize)
    return domain

#flowrate.py
def pump3_flow_rate1(df_lhs,df_pump,df_reactor):
    '''
    ----------------------REACTION SCHEME---------------------------
    Reaction Scheme : KSM+R1+R2 -> P
    ----------------------------------------------------------------
    
    -------------------------INPUTS---------------------------------
    df_lhs : DataFrame returned by LHS function - Internal Pass
    df_pump : Excel from GUI section 2 passed as DataFrame
    df_reactor : Excel from GUI section 2 passed as DataFrame
    ----------------------------------------------------------------
    '''
    df_final_values = df_lhs.copy()
    
    ## Calculate reaction time
    df_final_values['Reaction time'] = df_final_values['ResidencetimeT1']*df_reactor[3][0]
    
    ## Calculate pump 1 flowrate
    df_final_values['Flow rate of Pump 1'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*(
    (1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))/
    ((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))
     +([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/
                                           df_pump['Molecular weight (g/mol)'][1])*1000))+
       ([df_final_values['Equivalence2']]/((df_pump['Concentration of reagent (g/ml)'][2]/
                                            df_pump['Molecular weight (g/mol)'][2])*1000)))))[0]
    
    ## Calculate pump 2 flowrate
    df_final_values['Flow rate of Pump 2'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*(([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))/
                                            ((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+
                                             ([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))+
                                             ([df_final_values['Equivalence2']]/((df_pump['Concentration of reagent (g/ml)'][2]/df_pump['Molecular weight (g/mol)'][2])*1000)))))[0]
    
    ##Calculate pump 3 flowrate
    df_final_values['Flow rate of Pump 3'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*(([df_final_values['Equivalence2']]/((df_pump['Concentration of reagent (g/ml)'][2]/df_pump['Molecular weight (g/mol)'][2])*1000)))/((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))+([df_final_values['Equivalence2']]/((df_pump['Concentration of reagent (g/ml)'][2]/df_pump['Molecular weight (g/mol)'][2])*1000))))[0]
    
    return df_final_values

def pump3_flow_rate2(df_lhs,df_pump,df_reactor):
    '''
    ----------------------REACTION SCHEME---------------------------
    Reaction Scheme : KSM+ R1 -> I1+QD -> P
    ----------------------------------------------------------------
    
    -------------------------INPUTS---------------------------------
    df_lhs : DataFrame returned by LHS function - Internal Pass
    df_pump : Excel from GUI section 2 passed as DataFrame
    df_reactor : Excel from GUI section 2 passed as DataFrame
    ----------------------------------------------------------------
    '''
    df_final_values = df_lhs.copy()
    
    ## Calculate reaction time
    df_final_values['Reaction time'] = df_final_values['ResidencetimeT1']*df_reactor[3][0]
    
    ## Calculate pump 1 flowrate
    df_final_values['Flow rate of Pump 1'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))/
                                               ((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+
                                                ([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000)))))[0]
    
    ## Calculate pump 2 flowrate
    df_final_values['Flow rate of Pump 2'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*((([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))/
                                                          ((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+
                                                           ([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))))))[0]
    
    ## Calculate pump 3 flowrate
    Qpump_id = int(df_pump['(Quenching) w.r.t. Pump'][2])
    Qpump_factor = df_pump['(Quenching) Flow rate factor w.r.t. Pump'][2]
    df_final_values['Flow rate of Pump 3'] = Qpump_factor * df_final_values.iloc[:,-(3-Qpump_id)]
    
    return df_final_values

def pump4_flow_rate1(df_lhs,df_pump,df_reactor):
    '''
    ----------------------REACTION SCHEME---------------------------
    Reaction Scheme : KSM+R1->I1+R2->I2+QD->P
    ----------------------------------------------------------------
    
    -------------------------INPUTS---------------------------------
    df_lhs : DataFrame returned by LHS function - Internal Pass
    df_pump : Excel from GUI section 2 passed as DataFrame
    df_reactor : Excel from GUI section 2 passed as DataFrame
    ----------------------------------------------------------------
    '''
    
    df_final_values = df_lhs.copy()
    df_final_values['Reaction time'] = None
    
    ## Calculate pump 1 flowrate
    df_final_values['Flow rate of Pump 1'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))/
                                                             ((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+
                                                              ([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000)))))[0]
    
    ## Calculate pump 2 flowrate
    df_final_values['Flow rate of Pump 2'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*((([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))/
                                                          ((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+
                                                           ([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))))))[0]
    
    ## Calculate pump 3 flowrate
    df_final_values['Flow rate of Pump 3'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*(1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))/(((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))+([df_final_values['Equivalence2']]/((df_pump['Concentration of reagent (g/ml)'][2]/df_pump['Molecular weight (g/mol)'][2])*1000))))*[df_final_values['Equivalence2']])[0]
    
    ## Calculate pump 4 flowrate
    Qpump_id = int(df_pump['(Quenching) w.r.t. Pump'][3])
    Qpump_factor = df_pump['(Quenching) Flow rate factor w.r.t. Pump'][3]   
    df_final_values['Flow rate of Pump 4'] = Qpump_factor * df_final_values.iloc[:,-(4-Qpump_id)]
    
    ## Calculate reaction time
    df_final_values['Reaction time'] = df_reactor[1][1]/((df_final_values['Flow rate of Pump 1']+df_final_values['Flow rate of Pump 2']+df_final_values['Flow rate of Pump 3'])*df_reactor[3][0])
    
    return df_final_values

def pump4_flow_rate2(df_lhs,df_pump,df_reactor):
    '''
    ----------------------REACTION SCHEME---------------------------
    Reaction Scheme : KSM+R1+R2 -> I1+QD -> P
    ----------------------------------------------------------------
    
    -------------------------INPUTS---------------------------------
    df_lhs : DataFrame returned by LHS function - Internal Pass
    df_pump : Excel from GUI section 2 passed as DataFrame
    df_reactor : Excel from GUI section 2 passed as DataFrame
    ----------------------------------------------------------------
    '''
    
    df_final_values = df_lhs.copy()
    
    ## Calculate reaction time
    df_final_values['Reaction time'] = df_final_values['ResidencetimeT1']*df_reactor[3][0]
    
    ## Calculate pump 1 flowrate
    df_final_values['Flow rate of Pump 1'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))/
                                                         ((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+
                                                          ([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))+
                                                          ([df_final_values['Equivalence2']]/((df_pump['Concentration of reagent (g/ml)'][2]/df_pump['Molecular weight (g/mol)'][2])*1000)))))[0]
    
    ## Calculate pump 2 flowrate
    df_final_values['Flow rate of Pump 2'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*(([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))/((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))+([df_final_values['Equivalence2']]/((df_pump['Concentration of reagent (g/ml)'][2]/df_pump['Molecular weight (g/mol)'][2])*1000)))))[0]
    
    ## Calculate pump 3 flowrate
    df_final_values['Flow rate of Pump 3'] = ((df_reactor[1][0]/[df_final_values['ResidencetimeT1']])*(([df_final_values['Equivalence2']]/((df_pump['Concentration of reagent (g/ml)'][2]/df_pump['Molecular weight (g/mol)'][2])*1000))/
                                                         ((1/((df_pump['Concentration of reagent (g/ml)'][0]/df_pump['Molecular weight (g/mol)'][0])*1000))+
                                                          ([df_final_values['Equivalence1']]/((df_pump['Concentration of reagent (g/ml)'][1]/df_pump['Molecular weight (g/mol)'][1])*1000))+
                                                          ([df_final_values['Equivalence2']]/((df_pump['Concentration of reagent (g/ml)'][2]/df_pump['Molecular weight (g/mol)'][2])*1000)))))[0]
    
    ## Calculate pump 4 flowrate
    Qpump_id = int(df_pump['(Quenching) w.r.t. Pump'][3])
    Qpump_factor = df_pump['(Quenching) Flow rate factor w.r.t. Pump'][3]
    df_final_values['Flow rate of Pump 4'] = Qpump_factor * df_final_values.iloc[:,-(4-Qpump_id)]
    
    return df_final_values

##################hplcread######################
import os
import pandas as pd
import utils.hplcread
import time
import numpy as np

def HPLC_data_read(file_path):
    """
    -----------------------------INPUTS-----------------------------
    file_path : Common file path where the HPLC excel data is stored.
    ----------------------------------------------------------------
    
    ---------------------------CONDITIONS---------------------------
    Read only the sheet named "Peak" from the HPLC excel data.
    -----------------------------------------------------------------
    """
    ## Read data from HPLC generated output file - DADRead.m
    data = pd.read_excel(file_path,sheet_name='Peak')
    data_final = pd.DataFrame()
    data_final['Peak_Area'] = data.iloc[:,13]
    data_final['RT'] = data.iloc[:,10]
    data_final['Signal_data'] = data.iloc[:,3]
    return data_final.to_numpy()

def HPLC_data_read_csv(file_path):
    """
    -----------------------------INPUTS-----------------------------
    file_path : Common file path where the HPLC excel data is stored.
    ----------------------------------------------------------------
    
    ---------------------------CONDITIONS---------------------------
    Read only the sheet named "Peak" from the HPLC excel data.
    -----------------------------------------------------------------
    """
    ## Read data from HPLC generated output file - DADRead.m
    data = pd.read_csv(file_path)
    data_final = pd.DataFrame()
    data_final['Peak_Area'] = data['Area']
    data_final['RT'] = data['RT']
    # data_final['Signal_data'] = data.iloc[:,3]
    return data_final.to_numpy()

def impurity_response(data,IminRT,ImaxRT,areaISO):
    signalobjB=1
    areaB = 0
    # minRTB = 1.3
    # maxRTB = 1.98
    for i in range(data.shape[0]):
        # loop that checks each RT
        if IminRT <= data[i, 1] <= ImaxRT:  # if rt is within range
            if data[i, 2] == signalobjB:
                # if signalobjA matches the value in the third column
                areaB += data[i, 0]
    impurities_result = areaB/areaISO
    print(imimpurities_result,'+++++++++++++++++')
    return impurities_result

def impurity_response_csv(data,IminRT,ImaxRT,areaISO):
    signalobjB=1
    areaB = 0
    # minRTB = 1.3
    # maxRTB = 1.98
    for i in range(data.shape[0]):
        # loop that checks each RT
        if IminRT <= data[i, 1] <= ImaxRT:  # if rt is within range
            areaB += data[i, 0]
    impurities_result = areaB/areaISO
    print(impurities_result,'-------------------')
    return impurities_result

def response_HPLC(data,YminRT,YmaxRT,IminRT,ImaxRT,minRTISO,maxRTISO,nobj):
    
    """
    ---------------------------------INPUTS--------------------------------------------
    data : HPLC data as dataframe, read from shared folder - Internal Pass.
    YminRT - Minimum value for Yield related to HPLC Residence Time - User Input.
    YmaxRT - Maximum value for Yield related to HPLC Residence Time - User Input.
    IminRT - Minimum value for Impurity1 related to HPLC Residence Time - User Input.
    ImaxRT - Maximum value for Impurity1 related to HPLC Residence Time - User Input.
    minRTISO - Minimum value for ISO related to HPLC Residence Time - User Input.
    maxRTISO - Maximum value for ISO related to HPLC Residence Time - User Input.
    nobj - No of objectives - Internal pass
    ------------------------------------------------------------------------------------
    
    --------------------------------CONDITIONS------------------------------------------
    If no of objectives > 2:
    replicate the areaB computation with relevant user input RT values
    -------------------------------------------------------------------------------------
    """
    ## Compute response - responseFunction.m
    signalobjA=1
    signalobjB=1
    signalIS=1
    
    areaA = 0
    # YminRTA = 2.0
    # YmaxRTA = 2.35
    for i in range(data.shape[0]):
        # loop that checks each RT
        if YminRT <= data[i, 1] <= YmaxRT:  # if rt is within range
            # print("Data",data[i, 1])
            if data[i, 2] == signalobjA:
                # if signalobjA matches the value in the third column
                areaA += data[i, 0]
                
                
    # minRTISO = 3.9
    # maxRTISO = 4.2
    areaISO = 0
    for i in range(data.shape[0]):
        # loop that checks each RT
        if minRTISO <= data[i, 1] <= maxRTISO:  # if rt is within range
            if data[i, 2] == signalIS:
                # if signalobjA matches the value in the third column
                areaISO += data[i, 0]
                
    
    ## Run impurities calculation
    response = []
    for i in range(nobj-1): 
        impurities_result = utils.hplcread.impurity_response(data,IminRT[i],ImaxRT[i],areaISO)
        response.append(impurities_result)
                
    yield_result = areaA/areaISO
    response.insert(0, -np.log(yield_result))
    print(response,'!!!!!!!!!!!!!!!!!!!')
    return response

def response_HPLC_csv(data,YminRT,YmaxRT,IminRT,ImaxRT,minRTISO,maxRTISO,nobj):
    
    """
    ---------------------------------INPUTS--------------------------------------------
    data : HPLC data as dataframe, read from shared folder - Internal Pass.
    YminRT - Minimum value for Yield related to HPLC Residence Time - User Input.
    YmaxRT - Maximum value for Yield related to HPLC Residence Time - User Input.
    IminRT - Minimum value for Impurity1 related to HPLC Residence Time - User Input.
    ImaxRT - Maximum value for Impurity1 related to HPLC Residence Time - User Input.
    minRTISO - Minimum value for ISO related to HPLC Residence Time - User Input.
    maxRTISO - Maximum value for ISO related to HPLC Residence Time - User Input.
    nobj - No of objectives - Internal pass
    ------------------------------------------------------------------------------------
    
    --------------------------------CONDITIONS------------------------------------------
    If no of objectives > 2:
    replicate the areaB computation with relevant user input RT values
    -------------------------------------------------------------------------------------
    """
    ## Compute response - responseFunction.m
    signalobjA=1
    signalobjB=1
    signalIS=1
    
    areaA = 0
    # YminRTA = 2.0
    # YmaxRTA = 2.35
    for i in range(data.shape[0]):
        # loop that checks each RT
        if YminRT <= data[i, 1] <= YmaxRT:  # if rt is within range
            areaA += data[i, 0]
    print("AreaPurity",areaA)
                
                
    # minRTISO = 3.9
    # maxRTISO = 4.2
    areaISO = 0
    for i in range(data.shape[0]):
        # loop that checks each RT
        if minRTISO <= data[i, 1] <= maxRTISO:  # if rt is within range
            areaISO += data[i, 0]
                
    
    ## Run impurities calculation
    response = []
    for i in range(nobj-1): 
        impurities_result = utils.hplcread.impurity_response_csv(data,IminRT[i],ImaxRT[i],areaISO)
        response.append(impurities_result)
                
    print(areaA, '-----------', areaISO)
    yield_result = areaA/areaISO
    response.insert(0, -np.log(yield_result))
    return response

def monitor_folder_creation1(lhs_res,directory,nobj,YminRT,YmaxRT,IminRT,ImaxRT,minRTISO,maxRTISO,use_dataframe_last_idx=False):
    # Initial list of folders in the directory
    initial_folders = set(os.listdir(directory))
    iterations = 0
    lhs_exp = lhs_res.copy()
    
    while True:
        current_folders = set(os.listdir(directory))
        new_folders = current_folders - initial_folders
        if new_folders:
            print("New folders found")
            for folder in new_folders:
                if folder.endswith(".D"):
                    iterations+=1
                    print("HPLC Read")
                    filepath = os.path.join(directory+'/'+folder+'/Report01.xls')
                    ## Read peak area from HPLC file - DADRead.m
                    data = utils.hplcread.HPLC_data_read(filepath)
                    # print("Data",data)
                    ## Compute response for peak area - Yield & Impurity - responseFunction.m
                    response = utils.hplcread.response_HPLC(data,YminRT,YmaxRT,IminRT,ImaxRT,minRTISO,maxRTISO,nobj)
                    
                    ## Add the response to LHS data
                    cols = len(response)
                    
                    # Loop over each row in the DataFrame and add the array as new columns
                    if use_dataframe_last_idx:
                        lhs_exp.loc[lhs_exp.index[-1],'Purity'] = response[0]
                        for j in range(1,cols):
                            col_name = f'Impurity{j}'
                            lhs_exp.loc[lhs_exp.index[-1],col_name] = response[j]
                            # print(lhs_exp)
                        yield lhs_exp
                        
        
                    else:
                
                        lhs_exp.loc[iterations-1,'Purity'] = response[0]
                        for j in range(1,cols):
                            col_name = f'Impurity{j}'
                            lhs_exp.loc[iterations-1,col_name] = response[j]
    #                     print(lhs_exp)
                        yield lhs_exp
                        
                   
            initial_folders = current_folders
        else:
            print("No new folders found")
            yield None

        
#         if iterations==max_iter:
#             yield "No new folders found"
        time.sleep(10)  # Check every minute, you can adjust this interval as needed

def monitor_folder_creation1_csv(lhs_res,directory,nobj,YminRT,YmaxRT,IminRT,ImaxRT,minRTISO,maxRTISO,use_dataframe_last_idx=False):
    # Initial list of folders in the directory
    initial_files = set(os.listdir(directory))
    iterations = 0
    lhs_exp = lhs_res.copy()
    
    while True:
        current_files = set(os.listdir(directory))
        new_files = current_files - initial_files
        if new_files:
            print("New files found")
            for file in new_files:
                if file.endswith(".csv"):
                    iterations+=1
                    print("HPLC Read")
                    filepath = os.path.join(directory+'/'+file)
                    ## Read peak area from HPLC file - DADRead.m
                    data = utils.hplcread.HPLC_data_read_csv(filepath)
                    ## Compute response for peak area - Yield & Impurity - responseFunction.m
                    response = utils.hplcread.response_HPLC_csv(data,YminRT,YmaxRT,IminRT,ImaxRT,minRTISO,maxRTISO,nobj)
                    
                    ## Add the response to LHS data
                    cols = len(response)
                    
                    # Loop over each row in the DataFrame and add the array as new columns
                    if use_dataframe_last_idx:
                        lhs_exp.loc[lhs_exp.index[-1],'Purity'] = response[0]
                        for j in range(1,cols):
                            col_name = f'Impurity{j}'
                            lhs_exp.loc[lhs_exp.index[-1],col_name] = response[j]
                            # print(lhs_exp)
                        yield lhs_exp
                        
        
                    else:
                
                        lhs_exp.loc[iterations-1,'Purity'] = response[0]
                        for j in range(1,cols):
                            col_name = f'Impurity{j}'
                            lhs_exp.loc[iterations-1,col_name] = response[j]
    #                     print(lhs_exp)
                        yield lhs_exp
                        
                   
            initial_files = current_files
        else:
            print("No new files found")
            yield None

        
#         if iterations==max_iter:
#             yield "No new folders found"
        time.sleep(10)  # Check every minute, you can adjust this interval as needed
##########################lhs#######################
from summit.strategies import TSEMO,Random, SNOBFIT
import pandas as pd

def LHSDesign(nExp,domain,nobj):
    
    """
    --------------------------INPUTS---------------------------------------
    nExp : No of experiments to be generated by LHS algorithm - User Input.
    -----------------------------------------------------------------------
    
    -------------------------CONDITIONS------------------------------------
    If no of objectives is greater than 1:
        use strat_TSEMO
    else:
        use strat_SNOBFIT
    ------------------------------------------------------------------------
    """
    strat_TSEMO = TSEMO(domain,random_rate=0.00,n_spectral_points=4000)
    strat_SNOBFIT = SNOBFIT(domain)
    if nobj >1:
        lhs_exp = strat_TSEMO.suggest_experiments(nExp,use_spectral_sample=True, 
                                                pop_size=100, 
                                                iterations=100)
    else:
        lhs_exp = strat_SNOBFIT.suggest_experiments(nExp) 
    lhs_exp = lhs_exp.drop(columns={"strategy"})
    lhs_exp.columns = [col[0] for col in lhs_exp.columns]
    return lhs_exp

############################optimization############################
from summit.strategies import TSEMO,Random, SNOBFIT
import pandas as pd
from summit.utils.dataset import DataSet
import utils.optimization, utils.flowrate

def multi_tsemo(domain,lhs_exp):
    """
    ------------------------------INPUTS-------------------------------------------
    nExp : No of experiments to be suggested by TSEMO algorithm - User Input.
    lhs_exp : Initial experiments generated by LHS Design function - Internal Pass.
    -------------------------------------------------------------------------------
    """
    strat_TSEMO = TSEMO(domain,random_rate=0.00,n_spectral_points=4000)
    initial = DataSet.from_df(lhs_exp)
    # print("Data",lhs_exp)
    tsemo_exp = strat_TSEMO.suggest_experiments(1,initial,use_spectral_sample=True, 
                                            pop_size=100, 
                                            iterations=100)
    return tsemo_exp


def single_snobfit(domain,lhs_exp):
    """
    ------------------------------INPUTS-------------------------------------------
    nExp : No of experiments to be generated by LHS algorithm - User Input.
    lhs_exp : Initial experiments generated by LHS Design function - Internal Pass.
    --------------------------------------------------------------------------------
    """
    strat_SNOBFIT = SNOBFIT(domain)
    initial = DataSet.from_df(lhs_exp)
    snobfit_exp = strat_SNOBFIT.suggest_experiments(1,lhs_exp)
    return snobfit_exp

def run_optimization(domain,sor_in,nobj,npump,rscheme,lhs_exp,df_pump,df_reactor):
    # print("Optimization.py",sor_in)
    if nobj > 1:
        sor_out = utils.optimization.multi_tsemo(domain, sor_in)

    elif nobj == 1:
        sor_out = utils.optimization.single_snobfit(domain,sor_in)
    sor_out.columns = [col[0] for col in sor_out.columns]   
    sor_out = sor_out.drop(columns={"strategy"})
    sor_out = sor_out[lhs_exp.columns.tolist()]
    print(sor_out.columns,'-------------sor_out from opti.py')
    sor_in = pd.concat([sor_in,sor_out],axis=0).reset_index(drop=True)
    print(sor_in.columns,'----------sor_in from opti.py')
    print(lhs_exp.columns,'---------lhs_exp columns from opti.py')
    result = sor_in[lhs_exp.columns.tolist()]
    print(result.columns,'---------result from opti.py')
    if npump == 3 and rscheme == 1:
        result = utils.flowrate.pump3_flow_rate1(result,df_pump,df_reactor)
    elif npump == 3 and rscheme == 2:
        result = utils.flowrate.pump3_flow_rate2(result, df_pump, df_reactor)
    elif npump == 4 and rscheme == 1:
        result = utils.flowrate.pump4_flow_rate1(result, df_pump, df_reactor)
    elif npump == 4 and rscheme == 2:
        result = utils.flowrate.pump4_flow_rate2(result,df_pump,df_reactor)
    result = pd.concat([result,sor_in.iloc[:,-nobj:]],axis=1).reset_index(drop=True)
    print(result.columns,'-----------result from opti.py')

    return sor_in, result
#############################################################
    
    
    
    
