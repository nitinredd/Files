import os
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from zipfile import ZipFile
from scipy.stats import skew, kurtosis, probplot
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')

# ----------------------------------------
# Required Helper Functions (Improved)
# ----------------------------------------

def prepare_data(reference_df, test_df):
    """Validate and clean reference and test DataFrames."""
    # Ensure DataFrames are not empty
    if reference_df.empty or test_df.empty:
        raise ValueError("Reference or test DataFrame is empty.")

    # Convert first column to numeric, handling errors
    for df in [reference_df, test_df]:
        first_col = df.columns[0]
        df[first_col] = pd.to_numeric(df[first_col], errors='coerce')
        df.dropna(subset=[first_col], inplace=True)

    # Debug log for data inspection
    st.write("Reference DataFrame (first 5 rows):")
    st.write(reference_df.head())
    st.write("Test DataFrame (first 5 rows):")
    st.write(test_df.head())

    # Strip potential "time zero" rows
    if reference_df.iloc[0, 0] == 0:
        reference_df = reference_df.iloc[1:].reset_index(drop=True)
    if test_df.iloc[0, 0] == 0:
        test_df = test_df.iloc[1:].reset_index(drop=True)

    return reference_df, test_df

def conventional_f2(ref_means, test_means):
    """Calculate conventional f2"""
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    p = len(ref_means)
    if p == 0:
        return 50
    return 100 - 25 * np.log10(1 + (1/p) * sum_sq_diff)

def expected_f2(ref_df, test_df):
    """Calculate expected f2"""
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    
    # Conventional f2 component
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    
    # Variance components
    ref_var = row_variance(ref_df)
    test_var = row_variance(test_df)
    sum_var = (ref_var + test_var).sum()
    
    n = ref_df.shape[1] - 1  # Number of units per time point
    p = len(ref_means)
    
    adjustment = (1/n) * sum_var
    return 100 - 25 * np.log10(1 + (1/p) * (sum_sq_diff + adjustment))

def bias_corrected_f2(ref_df, test_df):
    """Calculate bias-corrected f2"""
    try:
        ref_means = ref_df.iloc[:, 1:].mean(axis=1)
        test_means = test_df.iloc[:, 1:].mean(axis=1)

        diff = test_means - ref_means
        sum_sq_diff = (diff ** 2).sum()

        ref_var = row_variance(ref_df)
        test_var = row_variance(test_df)
        sum_var = (ref_var + test_var).sum()

        n = ref_df.shape[1] - 1
        p = len(ref_means)

        adjustment = (1 / n) * sum_var
        right_side = sum_sq_diff + p
        
        # Handle scalar comparisons only
        if isinstance(adjustment, (int, float)) and isinstance(right_side, (int, float)):
            if adjustment >= right_side:
                return "Invalid"
        else:
            return "Invalid"
        
        adjusted_diff = sum_sq_diff - adjustment
        if adjusted_diff > 0:
            return 100 - 25 * np.log10(1 + (1 / p) * adjusted_diff)
        else:
            return "Invalid"

    except Exception as e:
        st.error(f"Error in bias_corrected_f2: {e}")
        return "Error"

def row_variance(df):
    """Calculate row-wise variance"""
    return df.iloc[:, 1:].var(axis=1, ddof=1)

def bootstrap_f2(ref_df, test_df, calc_func, n_iterations=10000):
    """Bootstrap f2 calculation (returns mean, median, skewness, kurtosis)."""
    n_ref_units = ref_df.shape[1] - 1
    n_test_units = test_df.shape[1] - 1

    original_f2 = calc_func(ref_df, test_df)
    f2_values = []
    
    for _ in range(n_iterations):
        ref_sample_idx = np.random.choice(range(1, ref_df.shape[1]), n_ref_units, replace=True)
        test_sample_idx = np.random.choice(range(1, test_df.shape[1]), n_test_units, replace=True)

        ref_sample = ref_df.iloc[:, [0] + list(ref_sample_idx)]
        test_sample = test_df.iloc[:, [0] + list(test_sample_idx)]

        f2_val = calc_func(ref_sample, test_sample)

        if isinstance(f2_val, (int, float)) and not np.isnan(f2_val):
            f2_values.append(f2_val)
    
    if not f2_values:
        return original_f2, None, None, None, None, None, None, []
    
    f2_values = np.array(f2_values)
    mean_f2 = np.mean(f2_values)
    median_f2 = np.median(f2_values)
    skewness_f2 = skew(f2_values)
    kurtosis_f2 = kurtosis(f2_values)
    lower_bound = np.percentile(f2_values, 5)
    upper_bound = np.percentile(f2_values, 95)

    return original_f2, lower_bound, upper_bound, mean_f2, median_f2, skewness_f2, kurtosis_f2, f2_values

def plot_bootstrap_distribution(f2_values, title):
    """Generate QQ plot and histogram for bootstrap f2 values."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    
    # QQ Plot
    probplot(f2_values, dist="norm", plot=axes[0])
    axes[0].set_title(f"QQ Plot: {title}")
    axes[0].grid()

    # Histogram with density
    sns.histplot(f2_values, kde=True, ax=axes[1])
    axes[1].set_title(f"Histogram: {title}")
    axes[1].set_xlabel("f2 Values")
    
    plt.tight_layout()
    return fig


# ----------------------------------------
# Batch and Single File Processing Functions (Improved)
# ----------------------------------------

def load_batch_data(folder_path):
    """Load test and reference data from multiple Excel files in a folder."""
    workbook_data = {}
    
    for file_name in os.listdir(folder_path):
        if file_name.endswith((".xlsx", ".xls")):
            file_path = os.path.join(folder_path, file_name)
            try:
                # Read all sheets and find reference/test sheets
                xl = pd.ExcelFile(file_path)
                sheet_names = xl.sheet_names
                
                # Find reference and test sheets (case-insensitive)
                ref_sheet = next((s for s in sheet_names if 'reference' in s.lower()), None)
                test_sheet = next((s for s in sheet_names if 'test' in s.lower()), None)
                
                if ref_sheet and test_sheet:
                    reference_df = pd.read_excel(file_path, sheet_name=ref_sheet)
                    test_df = pd.read_excel(file_path, sheet_name=test_sheet)
                    workbook_data[file_name] = (reference_df, test_df)
                else:
                    st.warning(f"Skipping file `{file_name}` - could not find reference/test sheets")
            except Exception as e:
                st.warning(f"Skipping file `{file_name}` due to error: {str(e)}")
    
    return workbook_data

def process_batch(workbook_data, methods, n_iterations=10000):
    """Process multiple workbooks and calculate f2 metrics."""
    all_results = []
    
    for file_name, (reference_df, test_df) in workbook_data.items():
        try:
            ref_clean, test_clean = prepare_data(reference_df.copy(), test_df.copy())
            
            results = {"File Name": file_name}
            
            # Bootstrap calculation methods
            for method in methods:
                if method.endswith("Bootstrap"):
                    calc_func = {
                        "Conventional Bootstrap": lambda r, t: conventional_f2(
                            r.iloc[:, 1:].mean(axis=1), t.iloc[:, 1:].mean(axis=1)
                        ),
                        "Expected Bootstrap": expected_f2,
                        "Bias Corrected Bootstrap": bias_corrected_f2
                    }.get(method)
                    
                    if calc_func is None:
                        continue
                    
                    orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                        ref_clean, test_clean, calc_func, n_iterations
                    )

                    # Store results
                    results[f"{method} f2"] = orig
                    results[f"{method} CI"] = f"{lower:.2f} - {upper:.2f}" if isinstance(lower, float) and isinstance(upper, float) else "N/A"
                    results[f"{method} Mean"] = mean
                    results[f"{method} Median"] = median
                    results[f"{method} Skewness"] = skewness
                    results[f"{method} Kurtosis"] = kurt
                    
                    # Generate and display plots
                    if vals:
                        fig = plot_bootstrap_distribution(vals, f"{method}: {file_name}")
                        st.pyplot(fig)
            
            all_results.append(results)
        
        except Exception as e:
            st.warning(f"Error processing file `{file_name}`: {str(e)}")
    
    return pd.DataFrame(all_results)

def process_single_file(file_buffer, methods, n_iterations=10000):
    """Process a single workbook and calculate f2 metrics."""
    workbook_data = {}

    try:
        # Read all sheets
        xl = pd.ExcelFile(file_buffer)
        sheet_names = xl.sheet_names
        
        # Find reference and test sheets
        ref_sheet = next((s for s in sheet_names if 'reference' in s.lower()), sheet_names[0])
        test_sheet = next((s for s in sheet_names if 'test' in s.lower()), sheet_names[1] if len(sheet_names) > 1 else None)
        
        if test_sheet is None:
            st.error("Could not find test sheet in the Excel file")
            return None
        
        reference_df = pd.read_excel(file_buffer, sheet_name=ref_sheet)
        test_df = pd.read_excel(file_buffer, sheet_name=test_sheet)
        workbook_data["uploaded_file.xlsx"] = (reference_df, test_df)
    except Exception as e:
        st.error(f"Error loading file: {str(e)}")
        return None

    return process_batch(workbook_data, methods, n_iterations)

def create_zip_report(report_df):
    """Create a ZIP file containing the report CSV."""
    report_file = "f2_similarities_report.csv"
    zip_file = "f2_similarities_report.zip"
    
    report_df.to_csv(report_file, index=False)
    
    with ZipFile(zip_file, "w") as zipf:
        zipf.write(report_file)
    
    # Clean up
    if os.path.exists(report_file):
        os.remove(report_file)
    
    return zip_file


# ----------------------------------------
# Streamlit App Code (Improved)
# ----------------------------------------

def main():
    st.set_page_config(page_title="Batch Similarity Analyzer", layout="wide")
    st.title("ðŸ“Š Batch Similarity Analyzer")
    st.markdown("""
    This tool calculates f2 similarity for multiple Excel workbooks at once or for a single workbook.
    You can upload a folder containing Excel files or a single Excel file for analysis.
    """)
    
    with st.sidebar:
        st.subheader("Configuration")
        upload_type = st.radio("Select upload type:", ["Folder Upload", "Single File Upload"])
        n_iterations = st.slider("Number of Bootstrap Iterations", 1000, 50000, 10000, 1000)
        st.markdown("---")
        st.info("Ensure Excel files have sheets named 'Reference' and 'Test'")
    
    options = ["Conventional Bootstrap", "Expected Bootstrap", "Bias Corrected Bootstrap"]
    selected_methods = st.multiselect("Select calculation methods:", options, default=options)

    if upload_type == "Folder Upload":
        st.subheader("Folder Upload")
        folder_path = st.text_input("Enter path to folder containing Excel files:", "")
        if st.button("Calculate and Generate Report") and selected_methods:
            if os.path.isdir(folder_path):
                with st.spinner("Processing files..."):
                    workbook_data = load_batch_data(folder_path)
                    if workbook_data:
                        report_df = process_batch(workbook_data, selected_methods, n_iterations)
                        st.subheader("ðŸ“Š Results Summary")
                        st.dataframe(report_df)
                        
                        if not report_df.empty:
                            zip_file_path = create_zip_report(report_df)
                            
                            with open(zip_file_path, "rb") as f:
                                st.download_button(
                                    label="Download Full Report (ZIP)",
                                    data=f,
                                    file_name="f2_similarities_report.zip",
                                    mime="application/zip"
                                )
                    else:
                        st.warning("No valid Excel files found in selected folder.")
            else:
                st.error("Invalid folder path. Please check the path and try again.")

    elif upload_type == "Single File Upload":
        st.subheader("Single File Upload")
        uploaded_file = st.file_uploader("Upload an Excel File:", type=["xlsx", "xls"])
        if uploaded_file is not None and selected_methods:
            if st.button("Calculate and Generate Report"):
                with st.spinner("Processing file..."):
                    report_df = process_single_file(uploaded_file, selected_methods, n_iterations)
                    if report_df is not None and not report_df.empty:
                        st.subheader("ðŸ“Š Results Summary")
                        st.dataframe(report_df)
                        
                        zip_file_path = create_zip_report(report_df)
                        
                        with open(zip_file_path, "rb") as f:
                            st.download_button(
                                label="Download Full Report (ZIP)",
                                data=f,
                                file_name="f2_similarities_report.zip",
                                mime="application/zip"
                            )
                    else:
                        st.warning("No valid results generated. Please check your file format.")

if __name__ == "__main__":
    main()
