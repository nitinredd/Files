import os
import io
import json
import uuid
import pandas as pd
import logging
import speech_recognition as sr
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Union, List, Dict, Any

from gtts import gTTS
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ─── CONFIG ──────────────────────────────────────────────────────

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure GPT configuration (fill these in)
base_url=""
api_version="2024-02-15-preview"
api_key=""
deployment_name="GPT4o"
model_name="GPT4o"

# Paths to embedded JSON files
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Prime_M+F_JSON.json",
}

# Sample‑prompt tiles (for frontend to fetch)
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X":["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# ─── SETUP LLM + EMBEDDINGS ──────────────────────────────────────

# Embedding store & cache
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment="Def_data_qa"
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url,
    temperature=0  # Reduce hallucination
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# ─── SMART CHUNKING UTILITIES ────────────────────────────────────

def flatten_json(data: Dict[str, Any], parent_key: str = '', sep: str = '.') -> Dict[str, Any]:
    """Flatten nested JSON to make it easier to chunk semantically."""
    items = []
    for k, v in data.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_json(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            if v and isinstance(v[0], dict):
                for i, item in enumerate(v):
                    items.extend(flatten_json(item, f"{new_key}[{i}]", sep=sep).items())
            else:
                items.append((new_key, str(v)))
        else:
            items.append((new_key, v))
    return dict(items)

def create_semantic_chunks(record: Dict[str, Any], record_id: str) -> List[Dict[str, Any]]:
    """
    Create semantic chunks from a JSON record while preserving context.
    Each chunk contains related fields with full metadata for accurate retrieval.
    """
    chunks = []
    
    # Strategy 1: Create a summary chunk with key identifiers
    identifier_fields = ['id', 'name', 'product', 'batch', 'formula', 'code', 'number']
    summary_parts = []
    identifiers = {}
    
    flat_record = flatten_json(record)
    
    for key, value in flat_record.items():
        key_lower = key.lower()
        if any(id_field in key_lower for id_field in identifier_fields):
            summary_parts.append(f"{key}: {value}")
            identifiers[key] = value
    
    if summary_parts:
        chunks.append({
            "content": "RECORD SUMMARY: " + " | ".join(summary_parts[:10]),
            "metadata": {
                "record_id": record_id,
                "chunk_type": "summary",
                "full_record": json.dumps(record),
                **identifiers
            }
        })
    
    # Strategy 2: Group related fields semantically
    field_groups = {
        "product_info": ['product', 'api', 'formulation', 'formula', 'ingredient', 'component'],
        "manufacturing": ['batch', 'size', 'scale', 'equipment', 'machine', 'line', 'facility'],
        "process": ['temperature', 'pressure', 'speed', 'time', 'step', 'process', 'method'],
        "quality": ['specification', 'test', 'quality', 'dissolution', 'assay', 'purity'],
        "supplier": ['manufacturer', 'supplier', 'vendor', 'source'],
        "regulatory": ['approval', 'regulatory', 'compliance', 'standard']
    }
    
    for group_name, keywords in field_groups.items():
        group_content = []
        for key, value in flat_record.items():
            if any(keyword in key.lower() for keyword in keywords):
                group_content.append(f"{key}: {value}")
        
        if group_content:
            chunks.append({
                "content": f"{group_name.upper()}: " + " | ".join(group_content),
                "metadata": {
                    "record_id": record_id,
                    "chunk_type": group_name,
                    "full_record": json.dumps(record),
                    **identifiers
                }
            })
    
    # Strategy 3: Ensure we have the full record as a fallback
    chunks.append({
        "content": f"COMPLETE RECORD: {json.dumps(record, indent=2)}",
        "metadata": {
            "record_id": record_id,
            "chunk_type": "complete",
            "full_record": json.dumps(record),
            **identifiers
        }
    })
    
    return chunks

# ─── ENHANCED PROMPT TEMPLATE ────────────────────────────────────

ENHANCED_PROMPT = PromptTemplate(
    template="""You are a precise data assistant. Use ONLY the information provided in the context below to answer the question.

Context Information:
{context}

Question: {question}

Instructions:
1. Answer ONLY based on the provided context
2. If the information is in the context, provide specific details
3. If the context contains partial information, state what you know and what's missing
4. If the answer is NOT in the context, say "I don't have that information in the provided data"
5. Never make up or infer information not explicitly stated
6. When multiple records match, provide information from all relevant records

Answer:""",
    input_variables=["context", "question"]
)

# ─── AGENT CLASSES ───────────────────────────────────────────────

class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,  # Get sources for better accuracy
            chain_type_kwargs={"prompt": ENHANCED_PROMPT}
        )
    
    def ask(self, query: str) -> tuple[str, List[Document]]:
        resp = self.chain.invoke({"query": query})
        answer = resp.get("result", "")
        sources = resp.get("source_documents", [])
        return answer, sources

class CoordinatorAgent:
    def __init__(self, children: List[ChildAgent]):
        self.children = children

    def coordinate(self, query: str) -> Union[tuple[str, List[Document]], tuple[None, None]]:
        best_answer = None
        best_sources = []
        best_confidence = 0
        
        for child in self.children:
            try:
                ans, sources = child.ask(query)
                
                # Confidence scoring
                confidence = 0
                if ans and len(ans) > 20:
                    confidence += 30
                if "don't have" not in ans.lower() and "not found" not in ans.lower():
                    confidence += 40
                if sources:
                    confidence += 30
                
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_answer = ans
                    best_sources = sources
                    
                logger.info(f"Child {child.name} confidence: {confidence}")
                
            except Exception as e:
                logger.warning(f"Child agent {child.name} error: {str(e)}")
        
        if best_confidence > 50:
            return best_answer, best_sources
        return None, None

class OversightAgent:
    def validate(self, answer: str, sources: List[Document]) -> str:
        """Validate answer quality and add source information if helpful."""
        if not answer or "don't have" in answer.lower():
            return answer
        
        # Check if answer seems complete
        if len(answer.strip()) < 10:
            return "The answer appears incomplete. Please try rephrasing your question."
        
        return answer

class LearningAgent:
    def __init__(self):
        self.logs: List[Dict] = []
    
    def log(self, q: str, a: str, sources: List[Document] = None):
        self.logs.append({
            "query": q,
            "response": a,
            "source_count": len(sources) if sources else 0
        })

class AgentManager:
    def __init__(self, agents: List[ChildAgent]):
        self.coordinator = CoordinatorAgent(agents)
        self.oversight  = OversightAgent()
        self.learning   = LearningAgent()
    
    def handle_query(self, query: str) -> str:
        raw, sources = self.coordinator.coordinate(query)
        answer = raw if raw else "I couldn't find relevant information for your query. Please try rephrasing or ask about specific fields in the dataset."
        validated = self.oversight.validate(answer, sources or [])
        self.learning.log(query, validated, sources)
        return validated

# ─── DATA LOADING & VECTORSTORE BUILD ────────────────────────────

def load_json_data_with_chunking(paths: Dict[str, str]) -> Dict[str, List[Dict[str, Any]]]:
    """Load JSON data and create semantic chunks."""
    all_chunks: Dict[str, List[Dict[str, Any]]] = {}
    
    for name, path in paths.items():
        if not path:
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            chunks = []
            if isinstance(data, list):
                for idx, item in enumerate(data):
                    record_id = f"{name}_{idx}"
                    record_chunks = create_semantic_chunks(item, record_id)
                    chunks.extend(record_chunks)
                    
                    # Log progress for large files
                    if (idx + 1) % 1000 == 0:
                        logger.info(f"[Chunking] Processed {idx + 1} records from '{name}'")
            else:
                record_chunks = create_semantic_chunks(data, f"{name}_0")
                chunks.extend(record_chunks)
            
            all_chunks[name] = chunks
            logger.info(f"[Data] Created {len(chunks)} chunks from {name}")
            
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    
    return all_chunks

def build_vectorstores(chunks_dict: Dict[str, List[Dict[str, Any]]]) -> List[ChildAgent]:
    """Build FAISS vectorstores from chunked data."""
    agents: List[ChildAgent] = []
    
    for key, chunks in chunks_dict.items():
        docs = [
            Document(
                page_content=chunk["content"],
                metadata=chunk["metadata"]
            )
            for chunk in chunks
        ]
        
        if not docs:
            continue
        
        # Create FAISS store with optimal parameters
        store = FAISS.from_documents(docs, cached_embeddings)
        
        # Retrieve more documents to ensure we get complete information
        retriever = store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 8}  # Increased from 5 to get more context
        )
        
        agents.append(ChildAgent(name=key, retriever=retriever))
        logger.info(f"[Vectorstore] Built store for '{key}' ({len(docs)} chunks)")
    
    return agents

# Initialize on startup
logger.info("[Startup] Loading and chunking data...")
CHUNKED_DATA = load_json_data_with_chunking(EMBEDDED_FILES)
logger.info("[Startup] Building vectorstores...")
AGENTS = build_vectorstores(CHUNKED_DATA)
logger.info("[Startup] Initializing agent manager...")
MANAGER = AgentManager(AGENTS)
recognizer = sr.Recognizer()
logger.info("[Startup] System ready!")

# ─── FASTAPI APP ─────────────────────────────────────────────────

app = FastAPI(title="Multi-Agent JSON Chatbot with Smart Chunking")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

# ─── ENDPOINTS ────────────────────────────────────────────────────

@app.get("/sample-tiles")
def sample_tiles():
    """Return the sample-prompt tiles and questions."""
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
def chat(req: ChatRequest):
    q = req.message.strip()
    if not q:
        raise HTTPException(400, "Empty query")
    logger.info(f"[API] /chat query='{q}'")
    ans = MANAGER.handle_query(q)
    return {"response": ans}

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    """Accepts an uploaded audio file and returns transcribed text."""
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"[API] STT error: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    """Returns an MP3 audio stream of the given text."""
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"[API] TTS error: {e}")
        raise HTTPException(500, str(e))

@app.get("/health")
def health():
    total_chunks = sum(len(chunks) for chunks in CHUNKED_DATA.values())
    return {
        "status": "ok",
        "agents": [a.name for a in AGENTS],
        "total_chunks": total_chunks
    }

@app.get("/stats")
def stats():
    """Get statistics about the loaded data."""
    stats_info = {}
    for name, chunks in CHUNKED_DATA.items():
        chunk_types = {}
        for chunk in chunks:
            ctype = chunk["metadata"].get("chunk_type", "unknown")
            chunk_types[ctype] = chunk_types.get(ctype, 0) + 1
        stats_info[name] = {
            "total_chunks": len(chunks),
            "chunk_types": chunk_types
        }
    return stats_info
