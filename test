import os
import streamlit as st
import logging
import google.auth
from vertexai.preview.generative_models import GenerativeModel, GenerationConfig, Part

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Gemini configuration exactly as per your template
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "D:/datascience-254609-genai.json"
credentials, project_id = google.auth.default()

# Initialize the transcription model (Gemini-1.5)
model = GenerativeModel("gemini-1.5-flash-002")

# Define the transcription prompt
prompt = """
Can you transcribe this interview, in the format of timecode, speaker, caption.
Use speaker A, speaker B, etc. to identify speakers.
"""

# For this example, we use a GCS URI.
# To use your own audio, first upload it to a GCS bucket and update the URI below.
audio_file_uri = "gs://cloud-samples-data/generative-ai/audio/pixel.mp3"
audio_file = Part.from_uri(audio_file_uri, mime_type="audio/mpeg")
contents = [audio_file, prompt]

st.title("Gemini Transcription App")
st.write("Click the button below to transcribe the audio using Gemini-1.5.")

if st.button("Transcribe Audio"):
    try:
        response = model.generate_content(
            contents,
            generation_config=GenerationConfig(audio_timestamp=True)
        )
        st.text_area("Transcription Output", response.text)
    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        logger.error("Transcription error", exc_info=True)
