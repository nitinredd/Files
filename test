def predictive_optimal_combinations_advanced(ref_df, test_df, regulation, 
                                             window_min, window_max, diff_threshold=None,
                                             interp_method='gpr', n_iter=100):
    """
    This function uses stratification based on predicted dissolution percentages.
    It precomputes the predicted dissolution for each valid time point (from the union
    of 3- and 5-minute intervals) and divides these predictions into strata:
         0-30%, 30-60%, 60-90%, 90-100%.
    For each candidate (over n_iter iterations), it forces the candidate to include
    at least two time points from each stratum (if available), plus the endpoints.
    Then, it computes the f2 similarity metric and checks regulatory compliance.
    """
    import random
    results = []
    
    # Create the union of valid time points
    valid_times = np.unique(np.concatenate([
        np.arange(window_min, window_max+1, 3),
        np.arange(window_min, window_max+1, 5)
    ]))
    valid_times = valid_times[(valid_times >= window_min) & (valid_times <= window_max)]
    
    # Set up the dissolution strata (in percentages)
    strata = [(0,30), (30,60), (60,90), (90,100)]
    
    # Setup interpolation
    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=10.0) + WhiteKernel()
    ref_times = ref_df.iloc[:, 0].values.astype(float)
    ref_diss = ref_df.iloc[:, 1].values.astype(float)
    test_times = test_df.iloc[:, 0].values.astype(float)
    test_diss = test_df.iloc[:, 1].values.astype(float)
    
    ref_mask = ~np.isnan(ref_times) & ~np.isnan(ref_diss)
    test_mask = ~np.isnan(test_times) & ~np.isnan(test_diss)
    
    if interp_method == 'gpr':
        def safe_gp_interpolator(x, y):
            gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)
            valid_mask = ~np.isnan(x) & ~np.isnan(y)
            X = x[valid_mask].reshape(-1, 1)
            gp.fit(X, y[valid_mask])
            return gp
        ref_model = safe_gp_interpolator(ref_times, ref_diss)
        test_model = safe_gp_interpolator(test_times, test_diss)
        def ref_interp(x):
            return ref_model.predict(np.array(x).reshape(-1, 1))
        def test_interp(x):
            return test_model.predict(np.array(x).reshape(-1, 1))
    else:
        valid_methods = ['linear', 'nearest', 'slinear', 'quadratic', 'cubic']
        interp_method = interp_method if interp_method in valid_methods else 'linear'
        ref_interp = interp1d(ref_times[ref_mask], ref_diss[ref_mask],
                              kind=interp_method, bounds_error=False, fill_value=np.nan)
        test_interp = interp1d(test_times[test_mask], test_diss[test_mask],
                               kind=interp_method, bounds_error=False, fill_value=np.nan)
    
    # Precompute predicted dissolution percentages for each valid time (using the reference curve)
    all_valid_pred = {}
    for t in valid_times:
        pred = ref_interp(np.array([t]).reshape(-1,1))
        if t == window_min:
            pred_val = 0.0
        else:
            pred_val = float(pred[0])
        all_valid_pred[t] = pred_val
    
    # Run stratified candidate generation n_iter times
    for i in range(n_iter):
        candidate = set()
        candidate.add(window_min)
        candidate.add(window_max)
        candidate_strata = {}  # to store points per stratum
        for (low, high) in strata:
            # Filter valid times that have a predicted dissolution in this stratum
            times_in_stratum = [t for t in valid_times if low <= all_valid_pred[t] < high]
            if len(times_in_stratum) >= 2:
                selected = random.sample(times_in_stratum, 2)
            else:
                selected = times_in_stratum
            candidate_strata[(low, high)] = selected
            candidate.update(selected)
        candidate = sorted(candidate)
        
        # Check diversity: each stratum should have at least 2 candidate points (if available in valid_times)
        diverse = True
        for (low, high) in strata:
            if len(candidate_strata[(low, high)]) < 2:
                diverse = False
                break
        
        # Compute predicted dissolution percentages for candidate points
        if interp_method == 'gpr':
            candidate_array = np.array(candidate).reshape(-1,1)
            ref_vals = ref_interp(candidate_array)
            test_vals = test_interp(candidate_array)
        else:
            ref_vals = ref_interp(candidate)
            test_vals = test_interp(candidate)
        
        if np.isnan(ref_vals).any() or np.isnan(test_vals).any():
            continue
        
        # Force dissolution at time window_min to be 0%
        if candidate[0] == window_min:
            ref_vals[0] = 0.0
            test_vals[0] = 0.0
        
        diff = test_vals - ref_vals
        p_val = len(candidate)
        f2 = 100 - 25 * np.log10(1 + (np.sum(diff**2)/p_val))
        
        # Regulatory compliance check (assumed external function)
        compliant, reasons = check_regulatory_compliance(
            candidate, regulation,
            dict(zip(candidate, ref_vals.flatten().tolist())),
            dict(zip(candidate, test_vals.flatten().tolist()))
        )
        
        results.append({
            'sequence': candidate,
            'f2': round(f2,2),
            'compliant': compliant,
            'reasons': reasons,
            'length': len(candidate),
            'diverse': diverse,
            'ref_vals': ref_vals.flatten().tolist(),
            'test_vals': test_vals.flatten().tolist()
        })
    
    results.sort(key=lambda x: -x['f2'])
    return results, results
#################################################33
if run_predictive.lower() == 'yes':
    # Determine candidate window
    window_min, window_max = determine_candidate_window(
        reference_mean_df,
        test_mean_df,
        step=5,
        initial_threshold=10
    )
    
    # Map regulation for predictive analysis
    regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
    selected_regulation = regulation_map.get(input1, "FDA")
    
    print(f"\nCandidate window for combination search: {window_min} to {window_max} (using stratification based on dissolution percentages)")
    
    # Run predictive analysis using the new stratified approach (e.g., 100 iterations)
    results, all_results = predictive_optimal_combinations_advanced(
        reference_mean_df,
        test_mean_df,
        regulation=selected_regulation,
        window_min=window_min,
        window_max=window_max,
        diff_threshold=None,
        interp_method='gpr',
        n_iter=100
    )
    
    # Filter unique candidates (by sequence) and convert candidate time points to Python ints
    unique_candidates_dict = {}
    for cand in results:
        seq_tuple = tuple(int(t) for t in cand['sequence'])
        if seq_tuple in unique_candidates_dict:
            if cand['f2'] > unique_candidates_dict[seq_tuple]['f2']:
                unique_candidates_dict[seq_tuple] = cand
        else:
            unique_candidates_dict[seq_tuple] = cand
    unique_candidates = list(unique_candidates_dict.values())
    
    def print_range_stats(candidates):
        ranges = ["0-30%", "30-60%", "60-90%", "90-100%"]
        stats = {r: {"total": 0, "compliant": 0} for r in ranges}
        for cand in candidates:
            rng = cand.get('diss_range', 'Unknown')
            if rng in stats:
                stats[rng]['total'] += 1
                if cand['compliant']:
                    stats[rng]['compliant'] += 1
            else:
                stats[rng] = {"total": 1, "compliant": 1 if cand['compliant'] else 0}
        print("\n=== Dissolution Range Distribution ===")
        for rng, data in stats.items():
            if data['total'] > 0:
                compliance_rate = (data['compliant']/data['total'])*100
                print(f"{rng}:")
                print(f"  Total combinations: {data['total']}")
                print(f"  Compliant combinations: {data['compliant']}")
                print(f"  Compliance rate: {compliance_rate:.1f}%")
    
    # Choose the candidate with highest f2 among those that are diverse.
    diverse_candidates = [cand for cand in unique_candidates if cand['diverse']]
    if diverse_candidates:
        overall_best = max(diverse_candidates, key=lambda x: x['f2'])
    elif unique_candidates:
        overall_best = max(unique_candidates, key=lambda x: x['f2'])
        print("\nNo candidate met the ideal dissolution diversity criteria; displaying candidate with highest f2.")
    else:
        overall_best = None
    
    if overall_best:
        overall_best['sequence'] = [int(t) for t in overall_best['sequence']]
        print("\n=== Optimal Predictive Combination ===")
        print(f"Condition: {overall_best.get('condition','N/A')}")
        print(f"Dissolution Range: {overall_best.get('diss_range','N/A')}")
        print(f"Time Points (stratified): {overall_best['sequence']}")
        print(f"Length: {len(overall_best['sequence'])}")
        print(f"Predicted f2 Score: {overall_best['f2']}")
        print(f"Diverse Combination: {overall_best.get('diverse', False)}")
        
        if overall_best['reasons']:
            print(f"Compliance Issues: {', '.join(overall_best['reasons'])}")
        else:
            print("Regulatory Compliance: Passed")
        
        print_range_stats(unique_candidates)
        
        # Plot the optimal candidate's predicted dissolution curves
        plt.figure(figsize=(12, 6))
        time_points = overall_best['sequence']
        ref_diss = interpolate_dissolution_curve(reference_mean_df, time_points, method='gpr')
        test_diss = interpolate_dissolution_curve(test_mean_df, time_points, method='gpr')
        if time_points[0] == window_min:
            ref_diss[0] = 0.0
            test_diss[0] = 0.0
        plt.plot(time_points, ref_diss, 'bo-', label='Reference')
        plt.plot(time_points, test_diss, 'r*--', label='Test')
        plt.title(f"Optimal Profile: {overall_best.get('diss_range','N/A')} Dissolution (f2 = {overall_best['f2']})")
        plt.xlabel('Time (min)')
        plt.ylabel('Dissolution (%)')
        plt.legend()
        plt.grid(True)
        plt.show()
        
        print("\nPredicted Reference Dissolution Percentages:")
        for t, d in zip(time_points, ref_diss):
            print(f"Time {t} min: {d:.2f}%")
        print("\nPredicted Test Dissolution Percentages:")
        for t, d in zip(time_points, test_diss):
            print(f"Time {t} min: {d:.2f}%")
    else:
        print("❌ No candidate sequences were generated.")
    
    print("\n=== All Unique Candidate Combinations ===")
    sorted_candidates = sorted(unique_candidates, key=lambda x: -x['f2'])
    for idx, cand in enumerate(sorted_candidates):
        seq_print = [int(t) for t in cand['sequence']]
        print(f"{idx+1:3d}. {cand.get('diss_range','N/A')} | Points: {seq_print} | Length: {len(seq_print)} | f2: {cand['f2']} | Compliant: {cand['compliant']}")
