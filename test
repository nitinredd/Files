import os
import io
import json
import uuid
import pandas as pd
import logging
import speech_recognition as sr
import time
import pickle
from pathlib import Path
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Union, List, Dict, Any

from gtts import gTTS
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ─── CONFIG ──────────────────────────────────────────────────────

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure GPT configuration (fill these in)
base_url=""
api_version="2024-02-15-preview"
api_key=""
deployment_name="GPT4o"
model_name="GPT4o"

# Paths to embedded JSON files
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Prime_M+F_JSON.json",
}

# Sample‑prompt tiles (for frontend to fetch)
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X":["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# AGGRESSIVE RATE LIMITING CONFIG
BATCH_SIZE = 1  # Process ONE document at a time
BATCH_DELAY = 2  # 2 second delay between EACH document
MAX_RETRIES = 10  # Many retries
RETRY_DELAY = 60  # Match Azure's retry delay
REQUEST_DELAY = 1  # Additional delay before each request

# PERSISTENCE CONFIG
CACHE_DIR = "vectorstore_cache"
FAISS_CACHE_FILE = os.path.join(CACHE_DIR, "faiss_store.pkl")
METADATA_CACHE_FILE = os.path.join(CACHE_DIR, "metadata.json")

# ─── SETUP LLM + EMBEDDINGS ──────────────────────────────────────

# Embedding store & cache with custom settings
file_store = LocalFileStore('langchain-embeddings')

# Create embeddings with request-level rate limiting
class RateLimitedAzureOpenAIEmbeddings(AzureOpenAIEmbeddings):
    """Custom embeddings class with built-in rate limiting."""
    
    def embed_documents(self, texts):
        """Override to add delay between embedding requests."""
        embeddings = []
        for i, text in enumerate(texts):
            logger.info(f"[Embedding] Processing document {i+1}/{len(texts)}")
            time.sleep(REQUEST_DELAY)  # Delay before each request
            try:
                embedding = super().embed_documents([text])
                embeddings.extend(embedding)
            except Exception as e:
                if "429" in str(e):
                    logger.warning(f"[Embedding] Rate limit hit, waiting {RETRY_DELAY}s...")
                    time.sleep(RETRY_DELAY)
                    # Retry once
                    embedding = super().embed_documents([text])
                    embeddings.extend(embedding)
                else:
                    raise
        return embeddings

base = RateLimitedAzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment="Def_data_qa"
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url,
    temperature=0
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# ─── SMART CHUNKING UTILITIES ────────────────────────────────────

def flatten_json(data: Dict[str, Any], parent_key: str = '', sep: str = '.') -> Dict[str, Any]:
    """Flatten nested JSON to make it easier to chunk semantically."""
    items = []
    for k, v in data.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_json(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            if v and isinstance(v[0], dict):
                for i, item in enumerate(v):
                    items.extend(flatten_json(item, f"{new_key}[{i}]", sep=sep).items())
            else:
                items.append((new_key, str(v)))
        else:
            items.append((new_key, v))
    return dict(items)

def create_semantic_chunks(record: Dict[str, Any], record_id: str) -> List[Dict[str, Any]]:
    """
    Create semantic chunks with SMART strategy to prevent hallucinations.
    - Creates 1-2 chunks per record (not 4+)
    - Each chunk is COMPLETE and SELF-CONTAINED
    - Preserves full record context to prevent information loss
    """
    chunks = []
    
    flat_record = flatten_json(record)
    
    # Extract identifiers that should be in EVERY chunk
    identifier_fields = ['id', 'name', 'product', 'batch', 'formula', 'code', 'number']
    identifiers = {}
    identifier_content = []
    
    for key, value in flat_record.items():
        key_lower = key.lower()
        if any(id_field in key_lower for id_field in identifier_fields):
            identifiers[key] = value
            identifier_content.append(f"{key}: {value}")
    
    # Build complete content string
    all_fields = [f"{k}: {v}" for k, v in flat_record.items()]
    
    # STRATEGY: Create chunks that stay under token limits but remain complete
    # Token limit is ~8191 for embeddings, ~2000 tokens = safe chunk
    # Rough estimate: 1 token ≈ 4 characters
    MAX_CHUNK_CHARS = 7000  # ~1750 tokens, leaves room for safety
    
    full_content = " | ".join(all_fields)
    
    if len(full_content) <= MAX_CHUNK_CHARS:
        # Small record: One complete chunk with ALL information
        chunks.append({
            "content": f"RECORD {record_id}: {full_content}",
            "metadata": {
                "record_id": record_id,
                "full_record": json.dumps(record),
                "chunk_part": "complete",
                **identifiers
            }
        })
    else:
        # Large record: Split into 2 overlapping chunks
        # CRITICAL: Both chunks include identifiers for context continuity
        mid_point = len(all_fields) // 2
        
        # First half with identifiers
        first_half = identifier_content + all_fields[:mid_point]
        chunk1_content = " | ".join(first_half)
        
        chunks.append({
            "content": f"RECORD {record_id} (Part 1/2): {chunk1_content}",
            "metadata": {
                "record_id": record_id,
                "full_record": json.dumps(record),
                "chunk_part": "1_of_2",
                **identifiers
            }
        })
        
        # Second half with identifiers (overlap ensures context)
        second_half = identifier_content + all_fields[mid_point:]
        chunk2_content = " | ".join(second_half)
        
        chunks.append({
            "content": f"RECORD {record_id} (Part 2/2): {chunk2_content}",
            "metadata": {
                "record_id": record_id,
                "full_record": json.dumps(record),
                "chunk_part": "2_of_2",
                **identifiers
            }
        })
    
    return chunks

# ─── ENHANCED PROMPT TEMPLATE ────────────────────────────────────

ENHANCED_PROMPT = PromptTemplate(
    template="""You are a precise data assistant. Use ONLY the information provided in the context below to answer the question.

Context Information:
{context}

Question: {question}

CRITICAL INSTRUCTIONS:
1. Answer ONLY based on the provided context above
2. If the information is clearly stated in the context, provide specific details with exact values
3. If the context contains partial information, state exactly what you know and what's missing
4. If the answer is NOT in the context, you MUST say "I don't have that information in the provided data"
5. NEVER make up, infer, or assume information not explicitly stated in the context
6. When multiple records match, provide information from all relevant records
7. Do not use phrases like "based on my knowledge" - only use "based on the provided data"

Answer:""",
    input_variables=["context", "question"]
)

# ─── AGENT CLASSES ───────────────────────────────────────────────

class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": ENHANCED_PROMPT}
        )
    
    def ask(self, query: str) -> tuple[str, List[Document]]:
        resp = self.chain.invoke({"query": query})
        answer = resp.get("result", "")
        sources = resp.get("source_documents", [])
        
        # ANTI-HALLUCINATION: Reconstruct full records from metadata
        # If we retrieved partial chunks, get the complete record
        enhanced_sources = []
        seen_records = set()
        
        for doc in sources:
            record_id = doc.metadata.get("record_id")
            if record_id and record_id not in seen_records:
                seen_records.add(record_id)
                # Get full record from metadata
                full_record = doc.metadata.get("full_record")
                if full_record:
                    enhanced_sources.append(Document(
                        page_content=f"COMPLETE RECORD: {full_record}",
                        metadata=doc.metadata
                    ))
                else:
                    enhanced_sources.append(doc)
            elif not record_id:
                enhanced_sources.append(doc)
        
        return answer, enhanced_sources

class CoordinatorAgent:
    def __init__(self, children: List[ChildAgent]):
        self.children = children

    def coordinate(self, query: str) -> Union[tuple[str, List[Document]], tuple[None, None]]:
        best_answer = None
        best_sources = []
        best_confidence = 0
        
        for child in self.children:
            try:
                ans, sources = child.ask(query)
                
                confidence = 0
                if ans and len(ans) > 20:
                    confidence += 30
                if "don't have" not in ans.lower() and "not found" not in ans.lower():
                    confidence += 40
                if sources:
                    confidence += 30
                
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_answer = ans
                    best_sources = sources
                    
                logger.info(f"Child {child.name} confidence: {confidence}")
                
            except Exception as e:
                logger.warning(f"Child agent {child.name} error: {str(e)}")
        
        if best_confidence > 50:
            return best_answer, best_sources
        return None, None

class OversightAgent:
    def validate(self, answer: str, sources: List[Document]) -> str:
        """
        Validate answer quality and check for hallucinations.
        """
        if not answer or "don't have" in answer.lower():
            return answer
        if len(answer.strip()) < 10:
            return "The answer appears incomplete. Please try rephrasing your question."
        
        # ANTI-HALLUCINATION CHECK: Verify key facts appear in sources
        # Extract specific values from answer (numbers, names, etc.)
        import re
        
        # Extract quoted values, numbers, and specific terms from answer
        answer_facts = set()
        
        # Get numbers (like batch sizes, temperatures, etc.)
        numbers = re.findall(r'\b\d+(?:\.\d+)?\b', answer)
        answer_facts.update(numbers)
        
        # Get capitalized words (likely names, products, etc.)
        words = re.findall(r'\b[A-Z][a-zA-Z0-9]+\b', answer)
        answer_facts.update(words)
        
        # Check if main facts appear in source documents
        if sources and answer_facts:
            source_text = " ".join([doc.page_content for doc in sources])
            
            # Count how many answer facts appear in sources
            facts_found = sum(1 for fact in answer_facts if fact in source_text)
            confidence_ratio = facts_found / len(answer_facts) if answer_facts else 0
            
            # If less than 50% of specific facts are in sources, it might be hallucinated
            if confidence_ratio < 0.5 and len(answer_facts) > 3:
                logger.warning(f"Low confidence answer detected: {confidence_ratio:.2%} facts verified")
                return "I found some information, but I'm not fully confident in the answer. Please verify or rephrase your question for better accuracy."
        
        return answer

class LearningAgent:
    def __init__(self):
        self.logs: List[Dict] = []
    
    def log(self, q: str, a: str, sources: List[Document] = None):
        self.logs.append({
            "query": q,
            "response": a,
            "source_count": len(sources) if sources else 0
        })

class AgentManager:
    def __init__(self, agents: List[ChildAgent]):
        self.coordinator = CoordinatorAgent(agents)
        self.oversight  = OversightAgent()
        self.learning   = LearningAgent()
    
    def handle_query(self, query: str) -> str:
        raw, sources = self.coordinator.coordinate(query)
        answer = raw if raw else "I couldn't find relevant information for your query. Please try rephrasing or ask about specific fields in the dataset."
        validated = self.oversight.validate(answer, sources or [])
        self.learning.log(query, validated, sources)
        return validated

# ─── PERSISTENT CACHE UTILITIES ──────────────────────────────────

def ensure_cache_dir():
    """Create cache directory if it doesn't exist."""
    Path(CACHE_DIR).mkdir(exist_ok=True)

def save_vectorstore_cache(store, metadata):
    """Save FAISS vectorstore and metadata to disk."""
    ensure_cache_dir()
    try:
        # Save FAISS index
        store.save_local(CACHE_DIR)
        logger.info(f"[Cache] Saved FAISS vectorstore to {CACHE_DIR}")
        
        # Save metadata
        with open(METADATA_CACHE_FILE, 'w') as f:
            json.dump(metadata, f)
        logger.info(f"[Cache] Saved metadata to {METADATA_CACHE_FILE}")
        return True
    except Exception as e:
        logger.error(f"[Cache] Failed to save cache: {e}")
        return False

def load_vectorstore_cache(embeddings):
    """Load FAISS vectorstore and metadata from disk."""
    try:
        if not os.path.exists(CACHE_DIR):
            return None, None
        
        # Check if FAISS files exist
        index_file = os.path.join(CACHE_DIR, "index.faiss")
        if not os.path.exists(index_file):
            return None, None
        
        # Load FAISS index
        store = FAISS.load_local(CACHE_DIR, embeddings, allow_dangerous_deserialization=True)
        logger.info(f"[Cache] Loaded FAISS vectorstore from {CACHE_DIR}")
        
        # Load metadata
        if os.path.exists(METADATA_CACHE_FILE):
            with open(METADATA_CACHE_FILE, 'r') as f:
                metadata = json.load(f)
            logger.info(f"[Cache] Loaded metadata from {METADATA_CACHE_FILE}")
        else:
            metadata = {}
        
        return store, metadata
    except Exception as e:
        logger.error(f"[Cache] Failed to load cache: {e}")
        return None, None

# ─── DATA LOADING & VECTORSTORE BUILD ────────────────────────────

def load_json_data_with_chunking(paths: Dict[str, str]) -> Dict[str, List[Dict[str, Any]]]:
    """Load JSON data and create semantic chunks."""
    all_chunks: Dict[str, List[Dict[str, Any]]] = {}
    
    for name, path in paths.items():
        if not path:
            continue
        try:
            logger.info(f"[Loading] Reading file: {name}")
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            chunks = []
            if isinstance(data, list):
                total_records = len(data)
                logger.info(f"[Chunking] Processing {total_records} records from '{name}'")
                
                for idx, item in enumerate(data):
                    record_id = f"{name}_{idx}"
                    record_chunks = create_semantic_chunks(item, record_id)
                    chunks.extend(record_chunks)
                    
                    if (idx + 1) % 500 == 0:
                        logger.info(f"[Chunking] Processed {idx + 1}/{total_records} records")
            else:
                record_chunks = create_semantic_chunks(data, f"{name}_0")
                chunks.extend(record_chunks)
            
            all_chunks[name] = chunks
            logger.info(f"[Data] Created {len(chunks)} chunks from {name}")
            
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    
    return all_chunks

def build_vectorstores_with_robust_rate_limiting(chunks_dict: Dict[str, List[Dict[str, Any]]]) -> List[ChildAgent]:
    """
    Build FAISS vectorstores with ROBUST rate limiting and caching.
    Uses persistent cache to avoid re-embedding on restarts.
    """
    agents: List[ChildAgent] = []
    
    for key, chunks in chunks_dict.items():
        logger.info(f"[Vectorstore] Building store for '{key}' with {len(chunks)} chunks")
        
        # Check if we have a cached version
        cached_store, cached_metadata = load_vectorstore_cache(cached_embeddings)
        
        if cached_store and cached_metadata.get('source') == key and cached_metadata.get('chunk_count') == len(chunks):
            logger.info(f"[Cache] Using cached vectorstore for '{key}'")
            retriever = cached_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 10}
            )
            agents.append(ChildAgent(name=key, retriever=retriever))
            continue
        
        # No cache, build from scratch
        logger.info(f"[Vectorstore] No cache found. Building new vectorstore...")
        
        docs = [
            Document(
                page_content=chunk["content"],
                metadata=chunk["metadata"]
            )
            for chunk in chunks
        ]
        
        if not docs:
            continue
        
        # Process in VERY SMALL batches with aggressive rate limiting
        total_batches = (len(docs) + BATCH_SIZE - 1) // BATCH_SIZE
        logger.info(f"[Vectorstore] Processing {len(docs)} docs ONE AT A TIME with {BATCH_DELAY}s delays")
        logger.info(f"[Vectorstore] Estimated time: ~{(len(docs) * (BATCH_DELAY + REQUEST_DELAY)) / 60:.1f} minutes")
        
        store = None
        successful_count = 0
        
        for batch_idx in range(0, len(docs), BATCH_SIZE):
            batch_docs = docs[batch_idx:batch_idx + BATCH_SIZE]
            batch_num = (batch_idx // BATCH_SIZE) + 1
            
            retry_count = 0
            batch_success = False
            
            while retry_count < MAX_RETRIES and not batch_success:
                try:
                    logger.info(f"[Vectorstore] Processing doc {batch_num}/{total_batches}")
                    
                    # Extra safety: wait before processing
                    time.sleep(REQUEST_DELAY)
                    
                    if store is None:
                        # Create initial store
                        store = FAISS.from_documents(batch_docs, cached_embeddings)
                    else:
                        # Add to existing store
                        store.add_documents(batch_docs)
                    
                    successful_count += len(batch_docs)
                    logger.info(f"[Vectorstore] ✓ Doc {batch_num}/{total_batches} completed (Total: {successful_count}/{len(docs)})")
                    batch_success = True
                    
                    # Save progress every 5 documents
                    if successful_count % 5 == 0 and store:
                        save_vectorstore_cache(store, {
                            'source': key, 
                            'chunk_count': len(chunks), 
                            'partial': True,
                            'processed': successful_count
                        })
                        logger.info(f"[Cache] ✓ Saved checkpoint at {successful_count}/{len(docs)} docs")
                    
                except Exception as e:
                    error_str = str(e).lower()
                    retry_count += 1
                    
                    if "429" in error_str or "rate" in error_str or "quota" in error_str:
                        wait_time = RETRY_DELAY
                        logger.warning(f"[Vectorstore] ⚠ Rate limit hit on doc {batch_num}. Retry {retry_count}/{MAX_RETRIES} after {wait_time}s")
                        time.sleep(wait_time)
                    elif "token" in error_str or "limit" in error_str:
                        logger.error(f"[Vectorstore] ✗ Token limit error on doc {batch_num}: {e}")
                        logger.error(f"[Vectorstore] Doc content length: {len(batch_docs[0].page_content)} chars")
                        # Skip this document
                        logger.warning(f"[Vectorstore] Skipping problematic document {batch_num}")
                        break
                    else:
                        logger.error(f"[Vectorstore] ✗ Unexpected error on doc {batch_num}: {e}")
                        raise
            
            if retry_count >= MAX_RETRIES and not batch_success:
                logger.error(f"[Vectorstore] ✗ Failed doc {batch_num} after {MAX_RETRIES} retries")
                logger.error(f"[Vectorstore] Consider increasing BATCH_DELAY or checking Azure quota")
                raise Exception(f"Rate limit exceeded after {MAX_RETRIES} retries on doc {batch_num}")
            
            # Delay between documents
            if batch_idx + BATCH_SIZE < len(docs):
                logger.info(f"[Vectorstore] Waiting {BATCH_DELAY}s before next document...")
                time.sleep(BATCH_DELAY)
        
        if store:
            # Save final completed cache
            save_vectorstore_cache(store, {'source': key, 'chunk_count': len(chunks), 'partial': False})
            
            # Create retriever
            retriever = store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 10}
            )
            
            agents.append(ChildAgent(name=key, retriever=retriever))
            logger.info(f"[Vectorstore] ✓ Completed store for '{key}' with {len(docs)} total docs")
    
    return agents

# ─── INITIALIZE ON STARTUP ───────────────────────────────────────

logger.info("=" * 70)
logger.info("[Startup] Initializing Multi-Agent Chatbot with Robust Rate Limiting")
logger.info("=" * 70)

try:
    logger.info("[Startup] Step 1/3: Loading and chunking data...")
    CHUNKED_DATA = load_json_data_with_chunking(EMBEDDED_FILES)
    
    logger.info("[Startup] Step 2/3: Building vectorstores (this may take a while)...")
    logger.info("[Startup] Ultra-conservative rate limiting: 1 doc at a time, 2s delay")
    logger.info("[Startup] This ensures ZERO rate limit errors but will be slow")
    AGENTS = build_vectorstores_with_robust_rate_limiting(CHUNKED_DATA)
    
    logger.info("[Startup] Step 3/3: Initializing agent manager...")
    MANAGER = AgentManager(AGENTS)
    recognizer = sr.Recognizer()
    
    logger.info("=" * 70)
    logger.info("[Startup] ✓ System ready!")
    logger.info("=" * 70)
except Exception as e:
    logger.error("=" * 70)
    logger.error(f"[Startup] ✗ FATAL ERROR: {e}")
    logger.error("=" * 70)
    raise

# ─── FASTAPI APP ─────────────────────────────────────────────────

app = FastAPI(title="Multi-Agent JSON Chatbot - Production Ready")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

# ─── ENDPOINTS ────────────────────────────────────────────────────

@app.get("/sample-tiles")
def sample_tiles():
    """Return the sample-prompt tiles and questions."""
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
def chat(req: ChatRequest):
    q = req.message.strip()
    if not q:
        raise HTTPException(400, "Empty query")
    logger.info(f"[API] /chat query='{q}'")
    ans = MANAGER.handle_query(q)
    return {"response": ans}

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    """Accepts an uploaded audio file and returns transcribed text."""
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"[API] STT error: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    """Returns an MP3 audio stream of the given text."""
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"[API] TTS error: {e}")
        raise HTTPException(500, str(e))

@app.get("/health")
def health():
    total_chunks = sum(len(chunks) for chunks in CHUNKED_DATA.values())
    return {
        "status": "ok",
        "agents": [a.name for a in AGENTS],
        "total_chunks": total_chunks,
        "cache_enabled": True
    }

@app.get("/stats")
def stats():
    """Get statistics about the loaded data."""
    stats_info = {}
    for name, chunks in CHUNKED_DATA.items():
        chunk_types = {}
        for chunk in chunks:
            ctype = chunk["metadata"].get("chunk_part", "unknown")
            chunk_types[ctype] = chunk_types.get(ctype, 0) + 1
        stats_info[name] = {
            "total_chunks": len(chunks),
            "chunk_types": chunk_types
        }
    return stats_info

@app.post("/clear-cache")
def clear_cache():
    """Clear the vectorstore cache (use if data changes)."""
    try:
        import shutil
        if os.path.exists(CACHE_DIR):
            shutil.rmtree(CACHE_DIR)
            return {"status": "cache cleared", "message": "Please restart the server"}
        return {"status": "no cache found"}
    except Exception as e:
        raise HTTPException(500, str(e))
