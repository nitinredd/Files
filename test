import os
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from zipfile import ZipFile
from scipy.stats import skew, kurtosis, probplot


# ----------------------------------------
# Required Helper Functions
# ----------------------------------------

def prepare_data(reference_df, test_df):
    """Validate and clean reference and test DataFrames."""
    # Ensure DataFrames are not empty
    if reference_df.empty or test_df.empty:
        raise ValueError("Reference or test DataFrame is empty.")

    # Debug log for data inspection
    st.write("Reference DataFrame (first 5 rows):")
    st.write(reference_df.head())
    st.write("Test DataFrame (first 5 rows):")
    st.write(test_df.head())

    # Validate that the first column contains numeric or valid scalar values
    if not np.isscalar(reference_df.iloc[0, 0]):
        st.error(f"Invalid value in reference DataFrame first column: {reference_df.iloc[0, 0]}")
        raise ValueError("Reference DataFrame contains non-scalar values in the time column.")

    if not np.isscalar(test_df.iloc[0, 0]):
        st.error(f"Invalid value in test DataFrame first column: {test_df.iloc[0, 0]}")
        raise ValueError("Test DataFrame contains non-scalar values in the time column.")

    # Strip potential "time zero" rows
    if reference_df.iloc[0, 0] == 0 or str(reference_df.iloc[0, 0]).strip() in ['0', '0.0']:
        reference_df = reference_df.iloc[1:].reset_index(drop=True)

    if test_df.iloc[0, 0] == 0 or str(test_df.iloc[0, 0]).strip() in ['0', '0.0']:
        test_df = test_df.iloc[1:].reset_index(drop=True)

    return reference_df, test_df

def conventional_f2(ref_means, test_means):
    """Calculate conventional f2"""
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    p = len(ref_means)
    return 50 if p == 0 else 100 - 25 * np.log10(1 + (1/p) * sum_sq_diff)

def expected_f2(ref_df, test_df):
    """Calculate expected f2"""
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    
    # Conventional f2 component
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    
    # Variance components
    ref_var = row_variance(ref_df)
    test_var = row_variance(test_df)
    sum_var = (ref_var + test_var).sum()
    
    n = ref_df.shape[1] - 1  # Number of units per time point
    p = len(ref_means)
    
    adjustment = (1/n) * sum_var
    return 100 - 25 * np.log10(1 + (1/p) * (sum_sq_diff + adjustment))

def bias_corrected_f2(ref_df, test_df):
    """Calculate bias-corrected f2"""
    try:
        ref_means = ref_df.iloc[:, 1:].mean(axis=1)
        test_means = test_df.iloc[:, 1:].mean(axis=1)

        diff = test_means - ref_means
        sum_sq_diff = (diff ** 2).sum()

        ref_var = row_variance(ref_df)
        test_var = row_variance(test_df)
        sum_var = (ref_var + test_var).sum()

        n = ref_df.shape[1] - 1
        p = len(ref_means)

        adjustment = (1 / n) * sum_var
        right_side = sum_sq_diff + p
        
        if adjustment >= right_side:
            return "Invalid"
        
        adjusted_diff = sum_sq_diff - adjustment
        if adjusted_diff > 0:
            return 100 - 25 * np.log10(1 + (1 / p) * adjusted_diff)
        else:
            return "Invalid"

    except Exception:
        return "Error"

def row_variance(df):
    """Calculate row-wise variance"""
    return df.iloc[:, 1:].var(axis=1, ddof=1)

def bootstrap_f2(ref_df, test_df, calc_func, n_iterations=10000):
    """Bootstrap f2 calculation (returns mean, median, skewness, kurtosis)."""
    n_ref_units = ref_df.shape[1] - 1
    n_test_units = test_df.shape[1] - 1

    original_f2 = calc_func(ref_df, test_df)
    f2_values = []
    
    for _ in range(n_iterations):
        ref_sample_idx = np.random.choice(range(1, ref_df.shape[1]), n_ref_units, replace=True)
        test_sample_idx = np.random.choice(range(1, test_df.shape[1]), n_test_units, replace=True)

        ref_sample = ref_df.iloc[:, [0] + list(ref_sample_idx)]
        test_sample = test_df.iloc[:, [0] + list(test_sample_idx)]

        f2_val = calc_func(ref_sample, test_sample)

        if f2_val is not None and isinstance(f2_val, (int, float)):
            f2_values.append(f2_val)
    
    if not f2_values:
        return original_f2, None, None, None, None, None, None, []
    
    f2_values = np.array(f2_values)
    mean_f2 = np.mean(f2_values)
    median_f2 = np.median(f2_values)
    skewness_f2 = skew(f2_values)
    kurtosis_f2 = kurtosis(f2_values)
    lower_bound = np.percentile(f2_values, 5)
    upper_bound = np.percentile(f2_values, 95)

    return original_f2, lower_bound, upper_bound, mean_f2, median_f2, skewness_f2, kurtosis_f2, f2_values

def plot_bootstrap_distribution(f2_values, title):
    """Generate QQ plot and histogram for bootstrap f2 values."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    
    # QQ Plot
    probplot(f2_values, dist="norm", plot=axes[0])
    axes[0].set_title(f"QQ Plot: {title}")
    axes[0].grid()

    # Histogram with density
    sns.histplot(f2_values, kde=True, ax=axes[1])
    axes[1].set_title(f"Histogram: {title}")
    axes[1].set_xlabel("f2 Values")
    
    return fig


# ----------------------------------------
# Batch and Single File Processing Functions
# ----------------------------------------

def load_batch_data(folder_path):
    """Load test and reference data from multiple Excel files in a folder."""
    workbook_data = {}
    
    for file_name in os.listdir(folder_path):
        if file_name.endswith(".xlsx"):
            file_path = os.path.join(folder_path, file_name)
            try:
                reference_df = pd.read_excel(file_path, sheet_name=0)
                test_df = pd.read_excel(file_path, sheet_name=1)
                workbook_data[file_name] = (reference_df, test_df)
            except Exception as e:
                st.warning(f"Skipping file `{file_name}` due to error: {e}")
    
    return workbook_data

def process_batch(workbook_data, methods, n_iterations=10000):
    """Process multiple workbooks and calculate f2 metrics."""
    all_results = []
    
    for file_name, (reference_df, test_df) in workbook_data.items():
        try:
            ref_clean, test_clean = prepare_data(reference_df, test_df)
            
            results = {"File Name": file_name}
            
            # Bootstrap calculation methods
            for method in methods:
                if method.endswith("Bootstrap"):
                    calc_func = {
                        "Conventional Bootstrap": lambda r, t: conventional_f2(
                            r.iloc[:, 1:].mean(axis=1), t.iloc[:, 1:].mean(axis=1)
                        ),
                        "Expected Bootstrap": expected_f2,
                        "Bias Corrected Bootstrap": lambda r, t: bias_corrected_f2(r, t)
                    }.get(method)
                    
                    orig, lower, upper, mean, median, skewness, kurt, vals = bootstrap_f2(
                        ref_clean, test_clean, calc_func, n_iterations
                    )

                    # Store results
                    results[f"{method} f2"] = orig
                    results[f"{method} CI"] = f"{lower:.2f} - {upper:.2f}" if lower and upper else None
                    results[f"{method} Mean"] = mean
                    results[f"{method} Median"] = median
                    results[f"{method} Skewness"] = skewness
                    results[f"{method} Kurtosis"] = kurt
                    
                    # Generate and display plots
                    if vals:
                        fig = plot_bootstrap_distribution(vals, f"{method}: {file_name}")
                        st.pyplot(fig)
            
            all_results.append(results)
        
        except Exception as e:
            st.warning(f"Error processing file `{file_name}`: {e}")
    
    return pd.DataFrame(all_results)

def process_single_file(file_buffer, methods, n_iterations=10000):
    """Process a single workbook and calculate f2 metrics."""
    workbook_data = {}

    try:
        reference_df = pd.read_excel(file_buffer, sheet_name=0)
        test_df = pd.read_excel(file_buffer, sheet_name=1)
        workbook_data["uploaded_file.xlsx"] = (reference_df, test_df)
    except Exception as e:
        st.error(f"Error loading file: {e}")
        return None

    return process_batch(workbook_data, methods, n_iterations)

def create_zip_report(report_df):
    """Create a ZIP file containing the report CSV."""
    report_file = "f2_similarities_report.csv"
    zip_file = "f2_similarities_report.zip"
    
    report_df.to_csv(report_file, index=False)
    
    with ZipFile(zip_file, "w") as zipf:
        zipf.write(report_file)
    
    return zip_file


# ----------------------------------------
# Streamlit App Code
# ----------------------------------------

def main():
    st.set_page_config(page_title="Batch Similarity Analyzer", layout="wide")
    st.title("Batch Similarity Analyzer")
    st.markdown("""
    This tool calculates f2 similarity for multiple Excel workbooks at once or for a single workbook.
    You can upload a folder containing Excel files or a single Excel file for analysis.
    """)
    
    options = ["Conventional Bootstrap", "Expected Bootstrap", "Bias Corrected Bootstrap"]
    selected_methods = st.multiselect("Select calculation methods:", options)

    with st.sidebar:
        upload_type = st.radio("Select upload type:", ["Folder Upload", "Single File Upload"])

    if upload_type == "Folder Upload":
        folder_path = st.text_input("Enter path to folder containing Excel files:", "")
        if st.button("Calculate and Generate Report") and selected_methods:
            if os.path.isdir(folder_path):
                with st.spinner("Processing files..."):
                    workbook_data = load_batch_data(folder_path)
                    if workbook_data:
                        report_df = process_batch(workbook_data, selected_methods)
                        st.subheader("ðŸ“Š Results Preview")
                        st.dataframe(report_df.head())
                        zip_file_path = create_zip_report(report_df)
                        
                        with open(zip_file_path, "rb") as f:
                            st.download_button(
                                label="Download Report (ZIP)",
                                data=f,
                                file_name="f2_similarities_report.zip",
                                mime="application/zip"
                            )
                    else:
                        st.warning("No valid Excel files found in selected folder.")
            else:
                st.error("Invalid folder path. Please check the path and try again.")

    elif upload_type == "Single File Upload":
        uploaded_file = st.file_uploader("Upload a Single Excel File:", type=["xlsx"])
        if st.button("Calculate and Generate Report") and selected_methods:
            if uploaded_file is not None:
                with st.spinner("Processing file..."):
                    report_df = process_single_file(uploaded_file, selected_methods)
                    if report_df is not None:
                        st.subheader("ðŸ“Š Results Preview")
                        st.dataframe(report_df.head())
                        zip_file_path = create_zip_report(report_df)
                        
                        with open(zip_file_path, "rb") as f:
                            st.download_button(
                                label="Download Report (ZIP)",
                                data=f,
                                file_name="f2_similarities_report.zip",
                                mime="application/zip"
                            )
            else:
                st.warning("Please upload an Excel file.")

if __name__ == "__main__":
    main()
