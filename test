import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from scipy.stats import norm
from sklearn.utils import resample
from matplotlib.ticker import MaxNLocator
import warnings
import rpy2.robjects as robjects
from rpy2.robjects.packages import importr, data
import os

# Suppress warnings
warnings.filterwarnings('ignore')

# Initialize R packages
utils = importr('utils')
base = importr('base')
bootf2 = importr('bootf2')
read = importr('readxl')
open = importr('writexl')

# Set random seed for reproducibility
np.random.seed(306)

# Initialize global array for bootstrap
arrayboot = []

# --------------------------
# Data Processing Functions
# --------------------------
def load_data(uploaded_file):
    """Load reference and test data from uploaded Excel file"""
    reference_df = pd.read_excel(uploaded_file, sheet_name=0)
    test_df = pd.read_excel(uploaded_file, sheet_name=1)
    return reference_df, test_df

# --------------------------
# Visualization Functions
# --------------------------
def dissolution_curve(reference_df, test_df):
    """Plot dissolution curves"""
    ref_data_means = pd.DataFrame(reference_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Reference'])
    test_data_means = pd.DataFrame(test_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Test'])
    
    ref_data_means.insert(0, 'Time', reference_df.iloc[:, 0])
    test_data_means.insert(0, 'Time', test_df.iloc[:, 0])
    
    plt.figure(figsize=(12, 6))
    plt.plot(ref_data_means['Time'], ref_data_means['Mean_Reference'], label='Reference', marker='o')
    plt.plot(test_data_means['Time'], test_data_means['Mean_Test'], label='Test', marker='o', linestyle='--')
    
    plt.xlabel('Time')
    plt.ylabel('Dissolution (%)')
    plt.title('Dissolution Curves')
    plt.grid(True)
    plt.gca().set_yticks(range(0, 101, 5))
    plt.legend(loc='lower right')
    st.pyplot(plt)
    plt.close()

def dissolution_curve_interval(reference_df, test_df):
    """Plot dissolution curves with intervals"""
    ref_data_means = pd.DataFrame(reference_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Reference'])
    ref_data_max = pd.DataFrame(reference_df.iloc[:, 1:].max(axis=1), columns=['Max_Reference'])
    ref_data_min = pd.DataFrame(reference_df.iloc[:, 1:].min(axis=1), columns=['Min_Reference'])
    
    test_data_means = pd.DataFrame(test_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Test'])
    test_data_max = pd.DataFrame(test_df.iloc[:, 1:].max(axis=1), columns=['Max_Test'])
    test_data_min = pd.DataFrame(test_df.iloc[:, 1:].min(axis=1), columns=['Min_Test'])
    
    ref_data_means.insert(0, 'Time', reference_df.iloc[:, 0])
    ref_data_max.insert(0, 'Time', reference_df.iloc[:, 0])
    ref_data_min.insert(0, 'Time', reference_df.iloc[:, 0])
    
    test_data_means.insert(0, 'Time', test_df.iloc[:, 0])
    test_data_max.insert(0, 'Time', test_df.iloc[:, 0])
    test_data_min.insert(0, 'Time', test_df.iloc[:, 0])
    
    plt.figure(figsize=(12, 6))
    plt.errorbar(ref_data_means['Time'], ref_data_means['Mean_Reference'],
                 yerr=[ref_data_means['Mean_Reference'] - ref_data_min['Min_Reference'],
                       ref_data_max['Max_Reference'] - ref_data_means['Mean_Reference']],
                 fmt='o', label='Reference Mean', color='blue', linestyle='-')
    
    plt.errorbar(test_data_means['Time'], test_data_means['Mean_Test'],
                 yerr=[test_data_means['Mean_Test'] - test_data_min['Min_Test'],
                       test_data_max['Max_Test'] - test_data_means['Mean_Test']],
                 fmt='o', label='Test Mean', color='green', linestyle='--')
    
    for time, min_val, max_val in zip(ref_data_means['Time'], ref_data_min['Min_Reference'], ref_data_max['Max_Reference']):
        plt.hlines(min_val, time - 0.2, time + 0.2, colors='blue', linestyles='--', alpha=0.5)
        plt.hlines(max_val, time - 0.2, time + 0.2, colors='blue', linestyles='--', alpha=0.5)
    
    for time, min_val, max_val in zip(test_data_means['Time'], test_data_min['Min_Test'], test_data_max['Max_Test']):
        plt.hlines(min_val, time - 0.2, time + 0.2, colors='green', linestyles='--', alpha=0.5)
        plt.hlines(max_val, time - 0.2, time + 0.2, colors='green', linestyles='--', alpha=0.5)
    
    plt.xlabel('Time')
    plt.ylabel('Dissolution (%)')
    plt.title('Dissolution Curves with Intervals')
    plt.grid(True)
    plt.gca().set_yticks(range(0, 101, 5))
    plt.legend(loc='lower right')
    st.pyplot(plt)
    plt.close()

# --------------------------
# Validation Checks
# --------------------------
def check_time_points(df):
    """Check minimum time points requirement"""
    if df.iloc[0, 0] == 0 or df.iloc[0, 0] == '0':
        return len(df.iloc[:, 0].values) - 1 >= 3
    return len(df.iloc[:, 0].values) >= 3

def two_time_points(df):
    """Check if first two time points exceed 85%"""
    mean_values = df.iloc[:, 1:].mean(axis=1)
    if mean_values[0] > 85 or mean_values[1] > 85:
        return False
    return True

def min15__check(df):
    """Check if dissolution exceeds 85% within 15 minutes"""
    time_string = df.columns[0]
    mean_values = df.iloc[:, 1:].mean(axis=1)
    
    if "min" in time_string.lower() or "minutes" in time_string.lower():
        condition = df.iloc[:, 0] <= 15
        indices = df.index[condition]
        filtered_df = mean_values.loc[indices]
        condition_other_columns = filtered_df > 85
        final_indices = filtered_df.index[condition_other_columns].tolist()
        return len(final_indices) > 0
    return False

def check_cv(df):
    """Check coefficient of variation requirements"""
    if df.iloc[0, 0] == 0 or df.iloc[0, 0] == '0':
        cv_values = df.iloc[1:, 1:].std(axis=1) / df.iloc[1:, 1:].mean(axis=1) * 100
        return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()
    cv_values = df.iloc[:, 1:].std(axis=1) / df.iloc[:, 1:].mean(axis=1) * 100
    return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()

def check_same_time_points(df1, df2):
    """Check if time points are the same for both products"""
    return df1.iloc[:, 0].equals(df2.iloc[:, 0])

def check_sample_units(df):
    """Check minimum sample units requirement"""
    return df.shape[1] - 1 >= 12

# --------------------------
# f2 Calculation Functions
# --------------------------
def row_variance(df):
    """Calculate row-wise variance"""
    return df.iloc[:, 1:].var(axis=1, ddof=1)

def stand(p, sum_diff_df_sqr):
    """Calculate conventional f2"""
    return 100 - 25 * np.log10((1 + (1 / p) * sum_diff_df_sqr))

def expected(p, sum_diff_df_sqr, left_side):
    """Calculate expected f2"""
    return 100 - 25 * np.log10((1 + (1 / p) * (sum_diff_df_sqr + left_side)))

def BiasCor(p, sum_diff_df_sqr, left_side):
    """Calculate bias-corrected f2"""
    Right_side = sum_diff_df_sqr + p
    if left_side >= Right_side:
        return 100 - 25 * np.log10((1 + (1 / p) * (sum_diff_df_sqr - left_side)))
    return "Bias Corrected f2 cannot be calculated"

def f2s(reference_df, test_df):
    """Calculate all f2 metrics"""
    ref_data_means = reference_df.iloc[:, 1:].mean(axis=1)
    test_data_means = test_df.iloc[:, 1:].mean(axis=1)
    
    diff_df = test_data_means - ref_data_means
    diff_df_sqr = diff_df ** 2
    sum_diff_df_sqr = diff_df_sqr.sum()
    
    ref_data_var = row_variance(reference_df)
    test_data_var = row_variance(test_df)
    
    addition_df = test_data_var + ref_data_var
    sum_addition_df = addition_df.sum()
    
    n_r, n_c = reference_df.shape
    n = n_c - 1
    p = len(reference_df)
    
    left_side = (1 / n) * sum_addition_df
    
    results = {
        "Conventional f2": stand(p, sum_diff_df_sqr),
        "Expected f2": expected(p, sum_diff_df_sqr, left_side),
        "Bias Corrected f2": BiasCor(p, sum_diff_df_sqr, left_side)
    }
    return results

# --------------------------
# Bootstrap Functions
# --------------------------
def cal_f2(resampled_test, resampled_reference):
    """Calculate f2 for bootstrap samples"""
    ref_data_means = resampled_reference.iloc[:, 1:].mean(axis=1)
    test_data_means = resampled_test.iloc[:, 1:].mean(axis=1)
    
    diff = test_data_means - ref_data_means
    sum_sq_diff = (diff ** 2).sum()
    
    p = len(resampled_reference)
    f2 = 100 - 25 * np.log10(1 + (1 / p) * sum_sq_diff)
    
    arrayboot.append(f2)
    return f2

def bca(test_data, ref_data, alpha=0.1):
    """Bias-Corrected and Accelerated Bootstrap"""
    arrayboot.clear()
    original_f2 = cal_f2(test_data, ref_data)
    n_iterations = 10000
    n_size = ref_data.shape[1]
    f2_values = np.zeros(n_iterations)
    
    for i in range(n_iterations):
        indices = np.random.choice(range(1, n_size), n_size - 1, replace=True)
        time_C = ref_data.iloc[:, 0]
        
        dfr = ref_data.iloc[:, indices]
        dft = test_data.iloc[:, indices]
        
        dfr.insert(0, time_C.name, time_C)
        dft.insert(0, time_C.name, time_C)
        
        f2_values[i] = cal_f2(dft, dfr)
    
    # BCa calculations
    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)
    
    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator
    
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
    
    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))
    
    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)
    
    return {
        "CI Lower": lower_percentile,
        "CI Upper": upper_percentile,
        "Original f2": original_f2,
        "Bootstrap Values": f2_values
    }

# --------------------------
# Streamlit App
# --------------------------
def main():
    st.title("Dissolution Profile Similarity Analyzer")
    st.markdown("### Upload your dissolution data")
    
    # File upload
    uploaded_file = st.file_uploader("Upload Excel file with two sheets (Reference and Test)", type=["xlsx"])
    
    if uploaded_file is not None:
        reference_df, test_df = load_data(uploaded_file)
        
        st.success("Data loaded successfully!")
        st.subheader("Data Validation Checks")
        
        # Perform checks
        col1, col2 = st.columns(2)
        with col1:
            st.write("**Reference Product Checks:**")
            st.write(f"- Minimum 3 time points: {'PASS' if check_time_points(reference_df) else 'FAIL'}")
            st.write(f"- First two points ≤85%: {'PASS' if two_time_points(reference_df) else 'FAIL'}")
            st.write(f"- >85% in 15min: {'FAIL (No f2)' if min15__check(reference_df) else 'PASS'}")
            st.write(f"- CV requirements: {'PASS' if check_cv(reference_df) else 'FAIL'}")
            st.write(f"- ≥12 units: {'PASS' if check_sample_units(reference_df) else 'FAIL'}")
        
        with col2:
            st.write("**Test Product Checks:**")
            st.write(f"- Minimum 3 time points: {'PASS' if check_time_points(test_df) else 'FAIL'}")
            st.write(f"- First two points ≤85%: {'PASS' if two_time_points(test_df) else 'FAIL'}")
            st.write(f"- >85% in 15min: {'FAIL (No f2)' if min15__check(test_df) else 'PASS'}")
            st.write(f"- CV requirements: {'PASS' if check_cv(test_df) else 'FAIL'}")
            st.write(f"- ≥12 units: {'PASS' if check_sample_units(test_df) else 'FAIL'}")
        
        st.write(f"- Same time points: {'PASS' if check_same_time_points(reference_df, test_df) else 'FAIL'}")
        
        # Show dissolution profiles
        st.subheader("Dissolution Profiles")
        dissolution_curve(reference_df, test_df)
        dissolution_curve_interval(reference_df, test_df)
        
        # Agency selection
        st.subheader("Regulatory Agency Selection")
        agency = st.selectbox("Select Regulatory Agency", 
                             ["FDA", "EMA", "China", "ASEAN", "ANVISA", "Other"])
        
        # f2 calculation options with recommendations
        st.subheader("f2 Calculation Method")
        
        # Agency recommendations
        recommendations = {
            "FDA": ["Conventional", "Conventional Bootstrap"],
            "EMA": ["Expected Bootstrap"],
            "China": ["Conventional", "Expected", "Bias Corrected", "All Bootstrap Methods"],
            "ASEAN": ["Conventional", "Expected", "Bias Corrected", "All Bootstrap Methods"],
            "ANVISA": ["Conventional", "Expected", "Bias Corrected", "All Bootstrap Methods"],
            "Other": ["All Methods"]
        }
        
        # Show recommendation
        rec = recommendations[agency]
        st.info(f"Recommended for {agency}: {', '.join(rec)}")
        
        # Calculation options
        options = [
            "Conventional f2",
            "Expected f2",
            "Bias Corrected f2",
            "Conventional Bootstrap",
            "Expected Bootstrap",
            "Bias Corrected Bootstrap"
        ]
        
        # Set default based on recommendation
        if agency == "FDA":
            default = [options[0], options[3]]
        elif agency == "EMA":
            default = [options[4]]
        else:
            default = options
        
        selected_methods = st.multiselect("Select calculation methods:", options, default=default)
        
        # Process data - remove time zero if needed
        if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
            reference_df = reference_df.iloc[1:]
        if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
            test_df = test_df.iloc[1:]
        
        # Results container
        results_container = st.container()
        
        # Perform selected calculations
        with results_container:
            st.subheader("Calculation Results")
            
            if "Conventional f2" in selected_methods:
                st.markdown("**Conventional f2 Calculation**")
                results = f2s(reference_df, test_df)
                st.write(f"Conventional f2: {results['Conventional f2']:.3f}")
            
            if "Expected f2" in selected_methods:
                st.markdown("**Expected f2 Calculation**")
                results = f2s(reference_df, test_df)
                st.write(f"Expected f2: {results['Expected f2']:.3f}")
            
            if "Bias Corrected f2" in selected_methods:
                st.markdown("**Bias Corrected f2 Calculation**")
                results = f2s(reference_df, test_df)
                st.write(f"Bias Corrected f2: {results['Bias Corrected f2']}")
            
            if "Conventional Bootstrap" in selected_methods:
                st.markdown("**Conventional Bootstrap (BCa)**")
                with st.spinner("Running bootstrap (10,000 iterations)..."):
                    bootstrap_results = bca(test_df, reference_df)
                st.write(f"Original f2: {bootstrap_results['Original f2']:.3f}")
                st.write(f"90% Confidence Interval: [{bootstrap_results['CI Lower']:.3f}, {bootstrap_results['CI Upper']:.3f}]")
                
                # Plot histogram
                plt.figure(figsize=(10, 6))
                sns.histplot(bootstrap_results['Bootstrap Values'], kde=True, bins=30)
                plt.title("Bootstrap f2 Distribution")
                plt.xlabel("f2 Value")
                plt.ylabel("Frequency")
                st.pyplot(plt)
                plt.close()
            
            if "Expected Bootstrap" in selected_methods:
                st.markdown("**Expected Bootstrap**")
                st.warning("Implementation pending - requires extended bootstrap logic")
            
            if "Bias Corrected Bootstrap" in selected_methods:
                st.markdown("**Bias Corrected Bootstrap**")
                st.warning("Implementation pending - requires extended bootstrap logic")

if __name__ == "__main__":
    main()
