import numpy as np
import time
from tqdm import tqdm
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import ConstantKernel as C, RBF, WhiteKernel
from scipy.interpolate import interp1d
import itertools
import matplotlib.pyplot as plt

def predictive_optimal_combinations_advanced(ref_df, test_df, regulation, 
                                             window_min, window_max, diff_threshold=None,
                                             interp_method='linear', num_samples=1000000):
    """Final robust version with proper array reshaping, error handling, and full dissolution range coverage"""
    results = []
    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=10.0) + WhiteKernel()
    
    # Convert to numpy arrays and handle NaNs
    ref_times = ref_df.iloc[:, 0].values.astype(float)
    ref_diss = ref_df.iloc[:, 1].values.astype(float)
    test_times = test_df.iloc[:, 0].values.astype(float)
    test_diss = test_df.iloc[:, 1].values.astype(float)

    ref_mask = ~np.isnan(ref_times) & ~np.isnan(ref_diss)
    test_mask = ~np.isnan(test_times) & ~np.isnan(test_diss)
    
    # ===== 2. Interpolation Setup =====
    if interp_method == 'gpr':
        def safe_gp_interpolator(x, y):
            gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)
            valid_mask = ~np.isnan(x) & ~np.isnan(y)
            X = x[valid_mask].reshape(-1, 1)
            gp.fit(X, y[valid_mask])
            return gp
        
        ref_model = safe_gp_interpolator(ref_times, ref_diss)
        test_model = safe_gp_interpolator(test_times, test_diss)
        
        def ref_interp(x):
            return ref_model.predict(np.array(x).reshape(-1, 1))
        
        def test_interp(x):
            return test_model.predict(np.array(x).reshape(-1, 1))
    else:
        valid_methods = ['linear', 'nearest', 'slinear', 'quadratic', 'cubic']
        interp_method = interp_method if interp_method in valid_methods else 'linear'
        ref_interp = interp1d(ref_times[ref_mask], ref_diss[ref_mask], kind=interp_method, bounds_error=False, fill_value=np.nan)
        test_interp = interp1d(test_times[test_mask], test_diss[test_mask], kind=interp_method, bounds_error=False, fill_value=np.nan)

    # ===== 3. Time Points Generation =====
    time_grid = np.arange(window_min, window_max + 1).reshape(-1, 1)
    try:
        max_ref = np.nanmax(ref_interp(time_grid))
        max_test = np.nanmax(test_interp(time_grid))
    except ValueError:
        max_ref = np.nanmax(ref_interp(time_grid.flatten()))
        max_test = np.nanmax(test_interp(time_grid.flatten()))
    max_diss = max(max_ref, max_test)

    # Always include both 3- and 5-minute intervals
    valid_times = np.unique(np.concatenate([
        np.arange(0, window_max+1, 3),
        np.arange(0, window_max+1, 5)
    ])).astype(int)
    valid_times = valid_times[(valid_times >= window_min) & (valid_times <= window_max)]

    # ===== 4. Combination Generation =====
    start_time = time.time()
    seq_lengths = np.random.randint(3, 7, num_samples)  # Change upper bound if longer sequences are desired.
    
    # Pre-allocate arrays for candidates
    all_seqs = np.full((num_samples, 6), -1, dtype=int)
    f2_scores = np.full(num_samples, np.nan)
    compliance_status = np.full(num_samples, False)
    compliance_reasons = [''] * num_samples
    
    with tqdm(total=num_samples, desc="Processing combinations") as pbar:
        for i in range(num_samples):
            try:
                available_points = valid_times[(valid_times > 0) & (valid_times < window_max)]
                if len(available_points) < (seq_lengths[i]-2):
                    raise ValueError("Not enough valid time points")
                
                mid_points = np.random.choice(available_points, size=seq_lengths[i]-2, replace=False)
                seq = np.sort(np.concatenate([[0], mid_points, [window_max]]))
                seq = seq.astype(int)
                
                seq_len = len(seq)
                if seq_len > 6:
                    raise ValueError("Sequence too long")
                all_seqs[i, :seq_len] = seq
                
                # Get dissolution values via interpolation
                if interp_method == 'gpr':
                    seq_2d = seq.reshape(-1, 1)
                    ref_vals = ref_interp(seq_2d)
                    test_vals = test_interp(seq_2d)
                else:
                    ref_vals = ref_interp(seq)
                    test_vals = test_interp(seq)
                
                # Validate values
                if np.isnan(ref_vals).any() or np.isnan(test_vals).any():
                    raise ValueError("NaN in dissolution values")
                
                # ---- NEW: Enforce coverage of dissolution ranges on reference values ----
                # Require at least 2 points in each range: 0-30%, 30-60%, 60-90%
                num_0_30 = np.sum((ref_vals >= 0) & (ref_vals <= 30))
                num_30_60 = np.sum((ref_vals > 30) & (ref_vals <= 60))
                num_60_90 = np.sum((ref_vals > 60) & (ref_vals <= 90))
                if num_0_30 < 2 or num_30_60 < 2 or num_60_90 < 2:
                    raise ValueError("Candidate does not meet required dissolution range coverage criteria")
                # ---------------------------------------------------------------------------
                
                # Calculate f2 similarity metric
                diff = test_vals - ref_vals
                p_val = len(seq)
                f2 = 100 - 25 * np.log10(1 + (np.sum(diff**2)/p_val))
                f2_scores[i] = f2
                
                # Check regulatory compliance (external function)
                compliant, reasons = check_regulatory_compliance(
                    seq, regulation,
                    dict(zip(seq, ref_vals)),
                    dict(zip(seq, test_vals))
                )
                compliance_status[i] = compliant
                compliance_reasons[i] = ', '.join(reasons)
            except (ValueError, IndexError) as e:
                f2_scores[i] = np.nan
                compliance_reasons[i] = str(e)
            pbar.update(1)

    valid_mask = ~np.isnan(f2_scores)
    results = []
    for i in np.where(valid_mask)[0]:
        seq = all_seqs[i][all_seqs[i] != -1].tolist()
        results.append({
            'sequence': seq,
            'f2': round(f2_scores[i], 2),
            'compliant': compliance_status[i],
            'reasons': compliance_reasons[i],
            'length': len(seq)
        })
    
    results.sort(key=lambda x: -x['f2'])
    
    print(f"\nProcessed {num_samples} combinations in {time.time()-start_time:.2f}s")
    return results[:500], results

# --------------------- Main Predictive Analysis Block ---------------------
if run_predictive.lower() == 'yes':
    # Determine candidate window
    window_min, window_max = determine_candidate_window(
        reference_mean_df, 
        test_mean_df,
        step=5,
        initial_threshold=10
    )
    
    # Map regulation for predictive analysis
    regulation_map = {1: "FDA", 2: "EMA", 3: "China", 4: "ASEAN", 5: "ANVISA"}
    selected_regulation = regulation_map.get(input1, "FDA")
    
    # Run predictive analysis under different conditions
    conditions = [("Diff threshold 10", 10), ("Diff threshold 20", 20), ("No diff check", None)]
    
    all_candidates = []
    overall_best = None
    overall_best_f2 = -np.inf

    print(f"\nCandidate window for combination search: {window_min} to {window_max} (using 3 & 5 minute intervals)")
    
    for cond_label, threshold in conditions:
        print(f"\nProcessing candidates for condition: {cond_label}")
        top_results, all_results = predictive_optimal_combinations_advanced(
            reference_mean_df,
            test_mean_df,
            regulation=selected_regulation,
            window_min=window_min,
            window_max=window_max,
            diff_threshold=threshold,
            num_samples=1000000
        )
        
        # Add condition label and updated dissolution range info to each candidate
        for res in top_results:
            res['condition'] = cond_label
            seq = res['sequence']
            ref_vals = interpolate_dissolution_curve(reference_mean_df, seq, method='gpr')
            test_vals = interpolate_dissolution_curve(test_mean_df, seq, method='gpr')
            max_diss = max(np.nanmax(ref_vals), np.nanmax(test_vals))
            res['diss_range'] = (
                "0-30%" if max_diss <= 30 else
                "30-60%" if max_diss <= 60 else
                "60-90%" if max_diss <= 90 else
                "90-120%" if max_diss <= 120 else
                "120%+"
            )
        all_candidates.extend(top_results)
        
        if top_results:
            current_best = max(top_results, key=lambda x: x['f2'])
            if current_best['f2'] > overall_best_f2:
                overall_best = current_best
                overall_best_f2 = current_best['f2']

    # ===== Duplicate Filtering =====
    unique_candidates_dict = {}
    for cand in all_candidates:
        seq_tuple = tuple(cand['sequence'])
        if seq_tuple in unique_candidates_dict:
            if cand['f2'] > unique_candidates_dict[seq_tuple]['f2']:
                unique_candidates_dict[seq_tuple] = cand
        else:
            unique_candidates_dict[seq_tuple] = cand
    unique_candidates = list(unique_candidates_dict.values())
    
    def print_range_stats(candidates):
        ranges = ["0-30%", "30-60%", "60-90%", "90-120%", "120%+"]
        stats = {r: {"total": 0, "compliant": 0} for r in ranges}
        for cand in candidates:
            rng = cand.get('diss_range', 'Unknown')
            if rng in stats:
                stats[rng]['total'] += 1
                if cand['compliant']:
                    stats[rng]['compliant'] += 1
            else:
                stats[rng] = {"total": 1, "compliant": 1 if cand['compliant'] else 0}
        print("\n=== Dissolution Range Distribution ===")
        for rng, data in stats.items():
            if data['total'] > 0:
                compliance_rate = (data['compliant']/data['total'])*100
                print(f"{rng}:")
                print(f"  Total combinations: {data['total']}")
                print(f"  Compliant combinations: {data['compliant']}")
                print(f"  Compliance rate: {compliance_rate:.1f}%")
    
    # ===== Display Final Results =====
    if overall_best:
        print("\n=== Optimal Predictive Combination ===")
        print(f"Condition: {overall_best.get('condition','N/A')}")
        print(f"Dissolution Range: {overall_best['diss_range']}")
        print(f"Time Points (3/5 min intervals): {overall_best['sequence']}")
        print(f"Length: {len(overall_best['sequence'])}")
        print(f"Predicted f2 Score: {overall_best['f2']}")
        
        # Compute predicted dissolution percentages for the optimal sequence
        optimal_sequence = overall_best['sequence']
        ref_diss_pred = interpolate_dissolution_curve(reference_mean_df, optimal_sequence, method='gpr')
        test_diss_pred = interpolate_dissolution_curve(test_mean_df, optimal_sequence, method='gpr')
        
        # Clip predicted values between 0 and 100
        ref_diss_pred = np.clip(ref_diss_pred, 0, 100)
        test_diss_pred = np.clip(test_diss_pred, 0, 100)
        if optimal_sequence[0] == 0:
            ref_diss_pred[0] = 0.0
            test_diss_pred[0] = 0.0
        
        print("\nPredicted Reference Dissolution Percentages:")
        for t, d in zip(optimal_sequence, ref_diss_pred):
            print(f"Time {t} min: {d:.2f}%")
        print("\nPredicted Test Dissolution Percentages:")
        for t, d in zip(optimal_sequence, test_diss_pred):
            print(f"Time {t} min: {d:.2f}%")
        
        if overall_best['reasons']:
            print(f"\nCompliance Issues: {', '.join(overall_best['reasons'])}")
        else:
            print("\nRegulatory Compliance: Passed")
        
        print_range_stats(unique_candidates)
        
        # Plot the results
        plt.figure(figsize=(12, 6))
        plt.plot(optimal_sequence, ref_diss_pred, 'bo-', label='Reference')
        plt.plot(optimal_sequence, test_diss_pred, 'r*--', label='Test')
        plt.title(f"Optimal Profile: {overall_best['diss_range']} Dissolution (f2 = {overall_best['f2']})")
        plt.xlabel('Time (min)')
        plt.ylabel('Dissolution (%)')
        plt.legend()
        plt.grid(True)
        plt.show()
    else:
        print("❌ No valid combinations found across all dissolution ranges")
    
    print("\n=== All Unique Candidate Combinations ===")
    sorted_candidates = sorted(unique_candidates, key=lambda x: -x['f2'])
    for idx, cand in enumerate(sorted_candidates):
        print(f"{idx+1:3d}. {cand['diss_range']} | Points: {cand['sequence']} | Length: {len(cand['sequence'])} | f2: {cand['f2']} | Compliant: {cand['compliant']}")
