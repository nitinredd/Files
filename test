#SCPCB - Intelligent Autonomous Multi-Agent System with Dynamic Reasoning
import os
import io
import json
import uuid
import pandas as pd
import logging
import speech_recognition as sr
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Union, List, Dict, Any, Optional, Tuple
import tiktoken
from dataclasses import dataclass, field
from enum import Enum
import re
from collections import defaultdict, deque
import asyncio
from datetime import datetime
import hashlib

from gtts import gTTS
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.schema import HumanMessage, SystemMessage

# ─── CONFIG ──────────────────────────────────────────────────────

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure GPT configuration (keep as-is)
base_url=""
api_version="2024-02-15-preview"
api_key=""
deployment_name="GPT4o"
model_name="GPT4o"

# Paths to embedded JSON files
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Prime_M+F_JSON.json",
    # 'data_2': r"C:\Users\p00095189\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Formulas.json",
    # additional files can be specified dynamically, and can be left None
}

# Sample‑prompt tiles (for frontend to fetch)
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X":["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# ─── SETUP LLM + EMBEDDINGS ──────────────────────────────────────

# Embedding store & cache
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment="Def_data_qa"
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# Initialize tokenizer
try:
    tokenizer = tiktoken.encoding_for_model("gpt-4")
except:
    tokenizer = tiktoken.get_encoding("cl100k_base")

# ─── INTELLIGENT AGENT FRAMEWORK ────────────────────────────────

@dataclass
class ReasoningContext:
    """Context for multi-step reasoning"""
    query: str
    reasoning_chain: List[Dict[str, Any]] = field(default_factory=list)
    discovered_facts: Dict[str, Any] = field(default_factory=dict)
    confidence_scores: Dict[str, float] = field(default_factory=dict)
    exploration_depth: int = 0
    max_depth: int = 3

@dataclass
class AgentCapability:
    """Defines what an agent can do"""
    specialization: str
    data_domains: List[str]
    reasoning_types: List[str]
    confidence_threshold: float = 0.7

@dataclass
class AgentMemory:
    """Long-term and short-term memory for agents"""
    episodic_memory: deque = field(default_factory=lambda: deque(maxlen=100))
    semantic_memory: Dict[str, Any] = field(default_factory=dict)
    learned_patterns: Dict[str, List[str]] = field(default_factory=dict)
    interaction_history: List[Dict] = field(default_factory=list)

class ReasoningType(Enum):
    ANALYTICAL = "analytical"      # Break down complex queries
    SYNTHESIS = "synthesis"        # Combine information from multiple sources
    INFERENCE = "inference"        # Make logical deductions
    EXPLORATION = "exploration"    # Discover related information
    VALIDATION = "validation"      # Cross-check information accuracy

class IntelligentChildAgent:
    """
    Autonomous agent with reasoning capabilities and dynamic learning
    """
    
    def __init__(self, name: str, data_chunks: List[Document], capabilities: AgentCapability):
        self.name = name
        self.capabilities = capabilities
        self.memory = AgentMemory()
        
        # Build vectorstore for this agent's data domain
        self.vectorstore = FAISS.from_documents(data_chunks, cached_embeddings)
        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 5})
        
        # Create specialized chain with dynamic prompting
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=self.retriever,
            return_source_documents=True
        )
        
        # Initialize reasoning patterns
        self._initialize_reasoning_patterns()
        
        logger.info(f"Initialized agent {name} with specialization: {capabilities.specialization}")
    
    def _initialize_reasoning_patterns(self):
        """Initialize domain-specific reasoning patterns"""
        domain_patterns = {
            'pharmaceutical': [
                "When analyzing formulations, consider API-excipient interactions",
                "Process parameters often have interdependencies",
                "Regulatory requirements may influence technical decisions"
            ],
            'manufacturing': [
                "Equipment specifications determine process capabilities",
                "Scale-up considerations affect process parameters",
                "Quality control measures are integrated with process steps"
            ],
            'analytical': [
                "Method validation requires multiple parameters",
                "Analytical results should be correlated with process data",
                "Specifications often have acceptance criteria ranges"
            ]
        }
        
        for domain in self.capabilities.data_domains:
            if domain in domain_patterns:
                self.memory.learned_patterns[domain] = domain_patterns[domain]
    
    async def reason_and_respond(self, context: ReasoningContext) -> Dict[str, Any]:
        """
        Autonomous reasoning with multi-step analysis
        """
        reasoning_result = {
            "agent": self.name,
            "reasoning_type": None,
            "confidence": 0.0,
            "answer": "",
            "reasoning_steps": [],
            "discovered_context": {},
            "suggested_followups": []
        }
        
        try:
            # Step 1: Analyze query complexity and determine reasoning approach
            reasoning_approach = await self._analyze_query_complexity(context.query)
            reasoning_result["reasoning_type"] = reasoning_approach.value
            
            # Step 2: Execute reasoning based on approach
            if reasoning_approach == ReasoningType.ANALYTICAL:
                result = await self._analytical_reasoning(context)
            elif reasoning_approach == ReasoningType.SYNTHESIS:
                result = await self._synthesis_reasoning(context)
            elif reasoning_approach == ReasoningType.INFERENCE:
                result = await self._inference_reasoning(context)
            elif reasoning_approach == ReasoningType.EXPLORATION:
                result = await self._exploration_reasoning(context)
            else:  # VALIDATION
                result = await self._validation_reasoning(context)
            
            reasoning_result.update(result)
            
            # Step 3: Learn from this interaction
            await self._update_memory(context, reasoning_result)
            
            # Step 4: Suggest intelligent follow-ups
            reasoning_result["suggested_followups"] = await self._generate_followups(context, reasoning_result)
            
            return reasoning_result
            
        except Exception as e:
            logger.error(f"Reasoning error in agent {self.name}: {e}")
            return {
                **reasoning_result,
                "answer": f"I encountered an issue while reasoning about your query. Let me try a different approach.",
                "confidence": 0.1
            }
    
    async def _analyze_query_complexity(self, query: str) -> ReasoningType:
        """Intelligently determine the best reasoning approach"""
        
        analysis_prompt = f"""
        Analyze this query and determine the best reasoning approach:
        
        Query: "{query}"
        
        My specialization: {self.capabilities.specialization}
        My data domains: {', '.join(self.capabilities.data_domains)}
        
        Reasoning approaches:
        - ANALYTICAL: Break down complex multi-part questions
        - SYNTHESIS: Combine information from multiple sources
        - INFERENCE: Make logical deductions from available data
        - EXPLORATION: Discover related information and connections
        - VALIDATION: Cross-check and verify information accuracy
        
        Respond with just the reasoning type and brief justification:
        """
        
        try:
            response = await asyncio.to_thread(
                chat_model.invoke,
                [SystemMessage(content=analysis_prompt)]
            )
            
            content = response.content.lower()
            
            if "analytical" in content:
                return ReasoningType.ANALYTICAL
            elif "synthesis" in content:
                return ReasoningType.SYNTHESIS
            elif "inference" in content:
                return ReasoningType.INFERENCE
            elif "exploration" in content:
                return ReasoningType.EXPLORATION
            else:
                return ReasoningType.VALIDATION
                
        except Exception:
            # Fallback to pattern-based analysis
            query_lower = query.lower()
            if any(word in query_lower for word in ['what', 'how', 'why', 'explain']):
                return ReasoningType.ANALYTICAL
            elif any(word in query_lower for word in ['compare', 'relate', 'combine']):
                return ReasoningType.SYNTHESIS
            elif any(word in query_lower for word in ['predict', 'expect', 'likely']):
                return ReasoningType.INFERENCE
            else:
                return ReasoningType.EXPLORATION
    
    async def _analytical_reasoning(self, context: ReasoningContext) -> Dict[str, Any]:
        """Break down complex queries into components"""
        
        # Retrieve relevant documents
        docs = await asyncio.to_thread(self.retriever.get_relevant_documents, context.query)
        
        # Construct analytical prompt
        analytical_prompt = f"""
        As an expert in {self.capabilities.specialization}, analyze this query step by step:
        
        Query: "{context.query}"
        
        Available context from my domain:
        {self._format_documents(docs)}
        
        Please provide:
        1. Step-by-step analysis
        2. Direct answer based on the context
        3. Confidence level (0-1)
        4. Any limitations or assumptions
        
        Remember: Base your analysis on the provided context, but use your expertise to provide insights.
        """
        
        try:
            response = await asyncio.to_thread(
                chat_model.invoke,
                [SystemMessage(content=analytical_prompt)]
            )
            
            return {
                "answer": response.content,
                "confidence": self._extract_confidence(response.content),
                "reasoning_steps": self._extract_reasoning_steps(response.content),
                "source_documents": [doc.page_content[:200] + "..." for doc in docs[:3]]
            }
        except Exception as e:
            return {
                "answer": "I couldn't complete the analytical reasoning due to a technical issue.",
                "confidence": 0.1,
                "reasoning_steps": [f"Error: {str(e)}"],
                "source_documents": []
            }
    
    async def _synthesis_reasoning(self, context: ReasoningContext) -> Dict[str, Any]:
        """Combine information from multiple sources"""
        
        # Get diverse documents
        docs = await asyncio.to_thread(self.retriever.get_relevant_documents, context.query)
        
        # Look for complementary information
        related_queries = self._generate_related_queries(context.query)
        additional_context = []
        
        for related_query in related_queries[:2]:  # Limit to avoid token explosion
            related_docs = await asyncio.to_thread(self.retriever.get_relevant_documents, related_query)
            additional_context.extend(related_docs[:2])
        
        synthesis_prompt = f"""
        Synthesize information to answer this query comprehensively:
        
        Primary Query: "{context.query}"
        
        Primary Context:
        {self._format_documents(docs)}
        
        Additional Related Context:
        {self._format_documents(additional_context)}
        
        Please:
        1. Identify connections between different pieces of information
        2. Provide a comprehensive synthesized answer
        3. Note any conflicting information
        4. Suggest areas for further investigation
        """
        
        try:
            response = await asyncio.to_thread(
                chat_model.invoke,
                [SystemMessage(content=synthesis_prompt)]
            )
            
            return {
                "answer": response.content,
                "confidence": self._extract_confidence(response.content),
                "reasoning_steps": ["Gathered primary context", "Explored related information", "Synthesized comprehensive response"],
                "discovered_context": {"related_queries": related_queries},
                "source_documents": [doc.page_content[:200] + "..." for doc in (docs + additional_context)[:5]]
            }
        except Exception as e:
            return {
                "answer": "I couldn't complete the synthesis due to a technical issue.",
                "confidence": 0.1,
                "reasoning_steps": [f"Error during synthesis: {str(e)}"],
                "discovered_context": {},
                "source_documents": []
            }
    
    async def _inference_reasoning(self, context: ReasoningContext) -> Dict[str, Any]:
        """Make logical deductions"""
        
        docs = await asyncio.to_thread(self.retriever.get_relevant_documents, context.query)
        
        # Check memory for relevant patterns
        relevant_patterns = []
        for domain, patterns in self.memory.learned_patterns.items():
            if domain in self.capabilities.data_domains:
                relevant_patterns.extend(patterns)
        
        inference_prompt = f"""
        Make logical inferences to answer this query:
        
        Query: "{context.query}"
        
        Available Facts:
        {self._format_documents(docs)}
        
        Relevant Domain Knowledge Patterns:
        {chr(10).join('- ' + pattern for pattern in relevant_patterns)}
        
        Please:
        1. State the known facts
        2. Apply logical reasoning
        3. Draw reasonable inferences
        4. Clearly distinguish between facts and inferences
        5. Provide confidence levels for inferences
        """
        
        try:
            response = await asyncio.to_thread(
                chat_model.invoke,
                [SystemMessage(content=inference_prompt)]
            )
            
            return {
                "answer": response.content,
                "confidence": self._extract_confidence(response.content),
                "reasoning_steps": ["Analyzed known facts", "Applied domain patterns", "Drew logical inferences"],
                "discovered_context": {"applied_patterns": relevant_patterns[:3]},
                "source_documents": [doc.page_content[:200] + "..." for doc in docs[:3]]
            }
        except Exception as e:
            return {
                "answer": "I couldn't complete the inference reasoning.",
                "confidence": 0.1,
                "reasoning_steps": [f"Error during inference: {str(e)}"],
                "discovered_context": {},
                "source_documents": []
            }
    
    async def _exploration_reasoning(self, context: ReasoningContext) -> Dict[str, Any]:
        """Discover related information and connections"""
        
        # Start with initial query
        docs = await asyncio.to_thread(self.retriever.get_relevant_documents, context.query)
        
        # Explore related concepts
        exploration_queries = self._generate_exploration_queries(context.query)
        explored_data = {}
        
        for exp_query in exploration_queries[:3]:  # Limit exploration depth
            exp_docs = await asyncio.to_thread(self.retriever.get_relevant_documents, exp_query)
            explored_data[exp_query] = exp_docs[:2]
        
        exploration_prompt = f"""
        Explore and discover information related to this query:
        
        Original Query: "{context.query}"
        
        Primary Information:
        {self._format_documents(docs)}
        
        Explored Related Areas:
        {self._format_exploration_data(explored_data)}
        
        Please:
        1. Answer the original query
        2. Highlight interesting connections you discovered
        3. Suggest related areas the user might want to explore
        4. Provide a confidence assessment
        """
        
        try:
            response = await asyncio.to_thread(
                chat_model.invoke,
                [SystemMessage(content=exploration_prompt)]
            )
            
            return {
                "answer": response.content,
                "confidence": self._extract_confidence(response.content),
                "reasoning_steps": ["Explored primary information", "Investigated related areas", "Identified connections"],
                "discovered_context": {"exploration_queries": exploration_queries, "connections_found": list(explored_data.keys())},
                "source_documents": [doc.page_content[:200] + "..." for doc in docs[:3]]
            }
        except Exception as e:
            return {
                "answer": "I couldn't complete the exploration reasoning.",
                "confidence": 0.1,
                "reasoning_steps": [f"Error during exploration: {str(e)}"],
                "discovered_context": {},
                "source_documents": []
            }
    
    async def _validation_reasoning(self, context: ReasoningContext) -> Dict[str, Any]:
        """Cross-check and validate information"""
        
        docs = await asyncio.to_thread(self.retriever.get_relevant_documents, context.query)
        
        # Look for validation sources
        validation_queries = [
            f"verify {context.query}",
            f"confirm {context.query}",
            f"cross-reference {context.query}"
        ]
        
        validation_data = []
        for val_query in validation_queries[:2]:
            val_docs = await asyncio.to_thread(self.retriever.get_relevant_documents, val_query)
            validation_data.extend(val_docs[:2])
        
        validation_prompt = f"""
        Validate and cross-check information for this query:
        
        Query: "{context.query}"
        
        Primary Sources:
        {self._format_documents(docs)}
        
        Validation Sources:
        {self._format_documents(validation_data)}
        
        Please:
        1. Provide the best answer based on available data
        2. Cross-reference information between sources
        3. Note any inconsistencies or gaps
        4. Provide a reliability assessment
        5. Suggest ways to improve answer confidence
        """
        
        try:
            response = await asyncio.to_thread(
                chat_model.invoke,
                [SystemMessage(content=validation_prompt)]
            )
            
            return {
                "answer": response.content,
                "confidence": self._extract_confidence(response.content),
                "reasoning_steps": ["Gathered primary sources", "Sought validation data", "Cross-referenced information"],
                "discovered_context": {"validation_attempted": True, "sources_compared": len(docs) + len(validation_data)},
                "source_documents": [doc.page_content[:200] + "..." for doc in (docs + validation_data)[:5]]
            }
        except Exception as e:
            return {
                "answer": "I couldn't complete the validation reasoning.",
                "confidence": 0.1,
                "reasoning_steps": [f"Error during validation: {str(e)}"],
                "discovered_context": {},
                "source_documents": []
            }
    
    # Helper methods
    def _format_documents(self, docs: List[Document]) -> str:
        """Format documents for prompt inclusion"""
        formatted = []
        for i, doc in enumerate(docs[:5]):  # Limit to avoid token issues
            content = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
            formatted.append(f"Source {i+1}:\n{content}")
        return "\n\n".join(formatted)
    
    def _format_exploration_data(self, explored_data: Dict[str, List[Document]]) -> str:
        """Format exploration data"""
        formatted = []
        for query, docs in explored_data.items():
            formatted.append(f"Exploration: {query}")
            for doc in docs[:2]:
                content = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
                formatted.append(f"  - {content}")
        return "\n".join(formatted)
    
    def _generate_related_queries(self, original_query: str) -> List[str]:
        """Generate related queries for synthesis"""
        base_terms = re.findall(r'\w+', original_query.lower())
        related = []
        
        # Add context-aware related queries
        if any(term in original_query.lower() for term in ['api', 'active', 'ingredient']):
            related.extend(['excipients', 'formulation composition', 'drug substance'])
        if any(term in original_query.lower() for term in ['batch', 'size']):
            related.extend(['scale up', 'manufacturing capacity', 'production volume'])
        if any(term in original_query.lower() for term in ['temperature', 'temp']):
            related.extend(['process parameters', 'operating conditions'])
        
        return related[:3]
    
    def _generate_exploration_queries(self, original_query: str) -> List[str]:
        """Generate exploration queries"""
        return [
            f"related to {original_query}",
            f"associated with {original_query}",
            f"connected to {original_query}"
        ]
    
    def _extract_confidence(self, response_content: str) -> float:
        """Extract confidence score from response"""
        # Look for confidence indicators in the response
        confidence_patterns = [
            r'confidence[:\s]*(\d+(?:\.\d+)?)',
            r'certainty[:\s]*(\d+(?:\.\d+)?)',
            r'(\d+(?:\.\d+)?)%?\s*confident'
        ]
        
        for pattern in confidence_patterns:
            match = re.search(pattern, response_content.lower())
            if match:
                score = float(match.group(1))
                return score / 100 if score > 1 else score
        
        # Default confidence based on response characteristics
        if "uncertain" in response_content.lower() or "unclear" in response_content.lower():
            return 0.3
        elif "likely" in response_content.lower() or "probably" in response_content.lower():
            return 0.7
        else:
            return 0.8
    
    def _extract_reasoning_steps(self, response_content: str) -> List[str]:
        """Extract reasoning steps from response"""
        steps = []
        
        # Look for numbered lists or step indicators
        step_patterns = [
            r'(\d+\.?\s+[^.]+\.)',
            r'(Step \d+[^.]+\.)',
            r'(First[^.]+\.)',
            r'(Then[^.]+\.)',
            r'(Finally[^.]+\.)'
        ]
        
        for pattern in step_patterns:
            matches = re.findall(pattern, response_content)
            steps.extend([match.strip() for match in matches])
        
        return steps[:5]  # Limit to 5 steps
    
    async def _generate_followups(self, context: ReasoningContext, result: Dict[str, Any]) -> List[str]:
        """Generate intelligent follow-up questions"""
        followups = []
        
        # Based on reasoning type
        if result.get("reasoning_type") == "analytical":
            followups.append("Would you like me to dive deeper into any specific aspect?")
        elif result.get("reasoning_type") == "exploration":
            followups.append("Would you like me to explore any of the related areas I discovered?")
        
        # Based on discovered context
        if "related_queries" in result.get("discovered_context", {}):
            followups.append("I found some related topics - would you like to know more about them?")
        
        # Based on confidence
        if result.get("confidence", 0) < 0.6:
            followups.append("I can try to find more specific information if you can provide additional context.")
        
        return followups[:3]
    
    async def _update_memory(self, context: ReasoningContext, result: Dict[str, Any]):
        """Update agent memory with learned information"""
        # Update episodic memory
        self.memory.episodic_memory.append({
            "query": context.query,
            "reasoning_type": result.get("reasoning_type"),
            "confidence": result.get("confidence"),
            "timestamp": datetime.now().isoformat()
        })
        
        # Update semantic memory with successful patterns
        if result.get("confidence", 0) > 0.7:
            query_hash = hashlib.md5(context.query.encode()).hexdigest()[:8]
            self.memory.semantic_memory[query_hash] = {
                "query": context.query,
                "successful_approach": result.get("reasoning_type"),
                "key_concepts": re.findall(r'\w+', context.query.lower())
            }

class IntelligentCoordinator:
    """
    Autonomous coordinator that orchestrates agent collaboration
    """
    
    def __init__(self, agents: List[IntelligentChildAgent]):
        self.agents = agents
        self.collaboration_history = []
        self.performance_metrics = defaultdict(list)
    
    async def orchestrate_response(self, query: str) -> Dict[str, Any]:
        """
        Intelligently coordinate multiple agents for optimal response
        """
        context = ReasoningContext(query=query)
        
        # Phase 1: Agent Selection
        selected_agents = await self._intelligent_agent_selection(query)
        
        # Phase 2: Parallel Reasoning
        agent_tasks = []
        for agent in selected_agents:
            task = asyncio.create_task(agent.reason_and_respond(context))
            agent_tasks.append(task)
        
        agent_results = await asyncio.gather(*agent_tasks, return_exceptions=True)
        
        # Phase 3: Result Synthesis
        final_response = await self._synthesize_responses(query, agent_results, selected_agents)
        
        # Phase 4: Learning Update
        await self._update_coordination_learning(query, agent_results, final_response)
        
        return final_response
    
    async def _intelligent_agent_selection(self, query: str) -> List[IntelligentChildAgent]:
        """Select best agents for the query"""
        
        # Score each agent based on relevance
        agent_scores = []
        
        for agent in self.agents:
            score = 0
            
            # Domain relevance
            for domain in agent.capabilities.data_domains:
                if domain.lower() in query.lower():
                    score += 0.3
            
            # Specialization match
            if agent.capabilities.specialization.lower() in query.lower():
                score += 0.4
            
            # Historical performance
            recent_performance = self.performance_metrics.get(agent.name, [0.5])
            score += 0.3 * (sum(recent_performance) / len(recent_performance))
            
            agent_scores.append((agent, score))
        
        # Select top agents (at least 1, max 3)
        agent_scores.sort(key=lambda x: x[1], reverse=True)
        selected = [agent for agent, score in agent_scores[:3] if score > 0.2]
        
        if not selected:  # Fallback: select best agent
            selected = [agent_scores[0][0]]
        
        logger.info(f"Selected agents: {[agent.name for agent in selected]}")
        return selected
    
    async def _synthesize_responses(self, query: str, agent_results: List, selected_agents: List) -> Dict[str, Any]:
        """Synthesize multiple agent responses"""
        
        valid_results = []
        for i, result in enumerate(agent_results):
            if not isinstance(result, Exception) and isinstance(result, dict):
                result['agent_name'] = selected_agents[i].name
                valid_results.append(result)
        
        if not valid_results:
            return {
                "answer": "I apologize, but I encountered technical difficulties processing your query.",
                "confidence": 0.1,
                "reasoning_type": "error",
                "agent_responses": []
            }
        
        # Find best response
        best_result = max(valid_results, key=lambda x: x.get('confidence', 0))
        
        # If multiple high-confidence responses, combine them
        high_conf_results = [r for r in valid_results if r.get('confidence', 0) > 0.7]
        
        if len(high_conf_results) > 1:
            # Intelligent combination
            combined_answer = await self._combine_responses(query, high_conf_results)
            return {
                "answer": combined_answer,
                "confidence": min(1.0, sum(r.get('confidence', 0) for r in high_conf_results) / len(high_conf_results)),
                "reasoning_type": "collaborative",
                "agent_responses": high_conf_results,
                "synthesis_method": "multi_agent_collaboration"
            }
        else:
            # Single best response
            best_result['agent_responses'] = valid_results
            return best_result
    
    async def _combine_responses(self, query: str, responses: List[Dict]) -> str:
        """Intelligently combine multiple high-quality responses"""
        
        combination_prompt = f"""
        Combine these expert responses into a comprehensive answer:
        
        Query: "{query}"
        
        Expert Responses:
        {self._format_agent_responses(responses)}
        
        Please provide a cohesive, comprehensive answer that:
        1. Incorporates insights from all experts
        2. Resolves any conflicts or contradictions
        3. Provides the most complete answer possible
        4. Maintains accuracy and credibility
        """
        
        try:
            response = await asyncio.to_thread(
                chat_model.invoke,
                [SystemMessage(content=combination_prompt)]
            )
            return response.content
        except Exception as e:
            # Fallback: concatenate responses
            return "\n\n".join([f"From {r['agent_name']}: {r['answer']}" for r in responses])
    
    def _format_agent_responses(self, responses: List[Dict]) -> str:
        """Format agent responses for combination"""
        formatted = []
        for r in responses:
            formatted.append(f"Expert {r.get('agent_name', 'Unknown')} ({r.get('reasoning_type', 'N/A')}):\n{r.get('answer', 'No answer')}")
        return "\n\n".join(formatted)
    
    async def _update_coordination_learning(self, query: str, agent_results: List, final_response: Dict):
        """Update coordination learning based on results"""
        
        # Track agent performance
        for i, result in enumerate(agent_results):
            if not isinstance(result, Exception) and isinstance(result, dict):
                agent_name = getattr(self.agents[i] if i < len(self.agents) else None, 'name', 'unknown')
                confidence = result.get('confidence', 0)
                self.performance_metrics[agent_name].append(confidence)
                
                # Keep only recent performance data
                if len(self.performance_metrics[agent_name]) > 20:
                    self.performance_metrics[agent_name] = self.performance_metrics[agent_name][-20:]
        
        # Log collaboration pattern
        self.collaboration_history.append({
            "query": query,
            "agents_used": [agent.name for agent in self.agents],
            "final_confidence": final_response.get('confidence', 0),
            "synthesis_method": final_response.get('synthesis_method', 'single_agent'),
            "timestamp": datetime.now().isoformat()
        })

class AutonomousAgentManager:
    """
    High-level manager that provides autonomous decision-making
    """
    
    def __init__(self, agents: List[IntelligentChildAgent]):
        self.coordinator = IntelligentCoordinator(agents)
        self.conversation_context = deque(maxlen=50)  # Maintain conversation history
        self.user_preferences = {}  # Learn user preferences over time
        self.performance_analytics = defaultdict(list)
    
    async def handle_intelligent_query(self, query: str, user_context: Dict = None) -> Dict[str, Any]:
        """
        Handle query with full autonomous intelligence
        """
        
        # Enhance query with conversation context
        enhanced_context = self._build_enhanced_context(query, user_context)
        
        # Get coordinated response
        response = await self.coordinator.orchestrate_response(enhanced_context['enhanced_query'])
        
        # Add conversation management
        response['conversation_management'] = await self._manage_conversation_flow(query, response)
        
        # Learn from interaction
        await self._update_user_model(query, response, user_context)
        
        # Update conversation context
        self.conversation_context.append({
            'query': query,
            'response': response,
            'timestamp': datetime.now().isoformat()
        })
        
        return response
    
    def _build_enhanced_context(self, query: str, user_context: Dict = None) -> Dict[str, Any]:
        """Build enhanced context from conversation history"""
        
        enhanced_query = query
        context_insights = []
        
        # Add conversation context if relevant
        if self.conversation_context:
            recent_topics = []
            for conv in list(self.conversation_context)[-3:]:  # Last 3 interactions
                recent_topics.extend(re.findall(r'\w+', conv['query'].lower()))
            
            # If current query references recent topics, enhance it
            current_terms = set(re.findall(r'\w+', query.lower()))
            overlapping_terms = current_terms.intersection(set(recent_topics))
            
            if overlapping_terms and any(word in query.lower() for word in ['it', 'that', 'this', 'they']):
                enhanced_query = f"{query} (Context: recent discussion about {', '.join(list(overlapping_terms)[:3])})"
                context_insights.append("Added conversation context")
        
        # Add user preferences
        if user_context:
            for pref_key, pref_value in user_context.items():
                if pref_key in self.user_preferences:
                    context_insights.append(f"Applied user preference: {pref_key}")
        
        return {
            'enhanced_query': enhanced_query,
            'original_query': query,
            'context_insights': context_insights,
            'conversation_depth': len(self.conversation_context)
        }
    
    async def _manage_conversation_flow(self, query: str, response: Dict[str, Any]) -> Dict[str, Any]:
        """Manage natural conversation flow"""
        
        flow_management = {
            'suggested_actions': [],
            'conversation_state': 'active',
            'follow_up_recommendations': [],
            'clarification_needed': False
        }
        
        # Analyze response quality and suggest improvements
        confidence = response.get('confidence', 0)
        
        if confidence < 0.5:
            flow_management['clarification_needed'] = True
            flow_management['suggested_actions'].append("ask_for_clarification")
            flow_management['follow_up_recommendations'].append(
                "I'd like to provide a more accurate answer. Could you provide more specific details about what you're looking for?"
            )
        
        elif confidence > 0.8:
            # High confidence - suggest related explorations
            flow_management['suggested_actions'].append("suggest_exploration")
            
            if 'suggested_followups' in response:
                flow_management['follow_up_recommendations'].extend(response['suggested_followups'])
            else:
                flow_management['follow_up_recommendations'].append(
                    "I'm confident in this answer. Would you like me to explore any related aspects?"
                )
        
        # Detect conversation patterns
        if len(self.conversation_context) > 3:
            recent_queries = [conv['query'] for conv in list(self.conversation_context)[-3:]]
            if all('how' in q.lower() for q in recent_queries):
                flow_management['follow_up_recommendations'].append(
                    "I notice you're asking several 'how' questions. Would you like a comprehensive process overview?"
                )
        
        return flow_management
    
    async def _update_user_model(self, query: str, response: Dict[str, Any], user_context: Dict = None):
        """Learn and adapt to user preferences"""
        
        # Extract query patterns
        query_type = response.get('reasoning_type', 'unknown')
        confidence = response.get('confidence', 0)
        
        # Update performance analytics
        self.performance_analytics['query_types'].append(query_type)
        self.performance_analytics['confidence_scores'].append(confidence)
        
        # Learn user preferences from successful interactions
        if confidence > 0.8:
            query_terms = re.findall(r'\w+', query.lower())
            
            # Update preferred query patterns
            if 'preferred_topics' not in self.user_preferences:
                self.user_preferences['preferred_topics'] = defaultdict(int)
            
            for term in query_terms:
                if len(term) > 3:  # Skip short words
                    self.user_preferences['preferred_topics'][term] += 1
            
            # Update preferred reasoning types
            if 'preferred_reasoning' not in self.user_preferences:
                self.user_preferences['preferred_reasoning'] = defaultdict(int)
            
            self.user_preferences['preferred_reasoning'][query_type] += 1
    
    def get_analytics(self) -> Dict[str, Any]:
        """Get system analytics and insights"""
        
        if not self.performance_analytics['confidence_scores']:
            return {"message": "No analytics data available yet"}
        
        return {
            "conversation_stats": {
                "total_interactions": len(self.conversation_context),
                "average_confidence": sum(self.performance_analytics['confidence_scores']) / len(self.performance_analytics['confidence_scores']),
                "high_confidence_rate": sum(1 for score in self.performance_analytics['confidence_scores'] if score > 0.8) / len(self.performance_analytics['confidence_scores'])
            },
            "reasoning_distribution": dict(defaultdict(int, {k: self.performance_analytics['query_types'].count(k) for k in set(self.performance_analytics['query_types'])})),
            "user_preferences": dict(self.user_preferences) if self.user_preferences else {},
            "agent_performance": dict(self.coordinator.performance_metrics)
        }

# ─── SMART DATA LOADING WITH CONTEXT MANAGEMENT ─────────────────

def smart_chunk_json_with_context(data: Any, chunk_size: int = 3000, preserve_structure: bool = True) -> List[Document]:
    """
    Intelligently chunk JSON while preserving semantic context
    """
    documents = []
    
    def create_contextual_chunks(obj, path="", parent_context=""):
        if isinstance(obj, dict):
            # For dictionaries, create chunks that preserve object boundaries
            current_chunk = {}
            current_size = 0
            
            for key, value in obj.items():
                current_path = f"{path}.{key}" if path else key
                full_context = f"{parent_context} > {key}" if parent_context else key
                
                # Estimate size
                item_str = json.dumps({key: value}, ensure_ascii=False, separators=(',', ':'))
                item_tokens = len(tokenizer.encode(item_str))
                
                # If adding this item would exceed chunk size, finalize current chunk
                if current_size + item_tokens > chunk_size and current_chunk:
                    chunk_content = json.dumps(current_chunk, ensure_ascii=False, indent=2)
                    documents.append(Document(
                        page_content=chunk_content,
                        metadata={
                            "path": path,
                            "context": parent_context,
                            "chunk_type": "dict_chunk",
                            "token_count": current_size
                        }
                    ))
                    current_chunk = {}
                    current_size = 0
                
                # Handle large individual items
                if item_tokens > chunk_size:
                    if isinstance(value, (dict, list)):
                        # Recursively chunk large nested structures
                        create_contextual_chunks(value, current_path, full_context)
                    else:
                        # For large primitive values, create a dedicated chunk
                        documents.append(Document(
                            page_content=json.dumps({key: value}, ensure_ascii=False, indent=2),
                            metadata={
                                "path": current_path,
                                "context": full_context,
                                "chunk_type": "large_item",
                                "token_count": item_tokens
                            }
                        ))
                else:
                    # Add to current chunk
                    current_chunk[key] = value
                    current_size += item_tokens
            
            # Finalize remaining chunk
            if current_chunk:
                chunk_content = json.dumps(current_chunk, ensure_ascii=False, indent=2)
                documents.append(Document(
                    page_content=chunk_content,
                    metadata={
                        "path": path,
                        "context": parent_context,
                        "chunk_type": "dict_chunk",
                        "token_count": current_size
                    }
                ))
        
        elif isinstance(obj, list):
            # For lists, group items intelligently
            current_chunk = []
            current_size = 0
            
            for i, item in enumerate(obj):
                item_str = json.dumps(item, ensure_ascii=False, separators=(',', ':'))
                item_tokens = len(tokenizer.encode(item_str))
                
                if current_size + item_tokens > chunk_size and current_chunk:
                    chunk_content = json.dumps(current_chunk, ensure_ascii=False, indent=2)
                    documents.append(Document(
                        page_content=chunk_content,
                        metadata={
                            "path": f"{path}[{i-len(current_chunk)}:{i}]",
                            "context": f"{parent_context} (array items)",
                            "chunk_type": "list_chunk",
                            "token_count": current_size
                        }
                    ))
                    current_chunk = []
                    current_size = 0
                
                if item_tokens > chunk_size and isinstance(item, (dict, list)):
                    # Recursively handle large list items
                    create_contextual_chunks(item, f"{path}[{i}]", f"{parent_context}[{i}]")
                else:
                    current_chunk.append(item)
                    current_size += item_tokens
            
            # Finalize remaining chunk
            if current_chunk:
                chunk_content = json.dumps(current_chunk, ensure_ascii=False, indent=2)
                documents.append(Document(
                    page_content=chunk_content,
                    metadata={
                        "path": f"{path}[{len(obj)-len(current_chunk)}:{len(obj)}]",
                        "context": f"{parent_context} (array items)",
                        "chunk_type": "list_chunk",
                        "token_count": current_size
                    }
                ))
    
    create_contextual_chunks(data)
    return documents

def load_and_intelligently_chunk_data(paths: Dict[str, str]) -> Dict[str, List[Document]]:
    """
    Load and intelligently chunk JSON data for optimal retrieval
    """
    chunked_datasets: Dict[str, List[Document]] = {}
    
    for name, path in paths.items():
        if not path:
            continue
        
        try:
            logger.info(f"[Data] Loading and chunking '{name}' from {path}")
            
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            # Create contextual chunks
            chunks = smart_chunk_json_with_context(data)
            
            logger.info(f"[Data] '{name}' chunked into {len(chunks)} contextual pieces")
            
            # Log chunk statistics
            token_counts = [chunk.metadata.get('token_count', 0) for chunk in chunks]
            avg_tokens = sum(token_counts) / len(token_counts) if token_counts else 0
            max_tokens = max(token_counts) if token_counts else 0
            
            logger.info(f"[Data] '{name}' chunks: avg {avg_tokens:.0f} tokens, max {max_tokens} tokens")
            
            chunked_datasets[name] = chunks
            
        except Exception as e:
            logger.error(f"[Data] Failed to load and chunk '{name}': {e}")
    
    return chunked_datasets

def build_intelligent_agents(chunked_datasets: Dict[str, List[Document]]) -> List[IntelligentChildAgent]:
    """
    Build intelligent agents with specialized capabilities
    """
    agents: List[IntelligentChildAgent] = []
    
    # Define agent capabilities based on data characteristics
    agent_configs = [
        {
            "name_suffix": "_analytical",
            "specialization": "pharmaceutical_analysis",
            "data_domains": ["pharmaceutical", "formulation", "analytical"],
            "reasoning_types": [ReasoningType.ANALYTICAL, ReasoningType.VALIDATION],
            "confidence_threshold": 0.8
        },
        {
            "name_suffix": "_synthesis",
            "specialization": "process_integration",
            "data_domains": ["manufacturing", "process", "equipment"],
            "reasoning_types": [ReasoningType.SYNTHESIS, ReasoningType.EXPLORATION],
            "confidence_threshold": 0.7
        },
        {
            "name_suffix": "_explorer",
            "specialization": "knowledge_discovery",
            "data_domains": ["general", "relationships", "patterns"],
            "reasoning_types": [ReasoningType.EXPLORATION, ReasoningType.INFERENCE],
            "confidence_threshold": 0.6
        }
    ]
    
    for dataset_name, chunks in chunked_datasets.items():
        for config in agent_configs:
            try:
                agent_name = f"{dataset_name}{config['name_suffix']}"
                
                capabilities = AgentCapability(
                    specialization=config["specialization"],
                    data_domains=config["data_domains"],
                    reasoning_types=config["reasoning_types"],
                    confidence_threshold=config["confidence_threshold"]
                )
                
                agent = IntelligentChildAgent(agent_name, chunks, capabilities)
                agents.append(agent)
                
                logger.info(f"[Agent] Created {agent_name} with {len(chunks)} chunks")
                
            except Exception as e:
                logger.error(f"[Agent] Failed to create agent for {dataset_name}: {e}")
    
    return agents

# Initialize the intelligent autonomous system
logger.info("[Startup] Loading and intelligently chunking data...")
CHUNKED_DATASETS = load_and_intelligently_chunk_data(EMBEDDED_FILES)

logger.info("[Startup] Building intelligent autonomous agents...")
INTELLIGENT_AGENTS = build_intelligent_agents(CHUNKED_DATASETS)

logger.info("[Startup] Initializing autonomous agent manager...")
AUTONOMOUS_MANAGER = AutonomousAgentManager(INTELLIGENT_AGENTS)

recognizer = sr.Recognizer()

# ─── FASTAPI APP WITH AUTONOMOUS INTELLIGENCE ───────────────────

app = FastAPI(title="Intelligent Autonomous Multi-Agent JSON Chatbot")

# CORS for your React frontend on :5173
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request models
class ChatRequest(BaseModel):
    message: str
    user_context: Optional[Dict[str, Any]] = None

# ─── AUTONOMOUS ENDPOINTS ────────────────────────────────────────

@app.get("/sample-tiles")
def sample_tiles():
    """Return the sample-prompt tiles and questions."""
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
async def intelligent_chat(req: ChatRequest):
    """
    Autonomous intelligent chat with multi-agent reasoning
    """
    q = req.message.strip()
    if not q:
        raise HTTPException(400, "Empty query")
    
    logger.info(f"[API] Autonomous chat query: '{q}'")
    
    try:
        # Use autonomous agent manager for intelligent processing
        response = await AUTONOMOUS_MANAGER.handle_intelligent_query(q, req.user_context)
        
        return {
            "response": response.get('answer', 'I encountered an issue processing your request.'),
            "confidence": response.get('confidence', 0.5),
            "reasoning_type": response.get('reasoning_type', 'unknown'),
            "agent_insights": {
                "agents_involved": [r.get('agent_name', 'unknown') for r in response.get('agent_responses', [])],
                "synthesis_method": response.get('synthesis_method', 'single_agent'),
                "reasoning_steps": response.get('reasoning_steps', [])
            },
            "conversation_management": response.get('conversation_management', {}),
            "suggested_followups": response.get('suggested_followups', [])
        }
        
    except Exception as e:
        logger.error(f"[API] Error in autonomous chat: {e}")
        return {
            "response": "I apologize, but I encountered a technical issue. Please try rephrasing your question.",
            "confidence": 0.1,
            "reasoning_type": "error",
            "agent_insights": {"error": str(e)},
            "conversation_management": {},
            "suggested_followups": []
        }

@app.get("/system-analytics")
async def get_system_analytics():
    """Get comprehensive system analytics and performance insights"""
    return AUTONOMOUS_MANAGER.get_analytics()

@app.get("/agent-status")
def get_agent_status():
    """Get current status of all agents"""
    status = {
        "total_agents": len(INTELLIGENT_AGENTS),
        "agents": [],
        "system_health": "operational"
    }
    
    for agent in INTELLIGENT_AGENTS:
        agent_info = {
            "name": agent.name,
            "specialization": agent.capabilities.specialization,
            "data_domains": agent.capabilities.data_domains,
            "reasoning_types": [rt.value for rt in agent.capabilities.reasoning_types],
            "memory_stats": {
                "episodic_entries": len(agent.memory.episodic_memory),
                "semantic_entries": len(agent.memory.semantic_memory),
                "learned_patterns": len(agent.memory.learned_patterns)
            }
        }
        status["agents"].append(agent_info)
    
    return status

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    """Accepts an uploaded audio file and returns transcribed text."""
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"[API] STT error: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    """Returns an MP3 audio stream of the given text."""
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"[API] TTS error: {e}")
        raise HTTPException(500, str(e))

@app.get("/health")
def health():
    """Comprehensive health check for autonomous system"""
    try:
        analytics = AUTONOMOUS_MANAGER.get_analytics()
        
        return {
            "status": "operational",
            "system_type": "intelligent_autonomous_multi_agent",
            "agents": {
                "total": len(INTELLIGENT_AGENTS),
                "specializations": list(set(agent.capabilities.specialization for agent in INTELLIGENT_AGENTS))
            },
            "performance": {
                "conversation_count": analytics.get("conversation_stats", {}).get("total_interactions", 0),
                "average_confidence": analytics.get("conversation_stats", {}).get("average_confidence", 0),
                "success_rate": analytics.get("conversation_stats", {}).get("high_confidence_rate", 0)
            },
            "capabilities": {
                "autonomous_reasoning": True,
                "multi_agent_collaboration": True,
                "context_awareness": True,
                "continuous_learning": True
            }
        }
    except Exception as e:
        return {
            "status": "degraded",
            "error": str(e),
            "system_type": "intelligent_autonomous_multi_agent"
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
