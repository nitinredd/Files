import os
import io
import json
import uuid
import re
import pandas as pd
import logging
import speech_recognition as sr
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Union

from gtts import gTTS
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA

# ─── CONFIG ──────────────────────────────────────────────────────

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure GPT configuration (fill these in)
base_url = ""
api_version = "2024-02-15-preview"

api_key = ""
deployment_name = "GPT4o"
model_name = "GPT4o"

# Path to the formulas Excel that will be offered for download when "formula" is mentioned.
# <-- UPDATE THIS PATH to the actual location of your Excel file on the server.
FORMULAS_EXCEL_PATH = r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Formulas.xlsx"

# Paths to embedded JSON files
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Prime_M+F_JSON.json",
    # 'data_2': r"C:\Users\p00095189\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Formulas.json",
    # additional files can be specified dynamically, and can be left None
}

# Sample-prompt tiles (for frontend to fetch)
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X":["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# ─── SETUP LLM + EMBEDDINGS ──────────────────────────────────────

# Embedding store & cache
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment="Def_data_qa"
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# ─── AGENT CLASSES ───────────────────────────────────────────────

class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=False
        )
    def ask(self, query: str) -> str:
        resp = self.chain.invoke({"query": query})
        return resp.get("result", "")

class CoordinatorAgent:
    def __init__(self, children: list[ChildAgent]):
        self.children = children

    def coordinate(self, query: str) -> Union[str, None]:
        for child in self.children:
            try:
                ans = child.ask(query)
                if ans and "not found" not in ans.lower() and "error" not in ans.lower():
                    logger.info(f"Coordinator selected {child.name}")
                    return ans
            except Exception as e:
                logger.warning(f"Child agent {child.name} error: {str(e)}")
        return None

class OversightAgent:
    def validate(self, answer: str) -> str:
        return answer

class LearningAgent:
    def __init__(self):
        self.logs: list[dict] = []
    def log(self, q: str, a: str):
        self.logs.append({"query": q, "response": a})

class AgentManager:
    def __init__(self, agents: list[ChildAgent]):
        self.coordinator = CoordinatorAgent(agents)
        self.oversight  = OversightAgent()
        self.learning   = LearningAgent()
    def handle_query(self, query: str) -> str:
        raw = self.coordinator.coordinate(query)
        answer = raw if raw else "Oops! No relevant information found."
        validated = self.oversight.validate(answer)
        self.learning.log(query, validated)
        return validated

# ─── DATA LOADING & VECTORSTORE BUILD ────────────────────────────

def load_json_data(paths: dict[str, str]) -> dict[str, pd.DataFrame]:
    dfs: dict[str, pd.DataFrame] = {}
    for name, path in paths.items():
        if not path:
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            records = []
            if isinstance(data, list):
                for item in data:
                    records.append({"content": json.dumps(item, sort_keys=True)})
            else:
                records.append({"content": json.dumps(data, sort_keys=True)})
            df = pd.DataFrame(records)
            dfs[name] = df
            logger.info(f"[Data] Loaded {len(df)} records for '{name}'")
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    return dfs

def build_vectorstores(dfs: dict[str, pd.DataFrame]) -> list[ChildAgent]:
    agents: list[ChildAgent] = []
    for key, df in dfs.items():
        docs = [
            Document(page_content=row["content"], metadata={"source": key})
            for _, row in df.iterrows()
        ]
        if not docs:
            continue
        store = FAISS.from_documents(docs, cached_embeddings)
        retriever = store.as_retriever(search_kwargs={"k": 5})
        agents.append(ChildAgent(name=key, retriever=retriever))
        logger.info(f"[Vectorstore] Built store for '{key}' ({len(docs)} docs)")
    return agents

# Initialize on startup
DATAFRAMES = load_json_data(EMBEDDED_FILES)
AGENTS     = build_vectorstores(DATAFRAMES)
MANAGER    = AgentManager(AGENTS)
recognizer = sr.Recognizer()

# ─── FASTAPI APP ─────────────────────────────────────────────────

app = FastAPI(title="Multi-Agent JSON Chatbot")

# CORS for your React frontend on :5173
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request models
class ChatRequest(BaseModel):
    message: str

# ─── ENDPOINTS ────────────────────────────────────────────────────

@app.get("/sample-tiles")
def sample_tiles():
    """Return the sample-prompt tiles and questions."""
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
def chat(req: ChatRequest, request: Request):
    q = req.message.strip()
    if not q:
        raise HTTPException(400, "Empty query")

    logger.info(f"[API] /chat query='{q}'")

    # If the user mentions the word 'formula' (word boundary), return the formulas download link
    if re.search(r'\bformula\b', q, flags=re.IGNORECASE):
        # Build an absolute URL for the frontend to download easily
        try:
            download_url = request.url_for("download_formulas")
        except Exception:
            # fallback to relative path
            download_url = "/download-formulas"

        message = (
            "Please refer to the Excel sheet for formulas. "
            "You can download it here: " + download_url
        )
        logger.info(f"[API] /chat detected 'formula' keyword; returning download URL: {download_url}")

        # Return response along with a dedicated field pointing to file URL
        return {"response": message, "formula_download": download_url}

    # Normal handling otherwise
    ans = MANAGER.handle_query(q)
    return {"response": ans}

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    """
    Accepts an uploaded audio file (wav/mp3) and returns the transcribed text.
    """
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"[API] STT error: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    """
    Returns an MP3 audio stream of the given text.
    """
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"[API] TTS error: {e}")
        raise HTTPException(500, str(e))

@app.get("/download-formulas", name="download_formulas")
def download_formulas():
    """
    Stream the formulas Excel file for download.
    """
    path = FORMULAS_EXCEL_PATH
    if not path or not os.path.exists(path):
        logger.error(f"[API] /download-formulas file not found at {path}")
        raise HTTPException(404, detail="Formulas file not found on server.")

    filename = os.path.basename(path)

    def file_iterator():
        with open(path, "rb") as f:
            while True:
                chunk = f.read(1024 * 64)
                if not chunk:
                    break
                yield chunk

    media_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    headers = {"Content-Disposition": f'attachment; filename="{filename}"'}
    return StreamingResponse(file_iterator(), media_type=media_type, headers=headers)

@app.get("/health")
def health():
    return {"status": "ok", "agents": [a.name for a in AGENTS]}
