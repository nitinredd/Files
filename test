def predictive_optimal_combinations_advanced(ref_df, test_df, regulation, 
                                             window_min, window_max, diff_threshold=None,
                                             interp_method='gpr', points_per_stratum=None):
    """
    New approach based on dissolution-based stratification with user-specified points per stratum,
    plus an extra filtering step that ensures that among interior candidate points with predicted 
    reference dissolution >80%, only one is retained (the one with the lowest value above 80%).
    The endpoint (window_max) is not forced.
    """
    import random
    import numpy as np

    # Default: require 2 points per stratum if not provided.
    if points_per_stratum is None:
        points_per_stratum = {(0, 30): 2, (30, 60): 2, (60, 90): 2, (90, 100): 2}
    
    # Step 1: Determine valid time points from the union of 3- and 5-minute intervals.
    valid_times = np.unique(np.concatenate([
        np.arange(window_min, window_max+1, 3),
        np.arange(window_min, window_max+1, 5)
    ]))
    valid_times = valid_times[(valid_times >= window_min) & (valid_times <= window_max)]
    
    # Step 2: Setup dissolution strata from points_per_stratum keys.
    strata = list(points_per_stratum.keys())
    
    # Step 3: Setup interpolation using GPR.
    from sklearn.gaussian_process import GaussianProcessRegressor
    from sklearn.gaussian_process.kernels import ConstantKernel as C, RBF, WhiteKernel
    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=10.0) + WhiteKernel()
    
    ref_times_arr = ref_df.iloc[:, 0].values.astype(float)
    ref_diss = ref_df.iloc[:, 1].values.astype(float)
    test_times_arr = test_df.iloc[:, 0].values.astype(float)
    test_diss = test_df.iloc[:, 1].values.astype(float)
    
    ref_mask = ~np.isnan(ref_times_arr) & ~np.isnan(ref_diss)
    test_mask = ~np.isnan(test_times_arr) & ~np.isnan(test_diss)
    
    if interp_method == 'gpr':
        def safe_gp_interpolator(x, y):
            gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)
            valid_mask = ~np.isnan(x) & ~np.isnan(y)
            X = x[valid_mask].reshape(-1, 1)
            gp.fit(X, y[valid_mask])
            return gp
        ref_model = safe_gp_interpolator(ref_times_arr, ref_diss)
        test_model = safe_gp_interpolator(test_times_arr, test_diss)
        def ref_interp(x):
            return ref_model.predict(np.array(x).reshape(-1, 1))
        def test_interp(x):
            return test_model.predict(np.array(x).reshape(-1, 1))
    else:
        from scipy.interpolate import interp1d
        valid_methods = ['linear', 'nearest', 'slinear', 'quadratic', 'cubic']
        interp_method = interp_method if interp_method in valid_methods else 'linear'
        ref_interp = interp1d(ref_times_arr[ref_mask], ref_diss[ref_mask],
                              kind=interp_method, bounds_error=False, fill_value=np.nan)
        test_interp = interp1d(test_times_arr[test_mask], test_diss[test_mask],
                               kind=interp_method, bounds_error=False, fill_value=np.nan)
    
    # Step 4: Precompute predicted dissolution percentages (reference) for each valid time.
    all_valid_pred = {}
    for t in valid_times:
        pred = ref_interp(np.array([t]).reshape(-1,1))
        all_valid_pred[t] = 0.0 if t == window_min else float(pred[0])
    
    # Step 5: For each stratum, randomly select the required number of points.
    candidate = set()
    for (low, high), req in points_per_stratum.items():
        times_in_stratum = [t for t in valid_times if low <= all_valid_pred[t] < high]
        if len(times_in_stratum) >= req:
            selected = random.sample(times_in_stratum, req)
        else:
            selected = times_in_stratum
        candidate.update(selected)
    
    # Do not force-add the endpoint; only add window_min.
    candidate.add(window_min)
    candidate = sorted(candidate)
    
    # --- Extra Filtering Step ---
    # Among interior points (excluding the first), allow at most one with predicted dissolution >80%.
    candidate_array = np.array(candidate).reshape(-1, 1)
    pred_vals = ref_interp(candidate_array).flatten()
    interior_idx = list(range(1, len(candidate)))  # Exclude first element (window_min)
    # Identify all interior indices with predicted reference dissolution >80%.
    indices_above_80 = [i for i in interior_idx if pred_vals[i] > 80]
    if len(indices_above_80) > 1:
        # Choose the index with the smallest predicted value above 80.
        keep_idx = min(indices_above_80, key=lambda i: pred_vals[i])
        candidate_filtered = [candidate[0]]
        for i in interior_idx:
            if i in indices_above_80 and i != keep_idx:
                continue
            candidate_filtered.append(candidate[i])
        # If filtering removes all interior points, revert.
        if len(candidate_filtered) < 2:
            candidate = candidate  # no change
        else:
            candidate = sorted(candidate_filtered)
    
    # Step 6: Compute predicted dissolution percentages for the candidate sequence.
    if interp_method == 'gpr':
        candidate_array = np.array(candidate).reshape(-1, 1)
        ref_vals = ref_interp(candidate_array)
        test_vals = test_interp(candidate_array)
    else:
        ref_vals = ref_interp(candidate)
        test_vals = test_interp(candidate)
    
    if candidate[0] == window_min:
        ref_vals[0] = 0.0
        test_vals[0] = 0.0
    
    # Step 7: Compute f2 similarity metric.
    diff = test_vals - ref_vals
    p_val = len(candidate)
    f2 = 100 - 25 * np.log10(1 + (np.sum(diff**2) / p_val))
    
    # Step 8: Diversity flag â€“ candidate is diverse if each stratum has at least the required number of points.
    diverse = True
    for (low, high), req in points_per_stratum.items():
        pts = [t for t in candidate if low <= all_valid_pred[t] < high]
        if len(pts) < req:
            diverse = False
            break
    
    # Regulatory compliance check (assumed external function)
    compliant, reasons = check_regulatory_compliance(
        candidate, regulation,
        dict(zip(candidate, ref_vals.flatten().tolist())),
        dict(zip(candidate, test_vals.flatten().tolist()))
    )
    
    result = {
        'sequence': candidate,
        'f2': round(f2, 2),
        'compliant': compliant,
        'reasons': reasons,
        'length': len(candidate),
        'diverse': diverse,
        'ref_vals': ref_vals.flatten().tolist(),
        'test_vals': test_vals.flatten().tolist()
    }
    return [result], [result]
