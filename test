# main.py
import io
import os
import threading
from typing import List, Optional, Dict, Any

import numpy as np
import pandas as pd
from fastapi import FastAPI, UploadFile, File, Query, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# Attempt to import summit (optional)
try:
    from summit.domain import Domain, ContinuousVariable
    from summit.strategies import TSEMO, SNOBFIT
    from summit.utils.dataset import DataSet
except Exception as e:
    Domain = None
    ContinuousVariable = None
    TSEMO = None
    SNOBFIT = None
    DataSet = None
    print("Warning: summit import failed. Install summit for optimization to work:", e)

app = FastAPI(title="SOR AI Backend (updated workflow)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
    allow_credentials=True,
)

# ---------------- In-memory store & lock ----------------
store: Dict[str, Any] = {
    "lhs_table": None,               # pandas DataFrame with experiments
    "optimization_result": None,     # pandas DataFrame
    "hplc_responses": {},            # filename -> response vector
    "watching": False,
    "watch_path": None,
    # workflow control
    "objectives": None,              # list of {"name":..., "maximize":bool}
    "expected_files": 0,             # number of HPLC files expected (sor iterations)
    "next_row": 0,                   # next row index in lhs_table to fill with HPLC data
    "nobj": 0,                       # number of objectives (length of objectives list)
}
store_lock = threading.Lock()

observer: Optional[Observer] = None

# ---------------- Utilities ----------------
def latin_hypercube_sampling(bounds: List[List[float]], n_samples: int, random_state: Optional[int] = None):
    rng = np.random.RandomState(random_state)
    n_vars = len(bounds)
    cut = np.linspace(0, 1, n_samples + 1)
    u = rng.rand(n_samples, n_vars)
    samples = np.zeros_like(u)
    for j in range(n_vars):
        a = cut[:-1]
        b = cut[1:]
        pts = a + u[:, j] * (b - a)
        rng.shuffle(pts)
        samples[:, j] = pts
    scaled = np.zeros_like(samples)
    for j in range(n_vars):
        low, high = bounds[j]
        scaled[:, j] = low + samples[:, j] * (high - low)
    return scaled

def compute_sor_table(
    eq1_vals, eq2_vals, eq3_vals, rt3_vals, temp_vals,
    molecular_weights=[90.05, 101.19, 318.18, 532.09],
    densities=[1.00, 0.726, 1.08, 0.90]
):
    n = len(eq1_vals)
    rows = []
    coil1 = 5.00
    coil2 = 10.00
    coil3 = 10.00
    mw1, mw2, mw3, mw4 = molecular_weights
    d1, d2, d3, d4 = densities

    for i in range(n):
        E1 = float(eq1_vals[i])
        E2 = float(eq2_vals[i])
        E3 = float(eq3_vals[i])
        RT3 = float(rt3_vals[i])
        TEMP = float(temp_vals[i])

        vol_g_1 = 5 * E1 / 2.5
        vol_g_2 = 2 * E2 / 7.5
        vol_g_3 = 2.5 * E3 / 3.125
        vol_g_4 = 7.0

        mass4 = 1.00
        moles4 = mass4 / mw4

        mass1 = E1 * moles4 * mw1
        mass2 = E2 * moles4 * mw2
        mass3 = E3 * moles4 * mw3
        mass4 = mass4

        vol1 = mass1 / d1
        vol2 = vol_g_1 * mass4
        vol3 = mass2 / d2
        vol4 = vol_g_2 * mass4
        vol5 = mass3 / (0.5 * d3)
        vol6 = vol_g_3 * mass4
        vol7 = mass4
        vol8 = vol_g_4 * mass4

        obs1 = vol1 + vol2
        obs2 = vol3 + vol4
        obs3 = vol5 + vol6
        obs4 = vol7 + vol8
        total_obs = obs1 + obs2 + obs3 + obs4 if (obs1+obs2+obs3+obs4) != 0 else 1e-12

        total_flowrate = coil3 / RT3 if RT3 != 0 else 1e-12

        f1 = obs1 / total_obs * total_flowrate
        f2 = obs2 / total_obs * total_flowrate
        f3 = obs3 / total_obs * total_flowrate
        f4 = obs4 / total_obs * total_flowrate

        rt1 = coil1 / (f1 + f2) if (f1 + f2) != 0 else float('inf')
        rt2 = coil2 / (f1 + f2 + f3) if (f1 + f2 + f3) != 0 else float('inf')
        reaction_time = rt1 + rt2 + RT3

        row = {
            "Equiv1": round(E1, 8), "Equiv2": round(E2, 8), "Equiv3": round(E3, 8),
            "ResidenceTime3": round(RT3, 8),
            "ReactionTemperature": round(TEMP, 8),
            "ReactionTime": round(reaction_time, 8),
            "Flowrate1": round(f1, 8), "Flowrate2": round(f2, 8),
            "Flowrate3": round(f3, 8), "Flowrate4": round(f4, 8),
            "TotalFlowrate": round(total_flowrate, 8),
            "Obs1": round(obs1, 8), "Obs2": round(obs2, 8), "Obs3": round(obs3, 8), "Obs4": round(obs4, 8),
            "Vol1": round(vol1, 8), "Vol2": round(vol2, 8), "Vol3": round(vol3, 8), "Vol4": round(vol4, 8),
            "Vol5": round(vol5, 8), "Vol6": round(vol6, 8), "Vol7": round(vol7, 8), "Vol8": round(vol8, 8),
            "Mass1": round(mass1, 8), "Mass2": round(mass2, 8), "Mass3": round(mass3, 8), "Mass4": round(mass4, 8),
        }
        rows.append(row)

    df = pd.DataFrame(rows)
    cols_order = ["Equiv1", "Equiv2", "Equiv3", "ResidenceTime3", "ReactionTemperature",
                  "ReactionTime", "Flowrate1", "Flowrate2", "Flowrate3", "Flowrate4", "TotalFlowrate"]
    remaining = [c for c in df.columns if c not in cols_order]
    df = df[cols_order + remaining]
    return df

# ---------------- HPLC parsing & response ----------------
def HPLC_data_read_csv(file_path: str) -> np.ndarray:
    try:
        data = pd.read_csv(file_path)
        # accept variations of column names
        if 'Area' in data.columns and 'RT' in data.columns:
            df = pd.DataFrame({"Area": data['Area'], "RT": data['RT']})
        else:
            cols_lower = {c.lower(): c for c in data.columns}
            if 'area' in cols_lower and 'rt' in cols_lower:
                df = pd.DataFrame({"Area": data[cols_lower['area']], "RT": data[cols_lower['rt']]})
            else:
                # try to guess columns if small dataset
                raise ValueError("CSV missing 'Area' and 'RT' columns")
        return df.to_numpy()
    except Exception as e:
        print("HPLC read error:", e)
        return np.array([])

def impurity_response_csv(data_np: np.ndarray, IminRT: float, ImaxRT: float, areaISO: float) -> float:
    areaB = 0.0
    for i in range(data_np.shape[0]):
        if IminRT <= data_np[i, 1] <= ImaxRT:
            areaB += float(data_np[i, 0])
    return areaB / areaISO if areaISO != 0 else 0.0

def response_HPLC_csv(
    data_np: np.ndarray,
    YminRT: float, YmaxRT: float,
    IminRT_list: List[float], ImaxRT_list: List[float],
    minRTISO: float, maxRTISO: float,
    nobj: int
) -> List[float]:
    if data_np.size == 0:
        return [float('inf')] * nobj

    areaA = 0.0
    for i in range(data_np.shape[0]):
        if YminRT <= data_np[i, 1] <= YmaxRT:
            areaA += float(data_np[i, 0])

    areaISO = 0.0
    for i in range(data_np.shape[0]):
        if minRTISO <= data_np[i, 1] <= maxRTISO:
            areaISO += float(data_np[i, 0])

    response = []
    yield_result = areaA / areaISO if areaISO != 0 else 0.0
    response.append(-np.log(yield_result) if yield_result > 0 else float('inf'))

    for i in range(nobj - 1):
        if i < len(IminRT_list) and i < len(ImaxRT_list):
            impurities_result = impurity_response_csv(data_np, IminRT_list[i], ImaxRT_list[i], areaISO)
            response.append(impurities_result)
        else:
            response.append(0.0)

    return response

# ---------------- Domain builder & optimization ----------------
def build_domain_from_df(df: pd.DataFrame, objectives: List[Dict[str, Any]]):
    if Domain is None or ContinuousVariable is None:
        raise RuntimeError("Summit is not installed. Install summit for optimization.")

    required_names = ["Equiv1", "Equiv2", "Equiv3", "ResidenceTime3", "ReactionTemperature"]
    found = {}
    for name in required_names:
        for col in df.columns:
            if col.lower().replace(" ", "") == name.lower().replace(" ", ""):
                found[name] = col
                break

    domain = Domain()
    if len(found) >= 4:
        for canonical in ["Equiv1", "Equiv2", "Equiv3", "ResidenceTime3", "ReactionTemperature"]:
            if canonical in found:
                col = found[canonical]
                lb = float(df[col].min())
                ub = float(df[col].max())
                if lb == ub:
                    lb -= 1e-6; ub += 1e-6
                domain += ContinuousVariable(name=canonical, description=canonical, bounds=[lb, ub])
    else:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if len(numeric_cols) == 0:
            raise ValueError("No numeric columns in LHS to build domain.")
        for col in numeric_cols[:5]:
            lb = float(df[col].min()); ub = float(df[col].max())
            if lb == ub: lb -= 1e-6; ub += 1e-6
            domain += ContinuousVariable(name=str(col), description=str(col), bounds=[lb, ub])

    # Add objectives as ContinuousVariables (0-100)
    for obj in objectives:
        obj_name = obj.get("name", "objective")
        maximize = bool(obj.get("maximize", False))
        domain += ContinuousVariable(name=obj_name, description=obj_name, bounds=[0, 100], is_objective=True, maximize=maximize)

    return domain

def run_summit_optimization(domain, lhs_df: pd.DataFrame, nobj: int):
    if TSEMO is None or SNOBFIT is None or DataSet is None:
        raise RuntimeError("Summit is not installed or failed to import.")

    if nobj > 1:
        strat = TSEMO(domain, random_rate=0.00, n_spectral_points=4000)
        lhs_ds = DataSet.from_df(lhs_df)
        out = strat.suggest_experiments(1, lhs_ds, use_spectral_sample=True, pop_size=100, iterations=100)
    else:
        strat = SNOBFIT(domain)
        lhs_ds = DataSet.from_df(lhs_df)
        out = strat.suggest_experiments(1, lhs_ds)

    try:
        out.columns = [col[0] for col in out.columns]
    except Exception:
        pass
    if "strategy" in out.columns:
        out = out.drop(columns={"strategy"})
    common = [c for c in lhs_df.columns if c in out.columns]
    if len(common) > 0:
        out = out[common]
    return out

# ---------------- HPLC processing & update helper ----------------
def process_hplc_and_update_table(filename: str, data_np: np.ndarray,
                                  YminRT: float, YmaxRT: float,
                                  IminRT_list: List[float], ImaxRT_list: List[float],
                                  minRTISO: float, maxRTISO: float):
    """
    Compute HPLC response vector and write into the next empty row of lhs_table,
    filling objective columns in order.
    """
    with store_lock:
        lhs = store.get("lhs_table")
        objectives = store.get("objectives") or []
        nobj = store.get("nobj", len(objectives))

        if lhs is None:
            raise RuntimeError("No LHS table stored on server.")

        # compute response
        resp = response_HPLC_csv(data_np, YminRT, YmaxRT, IminRT_list, ImaxRT_list, minRTISO, maxRTISO, nobj)

        # determine which row to fill
        next_row = store.get("next_row", 0)
        total_rows = lhs.shape[0]
        if next_row >= total_rows:
            # nothing to fill, ignore or append? we will append a new row with NaNs and then fill
            new_row = {c: np.nan for c in lhs.columns}
            lhs = pd.concat([lhs, pd.DataFrame([new_row])], ignore_index=True)
            store["lhs_table"] = lhs
            total_rows = lhs.shape[0]

        # ensure objective columns exist
        obj_names = [o["name"] for o in objectives]
        for name in obj_names:
            if name not in lhs.columns:
                lhs[name] = np.nan

        # fill responses into objective columns for the next_row
        for j, name in enumerate(obj_names):
            if j < len(resp):
                try:
                    lhs.at[next_row, name] = resp[j]
                except Exception as e:
                    lhs.loc[next_row, name] = resp[j]

        # store hplc response mapping
        store["hplc_responses"][filename] = resp

        # increment next_row
        store["next_row"] = next_row + 1
        store["lhs_table"] = lhs

    # After updating table, run optimization (best-effort)
    try:
        with store_lock:
            current_lhs = store.get("lhs_table")
            objectives = store.get("objectives") or []
            nobj = store.get("nobj", len(objectives))

        domain = build_domain_from_df(current_lhs, objectives)
        out = run_summit_optimization(domain, current_lhs, nobj)
        with store_lock:
            # store optimization output as DataFrame
            try:
                out_df = pd.DataFrame(out)
            except Exception:
                out_df = out
            store["optimization_result"] = out_df
    except Exception as e:
        # keep optimization failures non-fatal; store error message
        with store_lock:
            store["optimization_result"] = {"error": str(e)}

    return resp

# ---------------- Watcher handler ----------------
class HPLCHandler(FileSystemEventHandler):
    def __init__(self, YminRT, YmaxRT, IminRT_list, ImaxRT_list, minRTISO, maxRTISO):
        super().__init__()
        self.YminRT = YminRT
        self.YmaxRT = YmaxRT
        self.IminRT_list = IminRT_list
        self.ImaxRT_list = ImaxRT_list
        self.minRTISO = minRTISO
        self.maxRTISO = maxRTISO

    def on_created(self, event):
        if event.is_directory:
            return
        filepath = event.src_path
        if not filepath.lower().endswith(".csv"):
            return
        filename = os.path.basename(filepath)
        try:
            data_np = HPLC_data_read_csv(filepath)
            resp = process_hplc_and_update_table(filename, data_np,
                                                self.YminRT, self.YmaxRT,
                                                self.IminRT_list, self.ImaxRT_list,
                                                self.minRTISO, self.maxRTISO)
            print(f"[watcher] processed {filename} -> resp={resp} next_row={store.get('next_row')}")
        except Exception as e:
            print(f"[watcher] error processing {filename}: {e}")

# ---------------- Pydantic models ----------------
class GenerateRequest(BaseModel):
    n_experiments: int = 11
    random_seed: Optional[int] = None

class ObjectiveItem(BaseModel):
    name: str
    maximize: Optional[bool] = False

class StartWatchRequest(BaseModel):
    path: str
    objectives: List[ObjectiveItem]
    sor_iterations: int
    # HPLC parsing params with defaults
    YminRT: float = 2.0
    YmaxRT: float = 4.0
    IminRT_list: List[float] = [0.5]
    ImaxRT_list: List[float] = [1.0]
    minRTISO: float = 10.0
    maxRTISO: float = 12.0

class OptimizeRequest(BaseModel):
    objectives: List[Dict[str, Any]]
    sor_iterations: int = 10
    nobj: int = 2

# ---------------- Endpoints ----------------
@app.post("/generate_lhs")
async def generate_lhs(req: GenerateRequest):
    n = max(1, int(req.n_experiments))
    seed = int(req.random_seed) if req.random_seed is not None else None
    bounds = [
        [1.5, 3.0],
        [6.0, 12.0],
        [1.5, 3.0],
        [1.5, 3.0],
        [35.0, 65.0]
    ]
    samples = latin_hypercube_sampling(bounds, n, random_state=seed)
    eq1_vals = samples[:, 0]; eq2_vals = samples[:, 1]; eq3_vals = samples[:, 2]
    rt3_vals = samples[:, 3]; temp_vals = samples[:, 4]
    df = compute_sor_table(eq1_vals, eq2_vals, eq3_vals, rt3_vals, temp_vals)
    with store_lock:
        store["lhs_table"] = df
        store["optimization_result"] = None
        store["hplc_responses"] = {}
        store["objectives"] = None
        store["expected_files"] = 0
        store["next_row"] = 0
        store["nobj"] = 0
    return {"status": "ok", "n": n, "table": df.to_dict(orient="records")}

@app.post("/upload_lhs")
async def upload_lhs(file: UploadFile = File(...)):
    try:
        content = await file.read()
        df = pd.read_excel(io.BytesIO(content), sheet_name=0)
        df.columns = [str(c).strip() for c in df.columns]
        with store_lock:
            store["lhs_table"] = df
            store["optimization_result"] = None
            store["hplc_responses"] = {}
            store["objectives"] = None
            store["expected_files"] = 0
            store["next_row"] = 0
            store["nobj"] = 0
        expected_any = {"Equiv1", "Equiv2", "Equiv3", "ResidenceTime3", "ReactionTemperature"}
        has_expected = bool(expected_any.intersection(set(df.columns)))
        resp = {"status": "ok", "n": df.shape[0], "columns": df.columns.tolist()}
        if not has_expected:
            resp["warning"] = "Uploaded sheet does not contain typical column names (Equiv1/Equiv2/Equiv3/ResidenceTime3/ReactionTemperature). Server stored it anyway."
        return resp
    except Exception as e:
        return {"status": "error", "message": f"Failed to parse Excel: {str(e)}"}

@app.post("/start_watch")
async def start_watch(req: StartWatchRequest = Body(...)):
    """
    Configure the run (objectives + sor_iterations) and start watching path.
    """
    global observer
    path = req.path
    objectives_in = [{"name": o.name, "maximize": bool(o.maximize)} for o in req.objectives]
    sor_iterations = int(req.sor_iterations)
    nobj = len(objectives_in)

    with store_lock:
        if store.get("lhs_table") is None:
            return {"status": "error", "message": "Upload LHS first via /upload_lhs before starting the watch."}
        # Add objective columns (if absent) to lhs_table
        lhs = store["lhs_table"]
        for obj in objectives_in:
            if obj["name"] not in lhs.columns:
                lhs[obj["name"]] = np.nan
        store["lhs_table"] = lhs
        store["objectives"] = objectives_in
        store["expected_files"] = sor_iterations
        store["next_row"] = 0
        store["nobj"] = nobj

    if not os.path.isdir(path):
        return {"status": "error", "message": "Path does not exist or is not a directory"}

    if store.get("watching"):
        return {"status": "already_watching", "path": store.get("watch_path")}

    handler = HPLCHandler(req.YminRT, req.YmaxRT, req.IminRT_list, req.ImaxRT_list, req.minRTISO, req.maxRTISO)
    observer = Observer()
    observer.schedule(handler, path=path, recursive=False)
    observer.daemon = True
    observer.start()

    with store_lock:
        store["watching"] = True
        store["watch_path"] = path

    return {"status": "watching", "path": path, "expected_files": sor_iterations, "objectives": objectives_in}

@app.post("/stop_watch")
async def stop_watch():
    global observer
    if observer:
        observer.stop()
        observer.join(timeout=2)
    with store_lock:
        store["watching"] = False
        store["watch_path"] = None
    return {"status": "stopped"}

@app.post("/upload_hplc")
async def upload_hplc(file: UploadFile = File(...),
                      YminRT: float = Query(2.0),
                      YmaxRT: float = Query(4.0),
                      IminRT_list: List[float] = Query([0.5]),
                      ImaxRT_list: List[float] = Query([1.0]),
                      minRTISO: float = Query(10.0),
                      maxRTISO: float = Query(12.0)):
    """
    Upload a single HPLC CSV via HTTP and process it in the same row-wise manner.
    """
    try:
        filename = file.filename
        content = await file.read()
        tmp = io.BytesIO(content)
        # read CSV bytes into pandas and then into numpy for processing
        df = pd.read_csv(tmp)
        # convert to numpy format expected by response_HPLC_csv
        if 'Area' in df.columns and 'RT' in df.columns:
            data_np = df[['Area', 'RT']].to_numpy()
        else:
            cols_lower = {c.lower(): c for c in df.columns}
            if 'area' in cols_lower and 'rt' in cols_lower:
                data_np = df[[cols_lower['area'], cols_lower['rt']]].to_numpy()
            else:
                return {"status": "error", "message": "CSV missing Area/RT columns"}
        resp = process_hplc_and_update_table(filename, data_np, YminRT, YmaxRT, IminRT_list, ImaxRT_list, minRTISO, maxRTISO)
        return {"status": "ok", "filename": filename, "response": resp, "next_row": store.get("next_row")}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/optimize")
async def optimize(req: OptimizeRequest):
    with store_lock:
        lhs_df = store.get("lhs_table")
    if lhs_df is None:
        return {"status": "error", "message": "No LHS table present. Upload one first via /upload_lhs."}

    try:
        domain = build_domain_from_df(lhs_df, req.objectives)
    except Exception as e:
        return {"status": "error", "message": f"Failed to build domain from LHS: {e}"}

    try:
        out = run_summit_optimization(domain, lhs_df, req.nobj)
    except Exception as e:
        return {"status": "error", "message": f"Optimization failed: {e}"}

    try:
        out_df = pd.DataFrame(out)
    except Exception:
        out_df = out

    with store_lock:
        store["optimization_result"] = out_df
    return {"status": "ok", "result": out_df.to_dict(orient='records')}

@app.get("/results")
async def get_results():
    with store_lock:
        lhs = store.get("lhs_table")
        opt = store.get("optimization_result")
        hplc = store.get("hplc_responses", {})
        watching = store.get("watching", False)
        watch_path = store.get("watch_path", None)
        objectives = store.get("objectives", None)
        expected_files = store.get("expected_files", 0)
        next_row = store.get("next_row", 0)
    return {
        "lhs_table": lhs.to_dict(orient="records") if lhs is not None else None,
        "optimization_result": opt.to_dict(orient="records") if isinstance(opt, pd.DataFrame) else opt,
        "hplc_responses": hplc,
        "watching": watching,
        "watch_path": watch_path,
        "objectives": objectives,
        "expected_files": expected_files,
        "next_row": next_row
    }

@app.on_event("shutdown")
def shutdown_event():
    global observer
    if observer:
        try:
            observer.stop()
            observer.join(timeout=2)
            print("Observer stopped on shutdown")
        except Exception:
            pass

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
