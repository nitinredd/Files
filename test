# Import required libraries
import os
from google.colab import userdata
import numpy as np
from typing import List
import pandas as pd
import json
import re
from sklearn.cluster import BisectingKMeans
from sklearn.metrics import silhouette_score, silhouette_samples
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain.schema import HumanMessage

# Configuration
filename = 'docs.xlsx'
text_column = 'para'
n_subtopics = 25
n_topics = int(n_subtopics ** 0.5)
min_similarity = 0.40

# Azure OpenAI Configuration
azure_openai_endpoint = "YOUR_AZURE_ENDPOINT"  # e.g., "https://your-resource.openai.azure.com/"
azure_api_key = "YOUR_AZURE_API_KEY"
azure_api_version = "2024-02-15-preview"  # Update this as needed
azure_embedding_deployment = "YOUR_EMBEDDING_DEPLOYMENT_NAME"  # The deployment name for your embedding model
azure_chat_deployment = "YOUR_CHAT_DEPLOYMENT_NAME"  # The deployment name for your chat model

# Install required packages
!pip install langchain langchain-openai openai tiktoken scikit-learn

# Set up langchain embeddings with Azure OpenAI
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    azure_endpoint=azure_openai_endpoint,
    azure_deployment=azure_embedding_deployment,
    api_key=azure_api_key,
    api_version=azure_api_version,
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace="azure_cache")

def classify(docs: List[str], topics: List[str], **kwargs):
    """
    Classify documents against topics using embeddings
    
    Args:
        docs: List of document texts
        topics: List of topic names
        
    Returns:
        Similarity matrix between documents and topics
    """
    doc_embed = np.array(cached_embeddings.embed_documents(docs))
    topic_embed = np.array(cached_embeddings.embed_documents(topics))
    return np.dot(doc_embed, topic_embed.T)

def cluster(docs: List[str], n: int = 20, **kwargs):
    """
    Cluster documents using BisectingKMeans
    
    Args:
        docs: List of document texts
        n: Number of clusters
        
    Returns:
        Dictionary containing clustering results
    """
    cluster_model = BisectingKMeans(init='k-means++', n_clusters=n, n_init=10, max_iter=300)
    doc_embed = np.array(cached_embeddings.embed_documents(docs))
    cluster_model.fit(doc_embed)
    distances = np.linalg.norm(doc_embed[:, np.newaxis] - cluster_model.cluster_centers_.T, axis=2)
    
    return {
        "label": cluster_model.labels_,
        "score": silhouette_score(doc_embed, cluster_model.labels_),
        "scores": silhouette_samples(doc_embed, cluster_model.labels_),
        "centroid": np.argmin(distances, axis=0),
    }

# Read and process documents
print("Reading documents...")
docs = pd.read_excel(filename).dropna(subset=text_column).fillna('').astype(str)
result = cluster(docs[text_column].tolist(), n=n_subtopics)
docs['cluster'] = result['label']
docs['score'] = result['scores']
clusters = (
    docs.groupby('cluster')
    .apply(lambda group: group.nlargest(3, 'score')[text_column].tolist())
    .tolist()
)

print(f"Processed {len(docs)} documents into {len(clusters)} clusters")

# Set up Azure OpenAI Chat model
chat_model = AzureChatOpenAI(
    azure_endpoint=azure_openai_endpoint,
    azure_deployment=azure_chat_deployment,
    api_key=azure_api_key,
    api_version=azure_api_version,
    temperature=0
)

# Generate subtopics
print("Generating subtopics...")
messages = [
    HumanMessage(
        content=f'''Here are {len(clusters)} clusters of documents.
        Suggest 2-4 word topic names for each cluster.
        Return a JSON string array of length {len(clusters)}.
        {json.dumps(clusters, indent=2)}'''
    )
]

subtopic_response = chat_model.invoke(messages)
match = re.search(r'```json(.*?)```', subtopic_response.content, re.DOTALL)
subtopics = json.loads(match.group(1) if match else subtopic_response.content)

print(f"Generated {len(subtopics)} subtopics")

# Create higher-level topic groups
print("Creating topic groups...")
messages = [
    HumanMessage(
        content=f'''Cluster these topics into {n_topics} groups.
        Return a JSON object with keys as a 2-4 word group name and values as arrays of topics.
        Ensure at least 2 topics per group.
        {json.dumps(subtopics, indent=2)}'''
    )
]

topic_response = chat_model.invoke(messages)
match = re.search(r'```json(.*?)```', topic_response.content, re.DOTALL)
topics = json.loads(match.group(1) if match else topic_response.content)
topics = pd.DataFrame([
    {'topic': topic, 'subtopic': subtopic}
    for topic, subtopics in topics.items()
    for subtopic in subtopics
])

print(f"Created {len(topics['topic'].unique())} topic groups")

# Calculate document-topic similarities
print("Calculating similarities...")
data = {
    'docs': docs.to_dict(orient='records'),
    'topics': topics.to_dict(orient='records'),
}

matches = data['matches'] = []
similarity = classify(
    [row[text_column] for row in data['docs']],
    [row['subtopic'] for row in data['topics']]
)

for row in range(len(similarity)):
    for col in range(len(similarity[row])):
        if similarity[row][col] > min_similarity:
            matches.append({'doc': row, 'topic': col, 'similarity': similarity[row][col]})

print(f"Found {len(matches)} matches above similarity threshold {min_similarity}")

# Save results to Excel
print("Saving results to Excel...")
with pd.ExcelWriter('docexplore.xlsx') as writer:
    # Save documents sheet
    docs.to_excel(writer, sheet_name='docs', index=False)
    
    # Save topics sheet
    topics.to_excel(writer, sheet_name='topics', index=False)
    
    # Create and save matches sheet
    grid = pd.DataFrame(matches).pivot_table(index='doc', columns='topic', values='similarity')
    grid.index = pd.Series(grid.index).replace(dict(enumerate(docs[text_column].tolist())))
    grid.columns = pd.Series(grid.columns).replace(dict(enumerate(topics['subtopic'].tolist())))
    grid.index.name = text_column
    grid.reset_index().to_excel(writer, sheet_name='matches', index=False)

# Save results to JSON
print("Saving results to JSON...")
with open("docexplore.json", "w") as handle:
    handle.write(json.dumps(data, indent=2))

print("Process complete!")
print("Results saved to:")
print("1. docexplore.xlsx (sheets: docs, topics, matches)")
print("2. docexplore.json")

# Usage instructions
print("\nHow to use this notebook:")
print("1. Replace the Azure OpenAI configuration variables at the top with your credentials:")
print("   - azure_openai_endpoint")
print("   - azure_api_key")
print("   - azure_api_version")
print("   - azure_embedding_deployment")
print("   - azure_chat_deployment")
print("2. Upload your docs.xlsx file with a 'para' column containing the text to analyze")
print("3. Adjust n_subtopics and min_similarity if needed")
print("4. Run all cells")
