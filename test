import os
import io
import json
import uuid
import pandas as pd
import logging
import speech_recognition as sr
import time
import pickle
from pathlib import Path
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Union, List, Dict, Any

from gtts import gTTS
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ─── CONFIG ──────────────────────────────────────────────────────

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure GPT configuration (fill these in)
base_url=""
api_version="2024-02-15-preview"
api_key=""
deployment_name="GPT4o"
model_name="GPT4o"

# Paths to embedded JSON files
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Prime_M+F_JSON.json",
}

# Sample‑prompt tiles (for frontend to fetch)
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X":["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# AGGRESSIVE RATE LIMITING CONFIG
BATCH_SIZE = 10  # Very small batches
BATCH_DELAY = 3  # Longer delay between batches (3 seconds)
MAX_RETRIES = 5  # More retries
RETRY_DELAY = 10  # Longer initial retry delay

# PERSISTENCE CONFIG
CACHE_DIR = "vectorstore_cache"
FAISS_CACHE_FILE = os.path.join(CACHE_DIR, "faiss_store.pkl")
METADATA_CACHE_FILE = os.path.join(CACHE_DIR, "metadata.json")

# ─── SETUP LLM + EMBEDDINGS ──────────────────────────────────────

# Embedding store & cache
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment="Def_data_qa"
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url,
    temperature=0
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# ─── SMART CHUNKING UTILITIES ────────────────────────────────────

def flatten_json(data: Dict[str, Any], parent_key: str = '', sep: str = '.') -> Dict[str, Any]:
    """Flatten nested JSON to make it easier to chunk semantically."""
    items = []
    for k, v in data.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_json(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            if v and isinstance(v[0], dict):
                for i, item in enumerate(v):
                    items.extend(flatten_json(item, f"{new_key}[{i}]", sep=sep).items())
            else:
                items.append((new_key, str(v)))
        else:
            items.append((new_key, v))
    return dict(items)

def create_semantic_chunks(record: Dict[str, Any], record_id: str) -> List[Dict[str, Any]]:
    """
    Create semantic chunks with SMART strategy to prevent hallucinations.
    - Creates 1-2 chunks per record (not 4+)
    - Each chunk is COMPLETE and SELF-CONTAINED
    - Preserves full record context to prevent information loss
    """
    chunks = []
    
    flat_record = flatten_json(record)
    
    # Extract identifiers that should be in EVERY chunk
    identifier_fields = ['id', 'name', 'product', 'batch', 'formula', 'code', 'number']
    identifiers = {}
    identifier_content = []
    
    for key, value in flat_record.items():
        key_lower = key.lower()
        if any(id_field in key_lower for id_field in identifier_fields):
            identifiers[key] = value
            identifier_content.append(f"{key}: {value}")
    
    # Build complete content string
    all_fields = [f"{k}: {v}" for k, v in flat_record.items()]
    
    # STRATEGY: Create chunks that stay under token limits but remain complete
    # Token limit is ~8191 for embeddings, ~2000 tokens = safe chunk
    # Rough estimate: 1 token ≈ 4 characters
    MAX_CHUNK_CHARS = 7000  # ~1750 tokens, leaves room for safety
    
    full_content = " | ".join(all_fields)
    
    if len(full_content) <= MAX_CHUNK_CHARS:
        # Small record: One complete chunk with ALL information
        chunks.append({
            "content": f"RECORD {record_id}: {full_content}",
            "metadata": {
                "record_id": record_id,
                "full_record": json.dumps(record),
                "chunk_part": "complete",
                **identifiers
            }
        })
    else:
        # Large record: Split into 2 overlapping chunks
        # CRITICAL: Both chunks include identifiers for context continuity
        mid_point = len(all_fields) // 2
        
        # First half with identifiers
        first_half = identifier_content + all_fields[:mid_point]
        chunk1_content = " | ".join(first_half)
        
        chunks.append({
            "content": f"RECORD {record_id} (Part 1/2): {chunk1_content}",
            "metadata": {
                "record_id": record_id,
                "full_record": json.dumps(record),
                "chunk_part": "1_of_2",
                **identifiers
            }
        })
        
        # Second half with identifiers (overlap ensures context)
        second_half = identifier_content + all_fields[mid_point:]
        chunk2_content = " | ".join(second_half)
        
        chunks.append({
            "content": f"RECORD {record_id} (Part 2/2): {chunk2_content}",
            "metadata": {
                "record_id": record_id,
                "full_record": json.dumps(record),
                "chunk_part": "2_of_2",
                **identifiers
            }
        })
    
    return chunks

# ─── ENHANCED PROMPT TEMPLATE ────────────────────────────────────

ENHANCED_PROMPT = PromptTemplate(
    template="""You are a precise data assistant. Use ONLY the information provided in the context below to answer the question.

Context Information:
{context}

Question: {question}

CRITICAL INSTRUCTIONS:
1. Answer ONLY based on the provided context above
2. If the information is clearly stated in the context, provide specific details with exact values
3. If the context contains partial information, state exactly what you know and what's missing
4. If the answer is NOT in the context, you MUST say "I don't have that information in the provided data"
5. NEVER make up, infer, or assume information not explicitly stated in the context
6. When multiple records match, provide information from all relevant records
7. Do not use phrases like "based on my knowledge" - only use "based on the provided data"

Answer:""",
    input_variables=["context", "question"]
)

# ─── AGENT CLASSES ───────────────────────────────────────────────

class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": ENHANCED_PROMPT}
        )
    
    def ask(self, query: str) -> tuple[str, List[Document]]:
        resp = self.chain.invoke({"query": query})
        answer = resp.get("result", "")
        sources = resp.get("source_documents", [])
        
        # ANTI-HALLUCINATION: Reconstruct full records from metadata
        # If we retrieved partial chunks, get the complete record
        enhanced_sources = []
        seen_records = set()
        
        for doc in sources:
            record_id = doc.metadata.get("record_id")
            if record_id and record_id not in seen_records:
                seen_records.add(record_id)
                # Get full record from metadata
                full_record = doc.metadata.get("full_record")
                if full_record:
                    enhanced_sources.append(Document(
                        page_content=f"COMPLETE RECORD: {full_record}",
                        metadata=doc.metadata
                    ))
                else:
                    enhanced_sources.append(doc)
            elif not record_id:
                enhanced_sources.append(doc)
        
        return answer, enhanced_sources

class CoordinatorAgent:
    def __init__(self, children: List[ChildAgent]):
        self.children = children

    def coordinate(self, query: str) -> Union[tuple[str, List[Document]], tuple[None, None]]:
        best_answer = None
        best_sources = []
        best_confidence = 0
        
        for child in self.children:
            try:
                ans, sources = child.ask(query)
                
                confidence = 0
                if ans and len(ans) > 20:
                    confidence += 30
                if "don't have" not in ans.lower() and "not found" not in ans.lower():
                    confidence += 40
                if sources:
                    confidence += 30
                
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_answer = ans
                    best_sources = sources
                    
                logger.info(f"Child {child.name} confidence: {confidence}")
                
            except Exception as e:
                logger.warning(f"Child agent {child.name} error: {str(e)}")
        
        if best_confidence > 50:
            return best_answer, best_sources
        return None, None

class OversightAgent:
    def validate(self, answer: str, sources: List[Document]) -> str:
        """
        Validate answer quality and check for hallucinations.
        """
        if not answer or "don't have" in answer.lower():
            return answer
        if len(answer.strip()) < 10:
            return "The answer appears incomplete. Please try rephrasing your question."
        
        # ANTI-HALLUCINATION CHECK: Verify key facts appear in sources
        # Extract specific values from answer (numbers, names, etc.)
        import re
        
        # Extract quoted values, numbers, and specific terms from answer
        answer_facts = set()
        
        # Get numbers (like batch sizes, temperatures, etc.)
        numbers = re.findall(r'\b\d+(?:\.\d+)?\b', answer)
        answer_facts.update(numbers)
        
        # Get capitalized words (likely names, products, etc.)
        words = re.findall(r'\b[A-Z][a-zA-Z0-9]+\b', answer)
        answer_facts.update(words)
        
        # Check if main facts appear in source documents
        if sources and answer_facts:
            source_text = " ".join([doc.page_content for doc in sources])
            
            # Count how many answer facts appear in sources
            facts_found = sum(1 for fact in answer_facts if fact in source_text)
            confidence_ratio = facts_found / len(answer_facts) if answer_facts else 0
            
            # If less than 50% of specific facts are in sources, it might be hallucinated
            if confidence_ratio < 0.5 and len(answer_facts) > 3:
                logger.warning(f"Low confidence answer detected: {confidence_ratio:.2%} facts verified")
                return "I found some information, but I'm not fully confident in the answer. Please verify or rephrase your question for better accuracy."
        
        return answer

class LearningAgent:
    def __init__(self):
        self.logs: List[Dict] = []
    
    def log(self, q: str, a: str, sources: List[Document] = None):
        self.logs.append({
            "query": q,
            "response": a,
            "source_count": len(sources) if sources else 0
        })

class AgentManager:
    def __init__(self, agents: List[ChildAgent]):
        self.coordinator = CoordinatorAgent(agents)
        self.oversight  = OversightAgent()
        self.learning   = LearningAgent()
    
    def handle_query(self, query: str) -> str:
        raw, sources = self.coordinator.coordinate(query)
        answer = raw if raw else "I couldn't find relevant information for your query. Please try rephrasing or ask about specific fields in the dataset."
        validated = self.oversight.validate(answer, sources or [])
        self.learning.log(query, validated, sources)
        return validated

# ─── PERSISTENT CACHE UTILITIES ──────────────────────────────────

def ensure_cache_dir():
    """Create cache directory if it doesn't exist."""
    Path(CACHE_DIR).mkdir(exist_ok=True)

def save_vectorstore_cache(store, metadata):
    """Save FAISS vectorstore and metadata to disk."""
    ensure_cache_dir()
    try:
        # Save FAISS index
        store.save_local(CACHE_DIR)
        logger.info(f"[Cache] Saved FAISS vectorstore to {CACHE_DIR}")
        
        # Save metadata
        with open(METADATA_CACHE_FILE, 'w') as f:
            json.dump(metadata, f)
        logger.info(f"[Cache] Saved metadata to {METADATA_CACHE_FILE}")
        return True
    except Exception as e:
        logger.error(f"[Cache] Failed to save cache: {e}")
        return False

def load_vectorstore_cache(embeddings):
    """Load FAISS vectorstore and metadata from disk."""
    try:
        if not os.path.exists(CACHE_DIR):
            return None, None
        
        # Check if FAISS files exist
        index_file = os.path.join(CACHE_DIR, "index.faiss")
        if not os.path.exists(index_file):
            return None, None
        
        # Load FAISS index
        store = FAISS.load_local(CACHE_DIR, embeddings, allow_dangerous_deserialization=True)
        logger.info(f"[Cache] Loaded FAISS vectorstore from {CACHE_DIR}")
        
        # Load metadata
        if os.path.exists(METADATA_CACHE_FILE):
            with open(METADATA_CACHE_FILE, 'r') as f:
                metadata = json.load(f)
            logger.info(f"[Cache] Loaded metadata from {METADATA_CACHE_FILE}")
        else:
            metadata = {}
        
        return store, metadata
    except Exception as e:
        logger.error(f"[Cache] Failed to load cache: {e}")
        return None, None

# ─── DATA LOADING & VECTORSTORE BUILD ────────────────────────────

def load_json_data_with_chunking(paths: Dict[str, str]) -> Dict[str, List[Dict[str, Any]]]:
    """Load JSON data and create semantic chunks."""
    all_chunks: Dict[str, List[Dict[str, Any]]] = {}
    
    for name, path in paths.items():
        if not path:
            continue
        try:
            logger.info(f"[Loading] Reading file: {name}")
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            chunks = []
            if isinstance(data, list):
                total_records = len(data)
                logger.info(f"[Chunking] Processing {total_records} records from '{name}'")
                
                for idx, item in enumerate(data):
                    record_id = f"{name}_{idx}"
                    record_chunks = create_semantic_chunks(item, record_id)
                    chunks.extend(record_chunks)
                    
                    if (idx + 1) % 500 == 0:
                        logger.info(f"[Chunking] Processed {idx + 1}/{total_records} records")
            else:
                record_chunks = create_semantic_chunks(data, f"{name}_0")
                chunks.extend(record_chunks)
            
            all_chunks[name] = chunks
            logger.info(f"[Data] Created {len(chunks)} chunks from {name}")
            
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    
    return all_chunks

def build_vectorstores_with_robust_rate_limiting(chunks_dict: Dict[str, List[Dict[str, Any]]]) -> List[ChildAgent]:
    """
    Build FAISS vectorstores with ROBUST rate limiting and caching.
    Uses persistent cache to avoid re-embedding on restarts.
    """
    agents: List[ChildAgent] = []
    
    for key, chunks in chunks_dict.items():
        logger.info(f"[Vectorstore] Building store for '{key}' with {len(chunks)} chunks")
        
        # Check if we have a cached version
        cached_store, cached_metadata = load_vectorstore_cache(cached_embeddings)
        
        if cached_store and cached_metadata.get('source') == key and cached_metadata.get('chunk_count') == len(chunks):
            logger.info(f"[Cache] Using cached vectorstore for '{key}'")
            retriever = cached_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 10}
            )
            agents.append(ChildAgent(name=key, retriever=retriever))
            continue
        
        # No cache, build from scratch
        logger.info(f"[Vectorstore] No cache found. Building new vectorstore...")
        
        docs = [
            Document(
                page_content=chunk["content"],
                metadata=chunk["metadata"]
            )
            for chunk in chunks
        ]
        
        if not docs:
            continue
        
        # Process in VERY SMALL batches with aggressive rate limiting
        total_batches = (len(docs) + BATCH_SIZE - 1) // BATCH_SIZE
        logger.info(f"[Vectorstore] Processing {len(docs)} docs in {total_batches} batches (batch_size={BATCH_SIZE}, delay={BATCH_DELAY}s)")
        
        store = None
        for batch_idx in range(0, len(docs), BATCH_SIZE):
            batch_docs = docs[batch_idx:batch_idx + BATCH_SIZE]
            batch_num = (batch_idx // BATCH_SIZE) + 1
            
            retry_count = 0
            while retry_count < MAX_RETRIES:
                try:
                    logger.info(f"[Vectorstore] Processing batch {batch_num}/{total_batches} ({len(batch_docs)} docs)")
                    
                    if store is None:
                        # Create initial store
                        store = FAISS.from_documents(batch_docs, cached_embeddings)
                    else:
                        # Add to existing store
                        store.add_documents(batch_docs)
                    
                    logger.info(f"[Vectorstore] ✓ Batch {batch_num}/{total_batches} completed")
                    
                    # Save progress every 10 batches
                    if batch_num % 10 == 0 and store:
                        save_vectorstore_cache(store, {'source': key, 'chunk_count': len(chunks), 'partial': True})
                        logger.info(f"[Cache] Saved progress checkpoint at batch {batch_num}")
                    
                    break  # Success, exit retry loop
                    
                except Exception as e:
                    error_str = str(e).lower()
                    if "429" in error_str or "rate" in error_str or "quota" in error_str:
                        retry_count += 1
                        # Exponential backoff with longer delays
                        wait_time = RETRY_DELAY * (2 ** (retry_count - 1))
                        logger.warning(f"[Vectorstore] ⚠ Rate limit hit. Retry {retry_count}/{MAX_RETRIES} after {wait_time}s")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"[Vectorstore] ✗ Error processing batch: {e}")
                        raise
            
            if retry_count >= MAX_RETRIES:
                logger.error(f"[Vectorstore] Failed after {MAX_RETRIES} retries. Stopping.")
                raise Exception(f"Rate limit exceeded after {MAX_RETRIES} retries")
            
            # Aggressive delay between batches
            if batch_idx + BATCH_SIZE < len(docs):
                logger.info(f"[Vectorstore] Waiting {BATCH_DELAY}s before next batch...")
                time.sleep(BATCH_DELAY)
        
        if store:
            # Save final completed cache
            save_vectorstore_cache(store, {'source': key, 'chunk_count': len(chunks), 'partial': False})
            
            # Create retriever
            retriever = store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 10}
            )
            
            agents.append(ChildAgent(name=key, retriever=retriever))
            logger.info(f"[Vectorstore] ✓ Completed store for '{key}' with {len(docs)} total docs")
    
    return agents

# ─── INITIALIZE ON STARTUP ───────────────────────────────────────

logger.info("=" * 70)
logger.info("[Startup] Initializing Multi-Agent Chatbot with Robust Rate Limiting")
logger.info("=" * 70)

try:
    logger.info("[Startup] Step 1/3: Loading and chunking data...")
    CHUNKED_DATA = load_json_data_with_chunking(EMBEDDED_FILES)
    
    logger.info("[Startup] Step 2/3: Building vectorstores (this may take a while)...")
    logger.info("[Startup] Using aggressive rate limiting: batch_size=10, delay=3s")
    AGENTS = build_vectorstores_with_robust_rate_limiting(CHUNKED_DATA)
    
    logger.info("[Startup] Step 3/3: Initializing agent manager...")
    MANAGER = AgentManager(AGENTS)
    recognizer = sr.Recognizer()
    
    logger.info("=" * 70)
    logger.info("[Startup] ✓ System ready!")
    logger.info("=" * 70)
except Exception as e:
    logger.error("=" * 70)
    logger.error(f"[Startup] ✗ FATAL ERROR: {e}")
    logger.error("=" * 70)
    raise

# ─── FASTAPI APP ─────────────────────────────────────────────────

app = FastAPI(title="Multi-Agent JSON Chatbot - Production Ready")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

# ─── ENDPOINTS ────────────────────────────────────────────────────

@app.get("/sample-tiles")
def sample_tiles():
    """Return the sample-prompt tiles and questions."""
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
def chat(req: ChatRequest):
    q = req.message.strip()
    if not q:
        raise HTTPException(400, "Empty query")
    logger.info(f"[API] /chat query='{q}'")
    ans = MANAGER.handle_query(q)
    return {"response": ans}

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    """Accepts an uploaded audio file and returns transcribed text."""
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"[API] STT error: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    """Returns an MP3 audio stream of the given text."""
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"[API] TTS error: {e}")
        raise HTTPException(500, str(e))

@app.get("/health")
def health():
    total_chunks = sum(len(chunks) for chunks in CHUNKED_DATA.values())
    return {
        "status": "ok",
        "agents": [a.name for a in AGENTS],
        "total_chunks": total_chunks,
        "cache_enabled": True
    }

@app.get("/stats")
def stats():
    """Get statistics about the loaded data."""
    stats_info = {}
    for name, chunks in CHUNKED_DATA.items():
        chunk_types = {}
        for chunk in chunks:
            ctype = chunk["metadata"].get("chunk_part", "unknown")
            chunk_types[ctype] = chunk_types.get(ctype, 0) + 1
        stats_info[name] = {
            "total_chunks": len(chunks),
            "chunk_types": chunk_types
        }
    return stats_info

@app.post("/clear-cache")
def clear_cache():
    """Clear the vectorstore cache (use if data changes)."""
    try:
        import shutil
        if os.path.exists(CACHE_DIR):
            shutil.rmtree(CACHE_DIR)
            return {"status": "cache cleared", "message": "Please restart the server"}
        return {"status": "no cache found"}
    except Exception as e:
        raise HTTPException(500, str(e))
