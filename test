backend
# main.py
import os
import re
import glob
import base64
import io
import json
import uuid
import mimetypes
import traceback
import threading
from typing import Union, List, Optional, Dict, Any
from datetime import datetime, timedelta
from difflib import SequenceMatcher

import fitz  # PyMuPDF
import pandas as pd
from PIL import Image
from pydantic import BaseModel
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Depends, Request
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

# Langchain / Azure (kept as you asked — do not change unless needed)
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA, LLMChain
from langchain.prompts import PromptTemplate

# Optional libs for PPTX generation and document parsing
try:
    from pptx import Presentation
    from pptx.util import Inches, Pt
except Exception:
    Presentation = None

try:
    import docx  # python-docx
except Exception:
    docx = None

try:
    import openpyxl
except Exception:
    openpyxl = None

# ---------------------------
# Configuration - Please populate secrets / endpoints
# ---------------------------
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Azure Configuration (kept exactly as requested)
base_url=""
api_version="2025-01-01-preview"

api_key=""
deployment_name="api-ai4o"
model_name="gpt-4o"

# Initialize Azure services
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-3-large",
    api_version="2025-01-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment="api-ai-3l"
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

AZ_BING_ENDPOINT = "https://api.bing.microsoft.com/"
AZ_BING_KEY = ""
USE_BING_DEFAULT = True

SECRET_KEY = os.environ.get("RDB_SECRET_KEY", "myFAVsecretKEY")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

BASE_DIR = os.environ.get("RDB_BASE_DIR", r"C:\Users\Desktop\WORK\API\Chemhub+\Backend\Reaction_Database")
PRODUCTS_DIR = os.path.join(BASE_DIR, "Products")
SCHEMES_DIR = os.path.join(BASE_DIR, "Synthetic_Schemes")
STRUCTURES_DIR = os.path.join(BASE_DIR, "Structures")
UPLOADS_DIR = os.path.join(BASE_DIR, "User_Uploads")

os.makedirs(PRODUCTS_DIR, exist_ok=True)
os.makedirs(SCHEMES_DIR, exist_ok=True)
os.makedirs(STRUCTURES_DIR, exist_ok=True)
os.makedirs(UPLOADS_DIR, exist_ok=True)

REACTION_TYPES = ["C-C_Bond_Formation", "C-N_Bond_Formation", "Salt_Formation", "Hydrolysis", "Esterification", "Amidation", "Reduction", "Oxidation", "Cyclization", "Purification", "Miscellaneous" , "Metal mediated catalyzed", "Gate Clearance Reports", "User_Uploads"]

# ---------------------------
# Prompts (kept/back-ported from your working original)
# ---------------------------
EXTRACTION_PROMPT_TEMPLATE = """
You are a pharmaceutical chemistry expert specializing in reaction chemistry. Extract the following information from the document in a structured format Mandatorily:
1. **API Name**: The active pharmaceutical ingredient
2. **Reaction Chemistry**: Type and description
3. **Yield**: Exact yield percentages or values mentioned in the source
4. **Procedure**: Summarize the complete procedure into clear, concise numbered bullet points, preserving the key steps and important details. Do NOT omit any steps or essential content.
5. **Tabular Data**: Provide COMPLETE tabular data in markdown table format. Do NOT omit, summarize, or transform any content.

Structure your response as follows (literal headers must appear exactly like below):

### API Name
[API name here]
### Reaction Chemistry
[Reaction chemistry description here]
### Yield
[Yield value here]
### Procedure
[Complete procedure here]
### Tabular Data
[Markdown table here]

Document Content:
{context}
Question: {question}
Answer:
"""
EXTRACTION_PROMPT = PromptTemplate(template=EXTRACTION_PROMPT_TEMPLATE, input_variables=["context", "question"])

QA_PROMPT_TEMPLATE = """
You are a concise pharmaceutical chemistry expert. Use the provided document context to answer the user's question directly and concisely.

Rules (follow exactly):
- Use only the information present in the context. Do NOT hallucinate.
- Answer in one short paragraph (2-6 sentences) unless the user explicitly asks for step-by-step procedure.
- Do NOT reproduce the full document content. Do NOT provide unrequested long verbatim passages.
- If multiple documents were used, identify only those documents that were actually the sources of the answer.
- If the answer is not present in the context, say "I could not find an answer in the provided documents." (do not guess).

Context:
{context}

Question: {question}

Answer:
"""
QA_PROMPT = PromptTemplate(template=QA_PROMPT_TEMPLATE, input_variables=["context", "question"])

GENERATIVE_PROMPT_TEMPLATE = """
You are a helpful and concise pharmaceutical chemistry expert with general knowledge.
Answer the user's question directly and helpfully. If the question is a greeting or short chit-chat, reply naturally.
If you don't know the answer, say "I don't know" or "I couldn't find reliable information" (do NOT say you couldn't find it in provided documents).
Give a detailed and elaborated response.
Question: {question}
"""
GENERATIVE_PROMPT = PromptTemplate(template=GENERATIVE_PROMPT_TEMPLATE, input_variables=["question"])

# ------------------------------
# Internal caches & memory
# ------------------------------
_vectorstore_cache: Dict[str, FAISS] = {}
_product_details_cache: Dict[str, Dict[str, Any]] = {}
_vectorstore_lock = threading.Lock()

# Chat memory store (simple in-memory; replace with DB in prod)
_chat_memory: Dict[str, List[Dict[str, str]]] = {}

# ------------------------------
# Helper utilities
# ------------------------------
def find_scheme_image(reaction_type: str, product_name: str) -> Optional[str]:
    for ext in ['.jpeg', '.jpg', '.png', '.gif']:
        scheme_path = os.path.join(SCHEMES_DIR, reaction_type, f"{product_name}{ext}")
        if os.path.exists(scheme_path):
            return scheme_path
    return None

def find_structure_image(reaction_type: str, product_name: str) -> Optional[str]:
    for ext in ['.jpeg', '.jpg', '.png', '.gif']:
        structure_path = os.path.join(STRUCTURES_DIR, reaction_type, f"{product_name}{ext}")
        if os.path.exists(structure_path):
            return structure_path
    return None

def list_products() -> List[Dict[str, Any]]:
    products = []
    for reaction_type in REACTION_TYPES:
        reaction_dir = os.path.join(PRODUCTS_DIR, reaction_type)
        if not os.path.exists(reaction_dir):
            continue
        pdf_files = glob.glob(os.path.join(reaction_dir, "*.pdf"))
        for pdf_path in pdf_files:
            filename = os.path.basename(pdf_path)
            product_name = os.path.splitext(filename)[0]
            scheme_image = find_scheme_image(reaction_type, product_name)
            structure_image = find_structure_image(reaction_type, product_name)
            scheme_cdx = os.path.join(SCHEMES_DIR, reaction_type, f"{product_name}.cdx")
            product_id = f"{reaction_type}_{product_name}"
            products.append({
                "id": product_id,
                "name": product_name,
                "reaction_type": reaction_type,
                "pdf_path": pdf_path,
                "scheme_image": scheme_image if scheme_image else None,
                "structure_image": structure_image if structure_image else None,
                "scheme_cdx": scheme_cdx if os.path.exists(scheme_cdx) else None
            })
    return products

def extract_pdf_text(pdf_path: str) -> str:
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text() + "\n"
        return text
    except Exception as e:
        raise RuntimeError(f"Error reading PDF {pdf_path}: {e}")

def extract_docx_text(docx_path: str) -> str:
    if not docx:
        return ""
    try:
        d = docx.Document(docx_path)
        return "\n".join([p.text for p in d.paragraphs if p.text])
    except Exception:
        return ""

def extract_pptx_text(pptx_path: str) -> str:
    if not Presentation:
        return ""
    try:
        prs = Presentation(pptx_path)
        texts = []
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    texts.append(shape.text)
        return "\n".join(texts)
    except Exception:
        return ""

def extract_xlsx_text(xlsx_path: str) -> str:
    try:
        import openpyxl
        wb = openpyxl.load_workbook(xlsx_path, data_only=True)
        texts = []
        for sheet in wb.worksheets:
            for row in sheet.iter_rows(values_only=True):
                texts.append(" | ".join([str(c) if c is not None else "" for c in row]))
        return "\n".join(texts)
    except Exception:
        return ""

def chunk_text(text: str, max_chars: int = 3000) -> List[str]:
    """Simple chunker that splits into paragraphs accumulating until max_chars reached."""
    if not text:
        return []
    paragraphs = [p.strip() for p in re.split(r"\n{1,}", text) if p.strip()]
    chunks = []
    current = ""
    for p in paragraphs:
        if len(current) + len(p) + 1 > max_chars:
            if current:
                chunks.append(current.strip())
            current = p
        else:
            current = (current + "\n\n" + p).strip()
    if current:
        chunks.append(current.strip())
    return chunks

def build_product_vector_store(product: Dict[str, Any]) -> Optional[FAISS]:
    pid = product["id"]
    with _vectorstore_lock:
        if pid in _vectorstore_cache:
            return _vectorstore_cache[pid]
        # extract text from possible file types
        path = product.get("pdf_path")
        if not path or not os.path.exists(path):
            return None
        text = extract_pdf_text(path)
        if not text or len(text.strip()) < 50:
            return None
        doc = Document(page_content=text, metadata={
            "product_id": pid,
            "product_name": product["name"],
            "reaction_type": product["reaction_type"],
            "source": product["pdf_path"]
        })
        vs = FAISS.from_documents([doc], cached_embeddings)
        _vectorstore_cache[pid] = vs
        return vs

def parse_structured_response(text: str) -> Dict[str, Any]:
    result = {"raw": text, "api_name": None, "reaction_chemistry": None, "yield": None, "procedure": None, "tables": []}
    def grab(section):
        m = re.search(rf"###\s*{re.escape(section)}\s*(.*?)\s*(?=###\s*\w+|\Z)", text, re.DOTALL | re.IGNORECASE)
        return m.group(1).strip() if m else None
    result["api_name"] = grab("API Name")
    result["reaction_chemistry"] = grab("Reaction Chemistry")
    result["yield"] = grab("Yield")
    result["procedure"] = grab("Procedure")
    tab_raw = grab("Tabular Data")
    if tab_raw:
        table_patterns = re.findall(r"(\|[^\n]*\|\s*\n\|[-:\s|]*\|\s*\n(?:\|[^\n]*\|\s*\n?)*)", tab_raw, re.DOTALL)
        if table_patterns:
            for tbl_md in table_patterns:
                lines = [ln.strip().strip("|").strip() for ln in tbl_md.splitlines() if ln.strip()]
                if len(lines) >= 2:
                    header = [h.strip() for h in lines[0].split("|")]
                    rows = []
                    for rowline in lines[2:]:
                        cols = [c.strip() for c in rowline.split("|")]
                        rows.append(cols)
                    result["tables"].append({"headers": header, "rows": rows, "raw_md": tbl_md})
        else:
            result["tables"].append({"headers": [], "rows": [], "raw_md": tab_raw})
    return result

# ------------------------------
# Bing web search helper (Azure Bing)
# ------------------------------
def bing_search_azure(query: str, top: int = 3) -> List[Dict[str, str]]:
    """
    Return list of dicts: {"name":..., "snippet":..., "url":...}
    Requires AZ_BING_ENDPOINT and AZ_BING_KEY to be set.
    """
    if not AZ_BING_ENDPOINT or not AZ_BING_KEY:
        return []
    import requests
    endpoint = AZ_BING_ENDPOINT.rstrip("/") + "/v7.0/search"
    headers = {"Ocp-Apim-Subscription-Key": AZ_BING_KEY}
    params = {"q": query, "count": top}
    try:
        r = requests.get(endpoint, headers=headers, params=params, timeout=15)
        r.raise_for_status()
        j = r.json()
        results = []
        web = j.get("webPages", {}).get("value", [])
        for w in web[:top]:
            results.append({"name": w.get("name", ""), "snippet": w.get("snippet", ""), "url": w.get("url", "")})
        return results
    except Exception as e:
        print("Bing search error:", e)
        return []

# ------------------------------
# FastAPI app
# ------------------------------
app = FastAPI(title="Reaction Database AI (FastAPI)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # in prod restrict this
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ------------------------------
# Request models
# ------------------------------
class ProductOut(BaseModel):
    id: str
    name: str
    reaction_type: str
    has_scheme_image: bool
    has_scheme_cdx: bool

class QARequest(BaseModel):
    product_id: Optional[str] = None
    question: str = "Extract API Name, Reaction Chemistry, Yield, Procedure, and Tabular Data"
    session_id: Optional[str] = None

class QueryRequest(BaseModel):
    product_ids: List[str]
    question: str
    session_id: Optional[str] = None

# ------------------------------
# Endpoints
# ------------------------------
@app.get("/reactions", response_model=List[str])
def get_reactions():
    return REACTION_TYPES

@app.get("/products", response_model=List[ProductOut])
def get_products(reaction_type: Optional[str] = None):
    allp = list_products()
    if reaction_type:
        allp = [p for p in allp if p["reaction_type"] == reaction_type]
    out = []
    for p in allp:
        out.append(ProductOut(
            id=p["id"],
            name=p["name"],
            reaction_type=p["reaction_type"],
            has_scheme_image=bool(p["scheme_image"]),
            has_scheme_cdx=bool(p["scheme_cdx"])
        ))
    return out

@app.get("/product/{product_id}/meta")
def product_meta(product_id: str):
    products = list_products()
    for p in products:
        if p["id"] == product_id:
            return {
                "id": p["id"],
                "name": p["name"],
                "reaction_type": p["reaction_type"],
                "pdf_path": p["pdf_path"],
                "scheme_image": p["scheme_image"],
                "structure_image": p["structure_image"],
                "scheme_cdx": p["scheme_cdx"]
            }
    raise HTTPException(status_code=404, detail="Product not found")

@app.get("/product/{product_id}/pdf")
def product_pdf(product_id: str):
    meta = product_meta(product_id)
    pdf_path = meta["pdf_path"]
    if os.path.exists(pdf_path):
        return FileResponse(
            pdf_path,
            media_type="application/pdf",
            headers={"Content-Disposition": "inline"}
        )
    raise HTTPException(status_code=404, detail="PDF not found")

@app.get("/product/{product_id}/scheme-image")
def product_scheme_image(product_id: str):
    meta = product_meta(product_id)
    path = meta.get("scheme_image")
    if path and os.path.exists(path):
        # serve with the correct mimetype
        mime, _ = mimetypes.guess_type(path)
        return FileResponse(path, media_type=mime or "application/octet-stream", filename=os.path.basename(path))
    raise HTTPException(status_code=404, detail="Scheme image not found")

@app.get("/product/{product_id}/structure-image")
def product_structure_image(product_id: str):
    meta = product_meta(product_id)
    path = meta.get("structure_image")
    if path and os.path.exists(path):
        # serve with the correct mimetype
        mime, _ = mimetypes.guess_type(path)
        return FileResponse(path, media_type=mime or "application/octet-stream", filename=os.path.basename(path))
    raise HTTPException(status_code=404, detail="Structure image not found")

# ------------------------------
# Main product/details endpoint (merged behavior)
# ------------------------------
@app.post("/product/details")
def product_details(req: QARequest):
    """
    Behavior:
     - If req.product_id provided:
         * if canonical extraction question -> run structured extraction (cached)
         * else -> retrieval QA for that product
     - If no req.product_id:
         * Try to detect product name in question (exact substring / token-overlap / similarity)
         * If detected -> run retrieval for detected product
         * Else -> generative response (LLM) — if Bing is enabled we can optionally ground with web snippets
    """
    try:
        q_text = (req.question or "").strip()
        if not q_text:
            return JSONResponse(status_code=400, content={"error": "question is required"})

        CANONICAL_EXTRACTION = "extract api name, reaction chemistry, yield, procedure, and tabular data"

        def _normalize_alnum(s: str) -> str:
            return re.sub(r"[^a-z0-9]", "", (s or "").lower())

        def _tokens(s: str):
            return [t for t in re.split(r'[^a-z0-9]+', (s or "").lower()) if t]

        def _normalize_simple(s: str) -> str:
            return re.sub(r'[^a-z0-9\s]', ' ', (s or "").lower())

        # quick debug log
        print("=== /product/details called ===")
        print("Question:", q_text)
        print("product_id:", getattr(req, "product_id", None), "session_id:", getattr(req, "session_id", None))

        # detect whether question mentions any reaction keyword
        q_norm_simple = _normalize_simple(q_text)
        contains_reaction_keyword = any(_normalize_simple(rt).strip() in q_norm_simple for rt in REACTION_TYPES)
        print("DEBUG: contains_reaction_keyword=", contains_reaction_keyword)

        # product detection
        def _detect_product_by_name(question: str) -> Optional[Dict[str, Any]]:
            products = list_products()
            if not products:
                return None
            q_norm_alnum = _normalize_alnum(question)
            q_tokens = set(_tokens(question))
            # 1 exact normalized substring match (prefer longer names)
            sorted_products = sorted(products, key=lambda p: len(p["name"] or ""), reverse=True)
            for p in sorted_products:
                nm = _normalize_alnum(p["name"])
                if nm and nm in q_norm_alnum:
                    print(f"DEBUG: Exact normalized substring match -> {p['name']}")
                    return p
            # 2 token overlap
            token_matches = []
            for p in sorted_products:
                pname_tokens = set(_tokens(p["name"]))
                if not pname_tokens: continue
                overlap = pname_tokens.intersection(q_tokens)
                if overlap:
                    token_matches.append((p, len(overlap), len(pname_tokens), overlap))
            if token_matches:
                token_matches.sort(key=lambda x: (-(x[1] / x[2]), -x[1]))
                p_best, match_count, token_count, overlap = token_matches[0]
                ratio = match_count / token_count
                print(f"DEBUG: Token-overlap candidate -> {p_best['name']} ratio={ratio:.2f}")
                if ratio >= 0.5:
                    return p_best
            # 3 similarity fallback
            best = None
            best_ratio = 0.0
            for p in products:
                pname = (p["name"] or "").lower()
                if not pname.strip(): continue
                r1 = SequenceMatcher(None, pname, question.lower()).ratio()
                r2 = SequenceMatcher(None, _normalize_alnum(pname), _normalize_alnum(question)).ratio()
                ratio = (r1 + r2) / 2.0
                if ratio > best_ratio:
                    best_ratio = ratio
                    best = (p, ratio, r1, r2)
            if best and best[1] >= 0.60:
                p_best, ratio, r1, r2 = best
                print(f"DEBUG: Similarity candidate -> {p_best['name']} ratio={ratio:.3f}")
                return p_best
            return None

        def _run_retrieval_for_product(product: Dict[str, Any], question: str, k: int = 3) -> Dict[str, Any]:
            print(f"DEBUG: Running retrieval product {product['id']}")
            pdf_path = product.get("pdf_path")
            if not pdf_path or not os.path.exists(pdf_path):
                raise HTTPException(status_code=404, detail="PDF not found for product")
            # attempt to reuse cached vectorstore
            with _vectorstore_lock:
                vs = _vectorstore_cache.get(product["id"])
            if not vs:
                vs = build_product_vector_store(product)
                if not vs:
                    raise HTTPException(status_code=500, detail="Failed to build vector store for product")
            retriever = vs.as_retriever(search_kwargs={"k": k})
            prompt_to_use = QA_PROMPT
            qa_chain = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt_to_use},
                return_source_documents=True,
            )
            out = qa_chain({"query": question})
            answer_text = out.get("result") or out.get("output_text") or ""
            source_docs = out.get("source_documents", []) or []
            seen = set()
            sources = []
            for sd in source_docs:
                pid = sd.metadata.get("product_id")
                pname = sd.metadata.get("product_name")
                if pid and pid not in seen:
                    seen.add(pid)
                    sources.append({"product_id": pid, "product_name": pname})
            print(f"DEBUG: Retrieval answer length={len(answer_text)} sources={len(sources)}")
            return {"answer": answer_text, "sources": sources}

        # 1) explicit product path
        if req.product_id:
            products = list_products()
            product = next((p for p in products if p["id"] == req.product_id), None)
            if not product:
                return JSONResponse(status_code=404, content={"error": "Product not found"})
            is_extraction = q_text.strip().lower() == CANONICAL_EXTRACTION
            if not is_extraction:
                return _run_retrieval_for_product(product, q_text, k=3)
            # extraction path
            if req.product_id in _product_details_cache:
                print("DEBUG: returning cached extraction for", req.product_id)
                return _product_details_cache[req.product_id]
            vs = build_product_vector_store(product)
            if not vs:
                return JSONResponse(status_code=500, content={"error": "Failed to build vector store (empty/invalid PDF)"})
            retriever = vs.as_retriever(search_kwargs={"k": 1})
            qa_chain = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": EXTRACTION_PROMPT},
                return_source_documents=False,
            )
            raw_response = qa_chain.run(q_text)
            parsed = parse_structured_response(raw_response)
            _product_details_cache[req.product_id] = parsed
            return parsed

        # 2) short greeting / generative bypass (unless reaction keyword present)
        tokens = _tokens(q_text)
        if (not contains_reaction_keyword) and (len(q_text) < 4 or (len(tokens) <= 1 and not any(c.isdigit() for c in q_text))):
            # direct generative LLM
            try:
                # optionally include Bing if enabled
                context_snips = ""
                if AZ_BING_ENDPOINT and AZ_BING_KEY:
                    snips = bing_search_azure(q_text, top=3)
                    if snips:
                        context_snips = "\n\nWeb snippets:\n" + "\n".join([f"- {s['name']}: {s['snippet']}" for s in snips])
                prompt_chain = LLMChain(llm=chat_model, prompt=GENERATIVE_PROMPT)
                # If we have context_snips, append to the question
                question_for_llm = q_text + ("\n\n" + context_snips if context_snips else "")
                raw_response = prompt_chain.predict(question=question_for_llm)
                # store to memory if session provided
                if req.session_id:
                    _chat_memory.setdefault(req.session_id, []).append({"role": "assistant", "content": raw_response})
                return {"response": raw_response}
            except Exception as e:
                print("Generative error:", e)
                tb = traceback.format_exc()
                print(tb)
                raise HTTPException(status_code=500, detail=f"Generative model error: {e}")

        # 3) detect product name in free text
        detected = _detect_product_by_name(q_text)
        if detected:
            return _run_retrieval_for_product(detected, q_text, k=3)

        # 4) fallback generative QA (optionally ground via Bing)
        try:
            context_snips = ""
            if AZ_BING_ENDPOINT and AZ_BING_KEY:
                snips = bing_search_azure(q_text, top=3)
                if snips:
                    context_snips = "\n\nWeb snippets:\n" + "\n".join([f"- {s['name']}: {s['snippet']}" for s in snips])
            prompt_chain = LLMChain(llm=chat_model, prompt=GENERATIVE_PROMPT)
            question_for_llm = q_text + ("\n\n" + context_snips if context_snips else "")
            raw_response = prompt_chain.predict(question=question_for_llm)
            if req.session_id:
                _chat_memory.setdefault(req.session_id, []).append({"role": "assistant", "content": raw_response})
            return {"response": raw_response}
        except Exception as e:
            print("Fallback generative error:", e)
            tb = traceback.format_exc()
            print(tb)
            raise HTTPException(status_code=500, detail=f"LLM fallback error: {e}")

    except HTTPException:
        raise
    except Exception as e:
        tb = traceback.format_exc()
        print("=== /product/details ERROR ===")
        print(tb)
        return JSONResponse(status_code=500, content={"error": "internal server error", "detail": str(e), "trace": tb.splitlines()[-20:]})

# ------------------------------
# Query across multiple product documents
# ------------------------------
@app.post("/query")
def query_documents(req: QueryRequest):
    if not req.product_ids:
        raise HTTPException(status_code=400, detail="product_ids must be non-empty")
    if not req.question or not req.question.strip():
        raise HTTPException(status_code=400, detail="question required")
    docs = []
    for pid in req.product_ids:
        try:
            meta = product_meta(pid)
        except HTTPException:
            continue
        pdf_path = meta.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            continue
        text = extract_pdf_text(pdf_path)
        if not text or len(text.strip()) < 50:
            continue
        docs.append(Document(page_content=text, metadata={"product_id": pid, "product_name": meta.get("name", ""), "reaction_type": meta.get("reaction_type", "")}))
    if not docs:
        raise HTTPException(status_code=404, detail="No documents available for provided product_ids")
    try:
        vector_store = FAISS.from_documents(docs, cached_embeddings)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed building vector index: {e}")
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})
    qa_chain = RetrievalQA.from_chain_type(llm=chat_model, chain_type="stuff", retriever=retriever, chain_type_kwargs={"prompt": QA_PROMPT}, return_source_documents=True)
    out = qa_chain({"query": req.question})
    answer_text = out.get("result") or out.get("output_text") or str(out)
    source_docs = out.get("source_documents", [])
    seen = set()
    sources = []
    for sd in source_docs:
        pid = sd.metadata.get("product_id")
        pname = sd.metadata.get("product_name")
        if pid and pid not in seen:
            seen.add(pid)
            sources.append({"product_id": pid, "product_name": pname})
    # store to session memory
    if req.session_id:
        _chat_memory.setdefault(req.session_id, []).append({"role": "assistant", "content": answer_text})
    return {"answer": answer_text, "sources": sources}

# ------------------------------
# Product search (autocomplete)
# ------------------------------
@app.get("/products/search")
def search_products(q: str = "", limit: int = 10):
    if q is None:
        q = ""
    q_norm = re.sub(r'[^a-z0-9]', '', q.lower())
    if q_norm == "":
        prods = list_products()[:limit]
        return [{"id": p["id"], "name": p["name"], "reaction_type": p["reaction_type"]} for p in prods]
    products = list_products()
    prefix_matches = []
    substring_matches = []
    for p in products:
        name_norm = re.sub(r'[^a-z0-9]', '', p["name"].lower())
        if name_norm.startswith(q_norm):
            prefix_matches.append(p)
        elif q_norm in name_norm:
            substring_matches.append(p)
    prefix_matches = sorted(prefix_matches, key=lambda x: -len(x["name"]))
    substring_matches = sorted(substring_matches, key=lambda x: -len(x["name"]))
    combined = (prefix_matches + substring_matches)[:limit]
    return [{"id": p["id"], "name": p["name"], "reaction_type": p["reaction_type"]} for p in combined]

# ------------------------------
# File upload endpoint (documents/images). Saves under Products/<reaction_type>/filename
# Accepts optional reaction_type form value (default: Miscellaneous)
# ------------------------------
@app.post("/upload")
async def upload_file(file: UploadFile = File(...), reaction_type: str = Form("User_Uploads")):
    safe_rt = reaction_type if reaction_type in REACTION_TYPES else "User_Uploads"
    dest_dir = os.path.join(PRODUCTS_DIR, safe_rt)
    os.makedirs(dest_dir, exist_ok=True)
    filename = os.path.basename(file.filename)
    dest_path = os.path.join(dest_dir, filename)
    # save file
    with open(dest_path, "wb") as f:
        f.write(await file.read())
    # attempt to extract text if document-like and build vectorstore
    ext = os.path.splitext(filename)[1].lower()
    text = ""
    try:
        if ext == ".pdf":
            text = extract_pdf_text(dest_path)
        elif ext in (".docx", ".doc") and docx:
            text = extract_docx_text(dest_path)
        elif ext in (".pptx", ".ppt") and Presentation:
            text = extract_pptx_text(dest_path)
        elif ext in (".xlsx", ".xls") and openpyxl:
            text = extract_xlsx_text(dest_path)
        else:
            # images or unknown file types --- store file only
            text = ""
    except Exception as e:
        print("Upload extraction error:", e)
        text = ""
    # If we have text, chunk and create FAISS (single-doc)
    if text and len(text.strip()) > 50:
        pid = f"{safe_rt}_{os.path.splitext(filename)[0]}"
        product_meta = {"id": pid, "name": os.path.splitext(filename)[0], "reaction_type": safe_rt, "pdf_path": dest_path}
        try:
            vs = build_product_vector_store(product_meta)
            # vs created/cached
        except Exception as e:
            print("Error building vectorstore for uploaded file:", e)
    return {"status": "ok", "path": dest_path, "extracted_chars": len(text)}

# ------------------------------
# Prompt gallery endpoint
# ------------------------------
@app.get("/prompt-gallery")
def prompt_gallery():
    samples = [
        {"title": "Extract reaction details", "prompt": "Extract API Name, Reaction Chemistry, Yield, Procedure, and Tabular Data"},
        {"title": "Yield only", "prompt": "What is the yield for <product-name>?"},
        {"title": "Procedure steps", "prompt": "Give the complete procedure for <product-name> exactly as written."},
        {"title": "Summarize", "prompt": "Summarize the document into 5 bullets."},
        {"title": "Generate slides", "prompt": "Create 5 PPT slides on [topic] with slide titles and bullet points."}
    ]
    return {"prompts": samples}

# ------------------------------
# PPTX slide generation
# ------------------------------
@app.post("/generate/pptx")
def generate_pptx(topic: str = Form(...), slides: int = Form(5), session_id: Optional[str] = Form(None)):
    if Presentation is None:
        raise HTTPException(status_code=500, detail="python-pptx not installed on server.")
    try:
        # Ask LLM for slide contents
        slide_prompt = f"Create {slides} slide outlines for a presentation on: {topic}. For each slide return a title and 3-5 bullet points. Output in JSON array of objects: {{'title':'','bullets':['','',...]}}"
        llm_chain = LLMChain(llm=chat_model, prompt=PromptTemplate(template="Question: {question}\nAnswer (JSON):", input_variables=["question"]))
        raw = llm_chain.predict(question=slide_prompt)
        # Try to parse JSON from raw
        try:
            # LLM may return markdown or text - try to find JSON substring
            jstart = raw.find("[")
            jend = raw.rfind("]") + 1
            slides_json = json.loads(raw[jstart:jend]) if jstart != -1 and jend != -1 else json.loads(raw)
        except Exception:
            # fallback crude parse: create simple slides from split lines
            slides_json = []
            lines = [l.strip() for l in raw.splitlines() if l.strip()]
            for i in range(min(slides, max(1, len(lines)))):
                title = lines[i][:60]
                bullets = [lines[i+1:i+4]] if i+1 < len(lines) else [[""]]
                slides_json.append({"title": title, "bullets": [" ".join(b) for b in bullets[0]]})
        # Build PPTX
        prs = Presentation()
        for s in slides_json[:slides]:
            slide = prs.slides.add_slide(prs.slide_layouts[1] if len(prs.slide_layouts) > 1 else prs.slide_layouts[0])
            title = slide.shapes.title
            title.text = s.get("title", "")[:120]
            tf = slide.shapes.placeholders[1].text_frame
            tf.clear()
            for b in s.get("bullets", [])[:6]:
                p = tf.add_paragraph()
                p.text = b
                p.level = 0
        out_path = os.path.join(UPLOADS_DIR, f"slides_{uuid.uuid4().hex[:8]}.pptx")
        prs.save(out_path)
        return FileResponse(out_path, media_type="application/vnd.openxmlformats-officedocument.presentationml.presentation", filename=os.path.basename(out_path))
    except Exception as e:
        tb = traceback.format_exc()
        print("PPT generation error:", tb)
        raise HTTPException(status_code=500, detail=f"PPT generation failed: {e}")

# ------------------------------
# Transcribe endpoint (server-side optional Google STT)
# ------------------------------
@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...), use_google: Optional[bool] = Form(False)):
    content = await file.read()
    if not use_google:
        return {"error": "Server-side transcription disabled. Use browser Web Speech API for demo, or set use_google=True and provide GOOGLE_CREDENTIALS_JSON."}
    if not GOOGLE_CREDENTIALS_JSON:
        return {"error": "GOOGLE_CREDENTIALS_JSON not configured."}
    try:
        from google.cloud import speech_v1p1beta1 as speech
        from google.oauth2 import service_account
    except Exception as e:
        return {"error": f"google-cloud-speech library not installed: {e}"}
    creds_info = json.loads(GOOGLE_CREDENTIALS_JSON)
    credentials = service_account.Credentials.from_service_account_info(creds_info)
    client = speech.SpeechClient(credentials=credentials)
    audio = speech.RecognitionAudio(content=content)
    config = speech.RecognitionConfig(encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16, sample_rate_hertz=16000, language_code="en-US", enable_automatic_punctuation=True)
    response = client.recognize(config=config, audio=audio)
    transcripts = [r.alternatives[0].transcript for r in response.results]
    return {"transcript": " ".join(transcripts)}

# ------------------------------
# Chat memory endpoints
# ------------------------------
@app.post("/memory/{session_id}")
def append_memory(session_id: str, message: Dict[str, str]):
    """
    message = {"role": "user"|"assistant", "content": "..."}
    """
    if not session_id:
        raise HTTPException(status_code=400, detail="session_id required")
    _chat_memory.setdefault(session_id, []).append(message)
    return {"status": "ok", "count": len(_chat_memory[session_id])}

@app.get("/memory/{session_id}")
def get_memory(session_id: str):
    return {"session_id": session_id, "messages": _chat_memory.get(session_id, [])}

# ------------------------------
# Health
# ------------------------------
@app.get("/health")
def health():
    return {"status": "ok"}

# ------------------------------
# Utility: create access token and login (kept from original; relies on config.check_ldap_auth)
# ------------------------------
from jose import jwt
SECRET_KEY = "myFAVsecretKEY"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

def create_access_token(data: dict, expires_delta: Union[timedelta , None] = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

@app.post("/login")
async def login(user_info: dict = Depends(lambda: {"first_name":"demo","last_name":"user"})):
    # wire to your config.check_ldap_auth in prod
    user = user_info
    if not user:
        raise HTTPException(status_code=400, detail="Invalid credentials")
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    token = create_access_token(data={"sub": user["first_name"]}, expires_delta=access_token_expires)
    return {"access_token": token, "token_type": "Bearer", "firstname": user["first_name"], "lastname": user.get("last_name","")}
