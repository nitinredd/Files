import os
import subprocess
import json
import streamlit as st
from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field

# --- Azure settings ---
base_url = "https://your-resource.openai.azure.com/"  # Replace with your endpoint
api_version = "2024-02-15-preview"
api_key = "your_api_key_here"  # Replace with your key
deployment_name = "GPT4o"

# Initialize Azure Chat
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url,
    temperature=0
)

# === Improved Python execution function ===
def execute_python(code: str) -> str:
    """Executes Python code safely and returns output"""
    try:
        # Remove leading/trailing whitespace and triple backticks
        cleaned_code = code.strip().strip('`').strip()
        if cleaned_code.startswith('python'):
            cleaned_code = cleaned_code[6:].strip()
            
        # Properly indent the code for our wrapper
        indented_code = "\n".join(f"    {line}" for line in cleaned_code.splitlines())
        
        # Create the safe execution wrapper
        wrapped_code = f"""import sys
def safe_exec():
    try:
{indented_code}
    except Exception as e:
        print(f"ERROR: {{e}}", file=sys.stderr)
        
if __name__ == "__main__":
    safe_exec()
"""
        # Debug: Uncomment to see the actual executed code
        # print("Executing code:\n", wrapped_code)
        
        proc = subprocess.run(
            ["python", "-c", wrapped_code],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if proc.returncode != 0:
            return f"‚õî Execution Error (code {proc.returncode}):\n{proc.stderr.strip()}"
        return proc.stdout.strip() or "‚úÖ Executed successfully (no output)"
    
    except subprocess.TimeoutExpired:
        return "‚è∞ Timeout Error: Code took too long to execute"
    except Exception as e:
        return f"üî• System Error: {str(e)}"

# === Define structured output schema ===
class CodeExecutor(BaseModel):
    """Schema for Python code execution"""
    code: str = Field(..., 
        description="Python code to execute. Must assign final result to 'result' variable or print it."
    )
    
    def execute(self):
        return execute_python(self.code)

# Create structured model
structured_llm = chat_model.with_structured_output(CodeExecutor)

# === Streamlit UI ===
st.title("Azure GPT‚Äë4o + Python Sandbox")
st.caption("This agent can execute Python code for calculations and data processing")

# Initialize session state
if "history" not in st.session_state:
    st.session_state.history = []

# Display chat history
for msg in st.session_state.history:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])
        if "code" in msg:
            with st.expander("Generated Python Code"):
                st.code(msg["code"], language="python")
        if "execution" in msg:
            with st.expander("Execution Result"):
                st.text(msg["execution"])

# Handle user input
if prompt := st.chat_input("What would you like to compute?"):
    # Add user message to history
    st.session_state.history.append({"role": "user", "content": prompt})
    
    # Display user message
    with st.chat_message("user"):
        st.write(prompt)
    
    # Create prompt template with stronger instructions
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", 
         "You are a Python coding expert. Strictly follow these rules:\n"
         "1. Generate COMPLETE, READY-TO-RUN Python code for any math/data operations\n"
         "2. Code MUST be properly indented with 4 spaces\n"
         "3. Assign final result to 'result' OR print it\n"
         "4. DO NOT include markdown code blocks (```python or ```)\n"
         "5. Use only built-in Python modules\n"
         "6. Include error handling with try/except blocks\n\n"
         "Example for calculating factorial:\n"
         "def factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\nresult = factorial(5)"),
        ("human", "{input}")
    ])
    
    # Get AI response
    with st.spinner("Analyzing request..."):
        try:
            chain = prompt_template | structured_llm
            response = chain.invoke({"input": prompt})
            code = response.code
        except Exception as e:
            st.error(f"‚ùå AI Error: {str(e)}")
            st.stop()
    
    # Add AI message to history
    st.session_state.history.append({
        "role": "assistant",
        "content": "Generated Python code",
        "code": code
    })
    
    # Display code
    with st.chat_message("assistant"):
        st.write("Executing Python code...")
        with st.expander("Generated Code", expanded=False):
            st.code(code, language="python")
        
        # Execute code
        with st.spinner("Running code..."):
            execution_result = execute_python(code)
        
        # Display execution result
        st.session_state.history[-1]["execution"] = execution_result
        with st.expander("Execution Result", expanded=True):
            st.text(execution_result)

    # Rerun to update UI
    st.rerun()
