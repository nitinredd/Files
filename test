# Complete Dissolution Similarity Analyzer - Streamlit Application
# Integrates all backend modules for pharmaceutical dissolution testing

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from io import BytesIO
import warnings
warnings.filterwarnings('ignore')

# Import backend modules
from Similarity_analyzer_final_version import (
    main_function,
    dissolution_curve,
    dissolution_curve_interval,
    check_time_points,
    two_time_points,
    min15__check,
    check_cv,
    check_same_time_points
)

from sa_main import (
    main_f2,
    prepare_data,
    check_sample_units
)

from Recommended_time_points import check_cv_criteria

# Page configuration
st.set_page_config(
    page_title="Dissolution Similarity Analyzer",
    page_icon="üíä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS styling
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: bold;
        color: #2ca02c;
        margin-top: 2rem;
    }
    .info-box {
        background-color: #e7f3ff;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 5px solid #1f77b4;
        margin: 1rem 0;
    }
    .success-box {
        background-color: #d4edda;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 5px solid #28a745;
    }
    .warning-box {
        background-color: #fff3cd;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 5px solid #ffc107;
    }
    .error-box {
        background-color: #f8d7da;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 5px solid #dc3545;
    }
    .metric-card {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 0.5rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 2rem;
    }
    .stTabs [data-baseweb="tab"] {
        padding: 1rem 2rem;
        font-size: 1.1rem;
    }
</style>
""", unsafe_allow_html=True)

# Session state initialization
if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False
if 'results' not in st.session_state:
    st.session_state.results = None

def validate_data(df, name):
    """Validate uploaded data format"""
    errors = []

    if df is None or df.empty:
        errors.append(f"{name} dataframe is empty")
        return errors

    if df.shape[1] < 2:
        errors.append(f"{name} must have at least 2 columns (Time + at least 1 unit)")

    # Check if first column is numeric (time points)
    try:
        pd.to_numeric(df.iloc[:, 0])
    except:
        errors.append(f"{name} first column (Time) must be numeric")

    # Check if subsequent columns are numeric (dissolution values)
    for col_idx in range(1, df.shape[1]):
        try:
            pd.to_numeric(df.iloc[:, col_idx])
        except:
            errors.append(f"{name} column {col_idx+1} must be numeric")

    return errors

def display_validation_checks(ref_df, test_df, market):
    """Display validation checks in a nice format"""
    st.markdown('<div class="sub-header">üìã Validation Checks</div>', unsafe_allow_html=True)

    checks = {
        "Minimum 3 time points (excl. zero)":
            [check_time_points(ref_df), check_time_points(test_df)],
        "First two points ‚â§85% dissolution":
            [two_time_points(ref_df), two_time_points(test_df)],
        "No >85% dissolution in first 15 min":
            [not min15__check(ref_df), not min15__check(test_df)],
        "CV requirements met":
            [check_cv(ref_df), check_cv(test_df)],
        "‚â•12 individual units":
            [check_sample_units(ref_df), check_sample_units(test_df)],
        "Same time points for both products":
            [check_same_time_points(ref_df, test_df), check_same_time_points(ref_df, test_df)]
    }

    # Create dataframe for display
    check_df = pd.DataFrame({
        "Validation Check": list(checks.keys()),
        "Reference": ["‚úÖ Pass" if checks[k][0] else "‚ùå Fail" for k in checks.keys()],
        "Test": ["‚úÖ Pass" if checks[k][1] else "‚ùå Fail" for k in checks.keys()]
    })

    st.dataframe(check_df, use_container_width=True, hide_index=True)

    # Check if all validations passed
    all_passed = all([all(v) for v in checks.values()])

    if all_passed:
        st.markdown('<div class="success-box">‚úÖ All validation checks passed!</div>', unsafe_allow_html=True)
    else:
        st.markdown('<div class="warning-box">‚ö†Ô∏è Some validation checks failed. Results should be interpreted with caution.</div>', unsafe_allow_html=True)

    return all_passed

def download_excel(dataframes_dict, filename="results.xlsx"):
    """Create downloadable Excel file with multiple sheets"""
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        for sheet_name, df in dataframes_dict.items():
            if df is not None and not df.empty:
                df.to_excel(writer, sheet_name=sheet_name, index=False)
    output.seek(0)
    return output

def main():
    # Header
    st.markdown('<div class="main-header">üíä Dissolution Similarity Analyzer</div>', unsafe_allow_html=True)
    st.markdown("---")

    # Sidebar
    with st.sidebar:
        st.markdown("## üìÅ Data Upload")

        # File uploaders
        ref_file = st.file_uploader(
            "Upload Reference Data (Excel)",
            type=['xlsx', 'xls'],
            help="Excel file with Time in first column, followed by unit columns"
        )

        test_file = st.file_uploader(
            "Upload Test Data (Excel)",
            type=['xlsx', 'xls'],
            help="Excel file with Time in first column, followed by unit columns"
        )

        st.markdown("---")
        st.markdown("## üåç Regulatory Market")

        market = st.selectbox(
            "Select Regulatory Agency",
            ["US FDA", "EMA/ICH/Canada/Australia", "CHINA", "ASEAN", "ANVISA", "WHO"],
            help="Different markets have different dissolution testing requirements"
        )

        st.markdown("---")
        st.markdown("## ‚öôÔ∏è Analysis Settings")

        include_bootstrap = st.checkbox(
            "Include Bootstrap Analysis",
            value=True,
            help="Run bootstrap simulations for confidence intervals (10,000 iterations)"
        )

        show_optimal_timepoints = st.checkbox(
            "Show Optimal Timepoint Predictions",
            value=True,
            help="Calculate optimal timepoint sequences using predictive algorithms"
        )

        st.markdown("---")

        # Analyze button
        analyze_button = st.button("üöÄ Run Analysis", use_container_width=True, type="primary")

    # Main content area
    if not ref_file or not test_file:
        st.markdown('<div class="info-box">üëà Please upload both Reference and Test data files to begin analysis</div>', unsafe_allow_html=True)

        # Show example data format
        with st.expander("üìñ Expected Data Format"):
            st.markdown("""
            **Excel File Format:**
            - First column: Time points (e.g., 0, 5, 10, 15, 30, 45, 60 minutes)
            - Subsequent columns: Dissolution values for each unit (minimum 12 units recommended)
            - All values should be numeric
            - Header row is optional

            **Example:**
            ```
            Time  Unit1  Unit2  Unit3  ...  Unit12
            0     0      0      0      ...  0
            5     25     27     24     ...  26
            10    45     48     43     ...  46
            15    65     68     62     ...  66
            ...
            ```
            """)

        # Show market-specific requirements
        with st.expander("üåç Market-Specific Requirements"):
            st.markdown("""
            **US FDA:**
            - Both products must exceed 85% dissolution simultaneously
            - CV ‚â§20% at first non-zero timepoint, ‚â§10% thereafter
            - Minimum 12 units per product

            **EMA/ICH/Canada/Australia:**
            - Either product exceeds 85% dissolution
            - CV ‚â§20% up to 10 minutes, ‚â§10% after
            - Expected f2 preferred when CV criteria met

            **China/ASEAN:**
            - Similar to EMA requirements
            - Either 85% rule applies

            **ANVISA:**
            - Both 85% rule (similar to FDA)
            - CV ‚â§20% for first 40% of timepoints, ‚â§10% thereafter

            **WHO:**
            - Similar to EMA requirements
            - Either 85% rule applies
            """)

        return

    # Load data
    try:
        reference_df = pd.read_excel(ref_file, header=None)
        test_df = pd.read_excel(test_file, header=None)

        st.success(f"‚úÖ Files loaded successfully! Reference: {reference_df.shape}, Test: {test_df.shape}")

    except Exception as e:
        st.error(f"‚ùå Error loading files: {str(e)}")
        return

    # Validate data
    ref_errors = validate_data(reference_df, "Reference")
    test_errors = validate_data(test_df, "Test")

    if ref_errors or test_errors:
        st.markdown('<div class="error-box">‚ùå Data Validation Errors:</div>', unsafe_allow_html=True)
        for error in ref_errors + test_errors:
            st.error(error)
        return

    # Display data preview
    with st.expander("üëÄ Preview Uploaded Data", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**Reference Data**")
            st.dataframe(reference_df.head(10), use_container_width=True)
        with col2:
            st.markdown("**Test Data**")
            st.dataframe(test_df.head(10), use_container_width=True)

    # Run analysis
    if analyze_button:
        with st.spinner("üî¨ Running analysis... This may take a few minutes for bootstrap simulations."):
            try:
                # Display validation checks
                validation_passed = display_validation_checks(reference_df, test_df, market)

                st.markdown("---")

                # Determine recommended methodologies based on CV criteria
                st.markdown('<div class="sub-header">üéØ Recommended Methodologies</div>', unsafe_allow_html=True)

                recommended_methods = check_cv_criteria(reference_df, test_df, market)

                st.info(f"**Based on CV criteria and market selection ({market}), recommended methods:**")
                for method in recommended_methods:
                    st.markdown(f"- {method}")

                # Allow user to select additional methods
                all_methods = [
                    "Conventional F2",
                    "Expected F2",
                    "Bias-Corrected F2",
                    "Conventional Bootstrap F2",
                    "Expected Bootstrap F2",
                    "Bias-Corrected Bootstrap F2"
                ]

                # Filter bootstrap methods based on user selection
                if not include_bootstrap:
                    all_methods = [m for m in all_methods if "Bootstrap" not in m]

                selected_methods = st.multiselect(
                    "Select methodologies to calculate:",
                    all_methods,
                    default=recommended_methods,
                    help="Select which f2 calculation methods you want to include"
                )

                if not selected_methods:
                    st.warning("‚ö†Ô∏è Please select at least one methodology")
                    return

                st.markdown("---")

                # Run main function analysis
                st.markdown('<div class="sub-header">üìä Dissolution Profile Analysis</div>', unsafe_allow_html=True)

                progress_bar = st.progress(0)
                status_text = st.empty()

                status_text.text("Step 1/4: Calculating conventional f2...")
                progress_bar.progress(25)

                # Call main_function from Similarity_analyzer_final_version.py
                main_results = main_function(market, test_df, reference_df)

                # Unpack results based on return values
                if len(main_results) == 8:
                    conv_f2, fig1, fig2, seq_print, recom_f2, plotly_fig, cv_checks, ai_df = main_results
                else:
                    st.error("Unexpected number of return values from main_function")
                    return

                status_text.text("Step 2/4: Generating dissolution plots...")
                progress_bar.progress(50)

                # Display conventional f2 result
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(
                        label="Conventional f2",
                        value=f"{conv_f2:.2f}" if isinstance(conv_f2, (int, float)) else "N/A",
                        delta="Similar" if isinstance(conv_f2, (int, float)) and conv_f2 >= 50 else "Not Similar"
                    )
                with col2:
                    st.metric(
                        label="CV Criteria",
                        value="Pass" if cv_checks == "True" else "Fail"
                    )
                with col3:
                    if recom_f2:
                        st.metric(
                            label="Recommended f2",
                            value=f"{recom_f2:.2f}" if isinstance(recom_f2, (int, float)) else "N/A"
                        )

                # Display dissolution curves
                st.plotly_chart(fig1, use_container_width=True)
                st.plotly_chart(fig2, use_container_width=True)

                # Display optimal profile if available
                if plotly_fig and show_optimal_timepoints:
                    st.markdown('<div class="sub-header">üéØ Optimal Predicted Profile</div>', unsafe_allow_html=True)
                    st.plotly_chart(plotly_fig, use_container_width=True)

                    if ai_df is not None and not ai_df.empty:
                        st.markdown("**Predicted Dissolution Values:**")
                        st.dataframe(ai_df, use_container_width=True, hide_index=True)

                status_text.text("Step 3/4: Calculating point estimates...")
                progress_bar.progress(75)

                # Run bootstrap analysis if selected
                if include_bootstrap and any("Bootstrap" in m for m in selected_methods):
                    st.markdown("---")
                    st.markdown('<div class="sub-header">üîÑ Bootstrap Analysis</div>', unsafe_allow_html=True)

                    status_text.text("Step 4/4: Running bootstrap simulations (10,000 iterations)...")

                    # Prepare data for bootstrap
                    ref_clean, test_clean = prepare_data(reference_df, test_df)

                    # Call main_f2 from sa_main.py
                    results, bootstrap_results, qq_plots = main_f2(
                        ref_clean,
                        test_clean,
                        selected_methods,
                        conv_f2,
                        market
                    )

                    progress_bar.progress(100)
                    status_text.text("‚úÖ Analysis complete!")

                    # Display point estimates
                    if results:
                        st.markdown("### Point Estimates")

                        point_est_df = pd.DataFrame([
                            {"Method": k, "f2 Value": v}
                            for k, v in results.items()
                            if k not in ['cv_result_expected_f2']
                        ])

                        if not point_est_df.empty:
                            st.dataframe(point_est_df, use_container_width=True, hide_index=True)

                    # Display bootstrap results
                    if bootstrap_results:
                        st.markdown("### Bootstrap Results")

                        for method, boot_res in bootstrap_results.items():
                            with st.expander(f"üìà {method}", expanded=True):
                                col1, col2, col3, col4 = st.columns(4)

                                with col1:
                                    st.metric("Observed f2", boot_res.get("Observed F2", "N/A"))
                                with col2:
                                    st.metric("Mean", boot_res.get("Mean", "N/A"))
                                with col3:
                                    st.metric("Median", boot_res.get("Median", "N/A"))
                                with col4:
                                    ci = boot_res.get("Confidence Interval", "N/A")
                                    st.metric("90% CI", ci)

                                col5, col6, col7 = st.columns(3)
                                with col5:
                                    st.metric("Skewness", boot_res.get("Skewness", "N/A"))
                                with col6:
                                    st.metric("Kurtosis", boot_res.get("Kurtosis", "N/A"))
                                with col7:
                                    similarity = boot_res.get("Similarity of R and T", "N/A")
                                    st.metric("Similarity", similarity)

                                # Display histogram plot
                                if "Hist Plot" in boot_res and boot_res["Hist Plot"]:
                                    st.plotly_chart(boot_res["Hist Plot"], use_container_width=True)

                        # Display QQ plots
                        if qq_plots:
                            st.markdown("### QQ Plots (Normality Assessment)")

                            qq_cols = st.columns(min(len(qq_plots), 3))
                            for idx, (method, qq_fig) in enumerate(qq_plots.items()):
                                with qq_cols[idx % 3]:
                                    st.markdown(f"**{method}**")
                                    st.plotly_chart(qq_fig, use_container_width=True)

                else:
                    progress_bar.progress(100)
                    status_text.text("‚úÖ Analysis complete!")

                st.markdown("---")

                # Summary and interpretation
                st.markdown('<div class="sub-header">üìù Summary & Interpretation</div>', unsafe_allow_html=True)

                if isinstance(conv_f2, (int, float)):
                    if conv_f2 >= 50:
                        st.markdown(
                            '<div class="success-box">'
                            f'‚úÖ <b>Conclusion:</b> The test and reference products are considered <b>similar</b> '
                            f'based on conventional f2 value of {conv_f2:.2f} (‚â•50).'
                            '</div>',
                            unsafe_allow_html=True
                        )
                    else:
                        st.markdown(
                            '<div class="warning-box">'
                            f'‚ö†Ô∏è <b>Conclusion:</b> The test and reference products are <b>not similar</b> '
                            f'based on conventional f2 value of {conv_f2:.2f} (<50).'
                            '</div>',
                            unsafe_allow_html=True
                        )

                # Bootstrap interpretation
                if include_bootstrap and bootstrap_results:
                    st.markdown("**Bootstrap Interpretation:**")
                    for method, boot_res in bootstrap_results.items():
                        ci = boot_res.get("Confidence Interval", "")
                        if ci and " - " in ci:
                            lower = float(ci.split(" - ")[0])
                            if lower > 50:
                                st.success(f"‚úÖ {method}: 5th percentile ({lower:.2f}) > 50, supporting similarity")
                            else:
                                st.warning(f"‚ö†Ô∏è {method}: 5th percentile ({lower:.2f}) ‚â§ 50, similarity not confirmed")

                st.markdown("---")

                # Download results
                st.markdown('<div class="sub-header">üíæ Download Results</div>', unsafe_allow_html=True)

                # Prepare data for download
                download_data = {
                    "Reference_Data": reference_df,
                    "Test_Data": test_df,
                }

                if ai_df is not None and not ai_df.empty:
                    download_data["Optimal_Timepoints"] = ai_df

                if results:
                    results_df = pd.DataFrame([
                        {"Metric": k, "Value": v}
                        for k, v in results.items()
                    ])
                    download_data["Point_Estimates"] = results_df

                if bootstrap_results:
                    boot_summary = []
                    for method, boot_res in bootstrap_results.items():
                        boot_summary.append({
                            "Method": method,
                            "Observed_f2": boot_res.get("Observed F2", "N/A"),
                            "Mean": boot_res.get("Mean", "N/A"),
                            "Median": boot_res.get("Median", "N/A"),
                            "CI_90": boot_res.get("Confidence Interval", "N/A"),
                            "Skewness": boot_res.get("Skewness", "N/A"),
                            "Kurtosis": boot_res.get("Kurtosis", "N/A"),
                            "Similarity": boot_res.get("Similarity of R and T", "N/A")
                        })
                    download_data["Bootstrap_Results"] = pd.DataFrame(boot_summary)

                excel_file = download_excel(download_data)

                st.download_button(
                    label="üì• Download Complete Results (Excel)",
                    data=excel_file,
                    file_name=f"dissolution_analysis_{market.replace('/', '_')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )

                st.success("‚úÖ Analysis completed successfully!")

            except Exception as e:
                st.error(f"‚ùå An error occurred during analysis: {str(e)}")
                st.exception(e)

if __name__ == "__main__":
    main()
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
sa_main.py
#sa_main
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import MaxNLocator
import warnings
from scipy.stats import skew, kurtosis, norm
from joblib import Parallel, delayed
import pandas as pd
import plotly.graph_objects as go

from scipy import stats





# Suppress warnings
warnings.filterwarnings('ignore')

# Initialize global array for bootstrap
arrayboot = []

# Set random seed for reproducibility
np.random.seed(306)

# --------------------------
# Data Processing Functions
# --------------------------

def prepare_data(reference_df, test_df):
    """Remove time zero if present and reset index"""
    if reference_df.iloc[0, 0] == 0 or reference_df.iloc[0, 0] == '0':
        reference_df = reference_df.iloc[1:].reset_index(drop=True)
    if test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0':
        test_df = test_df.iloc[1:].reset_index(drop=True)
    return reference_df, test_df

# --------------------------
# Visualization Functions


def dissolution_curve(reference_df, test_df):
    try:
        # Calculate means for reference and test data
        ref_means = reference_df.iloc[:, 1:].mean(axis=1)
        test_means = test_df.iloc[:, 1:].mean(axis=1)
        times = reference_df.iloc[:, 0]

        # Create traces
        ref_trace = go.Scatter(
            x=times,
            y=ref_means,
            mode='lines+markers',
            name='Reference mean',
            marker=dict(symbol='circle', size=8),
            line=dict(color='blue')
        )

        test_trace = go.Scatter(
            x=times,
            y=test_means,
            mode='lines+markers',
            name='Test mean',
            marker=dict(symbol='circle', size=8),
            line=dict(color='green', dash='dash')
        )

        # Layout
        layout = go.Layout(
            title='Dissolution Curves',
            xaxis=dict(title='Time (minutes)', zeroline=True, zerolinewidth=1, zerolinecolor='black'),
            yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10,
                       zeroline=True, zerolinewidth=1, zerolinecolor='black',
                       showgrid=True, gridcolor='lightgray'),
            showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            plot_bgcolor='white',
            paper_bgcolor='white'
        )

        # Create figure
        fig = go.Figure(data=[ref_trace, test_trace], layout=layout)
        return fig

    except Exception as e:
        print(e, "dissolution_curve")

def dissolution_curve_interval(reference_df, test_df):
    try:
        # Calculate statistics for reference
        ref_means = reference_df.iloc[:, 1:].mean(axis=1)
        ref_mins = reference_df.iloc[:, 1:].min(axis=1)
        ref_maxs = reference_df.iloc[:, 1:].max(axis=1)

        # Calculate statistics for test
        test_means = test_df.iloc[:, 1:].mean(axis=1)
        test_mins = test_df.iloc[:, 1:].min(axis=1)
        test_maxs = test_df.iloc[:, 1:].max(axis=1)

        # Time points
        times = reference_df.iloc[:, 0]

        # Reference trace with error bars
        ref_trace = go.Scatter(
            x=times,
            y=ref_means,
            mode='lines+markers',
            name='Reference',
            marker=dict(symbol='circle', size=8),
            line=dict(color='blue'),
            error_y=dict(
                type='data',
                symmetric=False,
                array=ref_maxs - ref_means,
                arrayminus=ref_means - ref_mins,
                thickness=1.5,
                width=3
            )
        )

        # Test trace with error bars
        test_trace = go.Scatter(
            x=times,
            y=test_means,
            mode='lines+markers',
            name='Test',
            marker=dict(symbol='square', size=8),
            line=dict(color='green', dash='dash'),
            error_y=dict(
                type='data',
                symmetric=False,
                array=test_maxs - test_means,
                arrayminus=test_means - test_mins,
                thickness=1.5,
                width=3
            )
        )

        # Layout
        layout = go.Layout(
            title='Dissolution Profiles with Variability Ranges',
            xaxis=dict(title='Time (minutes)', zeroline=True, zerolinewidth=1, zerolinecolor='black'),
            yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10,
                       zeroline=True, zerolinewidth=1, zerolinecolor='black',
                       showgrid=True, gridcolor='lightgray', range=[0, 100]),
            showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            plot_bgcolor='white',
            paper_bgcolor='white'
        )

        # Create figure
        fig = go.Figure(data=[ref_trace, test_trace], layout=layout)
        return fig

    except Exception as e:
        print(e, "dissolution_curve_interval")

def simulate_lilliefors_critical_value(n, alpha=0.05, num_simulations=1000):
    """Simulate Lilliefors critical value for confidence bands"""
    D_values = []
    for _ in range(num_simulations):
        sample = np.random.normal(loc=0, scale=1, size=n)
        sample_mean = np.mean(sample)
        sample_std = np.std(sample, ddof=1)
        z_scores = (sample - sample_mean) / sample_std
        z_scores.sort()
        ecdf_sim = np.arange(1, n + 1) / (n + 1)
        cdf = norm.cdf(z_scores)
        D = np.max(np.abs(ecdf_sim - cdf))
        D_values.append(D)
    return np.percentile(D_values, 100 * (1 - alpha))

def check_cv(df):
    try:
        if df.iloc[0, 0] == 0:  # Check if the first time point is zero
            cv_values = df.iloc[1:, 1:].std(axis=1) / df.iloc[1:, 1:].mean(axis=1) * 100  # Exclude the zero time point and Time Points column
            return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()
        else:
            cv_values = df.iloc[:, 1:].std(axis=1) / df.iloc[:, 1:].mean(axis=1) * 100 # Calculate CV for all time points
            #print(df)
            #print(cv_values)
            return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()
    except Exception as e:
        print(e,"check_cv")

# import numpy as np
# from scipy.stats import norm, skew, kurtosis
# from scipy import stats
# import plotly.graph_objects as go

# def create_jmp_style_qq_plot(data, title, method_name):
#     if len(data) == 0:
#         return None

#     n = len(data)
#     sorted_data = np.sort(data)
#     plotting_positions = norm.ppf((np.arange(1, n + 1)) / (n + 2))
#     slope, intercept, r, _, _ = stats.linregress(plotting_positions, sorted_data)
#     fit_line = intercept + slope * plotting_positions
#     ecdf = (np.arange(1, n + 1)) / (n + 1)

#     D_critical = simulate_lilliefors_critical_value(n=n, alpha=0.05)
#     lower_quantile = norm.ppf(np.clip(ecdf - D_critical, 1e-10, 1 - 1e-10))
#     upper_quantile = norm.ppf(np.clip(ecdf + D_critical, 1e-10, 1 - 1e-10))
#     lower_band = intercept + slope * lower_quantile
#     upper_band = intercept + slope * upper_quantile

#     fig = go.Figure()

#     fig.add_trace(go.Scatter(x=plotting_positions, y=upper_band, mode='lines',
#                              name='Upper Confidence Band', line=dict(color='red', width=1, dash='dot')))
#     fig.add_trace(go.Scatter(x=plotting_positions, y=lower_band, mode='lines',
#                              name='Lower Confidence Band', line=dict(color='red', width=1, dash='dot')))
#     fig.add_trace(go.Scatter(x=plotting_positions, y=fit_line, mode='lines',
#                              name='Normal Reference Line', line=dict(color='red', width=2)))
#     fig.add_trace(go.Scatter(x=plotting_positions, y=sorted_data, mode='markers',
#                              name='Data Points', marker=dict(color='black', size=6)))

#     x_range = max(plotting_positions) - min(plotting_positions)
#     y_range = max(sorted_data) - min(sorted_data)
#     x_margin = x_range * 0.1
#     y_margin = y_range * 0.1

#     x_ticks = [-2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]
#     x_ticks = [tick for tick in x_ticks if min(plotting_positions)-x_margin <= tick <= max(plotting_positions)+x_margin]

#     fig.update_layout(
#         title=dict(
#             text=f"Normal Quantile Plot",
#             x=0.5
#         ),
#         xaxis=dict(
#             title='Normal Quantile',
#             range=[min(plotting_positions)-x_margin, max(plotting_positions)+x_margin],
#             tickvals=x_ticks,
#             # tickmode='array',
#             # gridcolor='lightgray',
#             # gridwidth=0.5,
#             # showgrid=True,
#             # zeroline=True,
#             # zerolinecolor='lightgray',
#             # zerolinewidth=0.8,
#             # tickfont=dict(size=10, color='black', family='Arial'),
#             # linecolor='black',
#             # linewidth=1,
#             # mirror=True
#         ),

#         yaxis=dict(
#             title='Sample Quantile',
#             range=[min(sorted_data)-y_margin, max(sorted_data)+y_margin]
#         ),
#         plot_bgcolor='white',
#         paper_bgcolor='white',
#         hovermode='closest',
#         width=600,
#         height=500,
#         margin=dict(l=80, r=250, t=80, b=60),
#         showlegend=True,
#         legend=dict(
#             title=dict(text='Legend'),
#             orientation="v",
#             yanchor="top",
#             y=1,
#             xanchor="left",
#             x=1.05
#         )
#     )

#     return fig

# import numpy as np
# import plotly.graph_objects as go
# from scipy import stats
# from scipy.stats import norm

# def simulate_lilliefors_critical_value(n, alpha=0.05):
#     # Placeholder for critical value simulation
#     return 0.05  # Example fixed value




def create_jmp_style_qq_plot(data, title, method_name):
    if len(data) == 0:
        return None

    n = len(data)
    sorted_data = np.sort(data)
    plotting_positions = norm.ppf((np.arange(1, n + 1)) / (n + 2))
    slope, intercept, r, _, _ = stats.linregress(plotting_positions, sorted_data)
    fit_line = intercept + slope * plotting_positions
    ecdf = (np.arange(1, n + 1)) / (n + 1)

    D_critical = simulate_lilliefors_critical_value(n=n, alpha=0.05)
    lower_quantile = norm.ppf(np.clip(ecdf - D_critical, 1e-10, 1 - 1e-10))
    upper_quantile = norm.ppf(np.clip(ecdf + D_critical, 1e-10, 1 - 1e-10))
    lower_band = intercept + slope * lower_quantile
    upper_band = intercept + slope * upper_quantile


    fig = go.Figure()

    fig.add_trace(go.Scatter(x=plotting_positions, y=upper_band, mode='lines', line=dict(color='red', width=1, dash='dot'), showlegend=False))
    fig.add_trace(go.Scatter(x=plotting_positions, y=lower_band, mode='lines', line=dict(color='red', width=1, dash='dot'), showlegend=False))
    fig.add_trace(go.Scatter(x=plotting_positions, y=fit_line, mode='lines', line=dict(color='red', width=2), showlegend=False))
    fig.add_trace(go.Scatter(x=plotting_positions, y=sorted_data, mode='markers', marker=dict(color='black', size=6), showlegend=False))

    x_range = max(plotting_positions) - min(plotting_positions)
    y_range = max(sorted_data) - min(sorted_data)
    x_margin = x_range * 0.1 # Increased margin for more spacing
    y_margin = y_range * 0.1

    x_ticks = [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7]
    x_ticks = [tick for tick in x_ticks if min(plotting_positions)-x_margin <= tick <= max(plotting_positions)+x_margin]
    # x_ticks = [tick for tick in x_ticks if min(plotting_positions) <= tick <= max(plotting_positions)]

    fig.update_layout(
        title=dict(
            text="Normal Quantile Plot",
            x=0.5
        ),
        xaxis=dict(
           title=dict(
                text='<b>Normal Quantile</b>',
                font=dict(size=12, color='black', family='Arial')
            ),
            range=[min(plotting_positions)-x_margin, max(plotting_positions)+x_margin],
            tickvals=x_ticks,
            tickmode='array',
            gridcolor='lightgray',
            gridwidth=1.0,
            showgrid=True,
            zeroline=True,
            zerolinecolor='lightgray',
            zerolinewidth=0.3,
            tickfont=dict(size=10, color='black', family='Arial'),
            linecolor='black',
            linewidth=1,
            mirror=True
        ),
        yaxis=dict(
            title='Sample Quantile',
            range=[min(sorted_data)-y_margin, max(sorted_data)+y_margin],
            # gridcolor='lightgray', 
            # gridwidth=1,
            # showgrid=True,

        ),
        plot_bgcolor='white',
        paper_bgcolor='white',
        hovermode='closest',
        width=600, 
        height=500,
        margin=dict(l=40, r=10, t=40, b=60),
    )

    return fig




def generate_qq_plots(bootstrap_data):
    """Generate all QQ plots and return them as a dictionary."""
    qq_plots_data = {}
    # plots = {
    #     'individual': {},
    #     'combined': None
    # }
    
    # Generate individual plots
    for method, data in bootstrap_data.items():
        if len(data) > 0:
            fig = create_jmp_style_qq_plot(data, f"QQ Plot", method)
            plots = fig
    
    # Generate combined plot
    # combined_fig = create_combined_qq_plots(file_bootstrap_data, file_name)
    # plots['combined'] = combined_fig
    
        qq_plots_data[method] = plots
    
    return qq_plots_data
# --------------------------
# Validation Checks
# --------------------------
def check_time_points(df):
    """Check minimum time points requirement"""
    if df.iloc[0, 0] == 0 or str(df.iloc[0, 0]).lower() == '0':
        return len(df) - 1 >= 3
    return len(df) >= 3

def two_time_points(df):
    """Check if first two time points exceed 85%"""
    means = df.iloc[:, 1:].mean(axis=1)
    if len(means) >= 2:
        return means.iloc[0] <= 85 and means.iloc[1] <= 85
    return True

def min15_check(df):
    """Check if dissolution exceeds 85% within 15 minutes"""
    time_col = df.columns[0]
    if "min" in str(time_col).lower() or "minutes" in str(time_col).lower():
        early_times = df[df.iloc[:, 0] <= 15]
        if not early_times.empty:
            means = early_times.iloc[:, 1:].mean(axis=1)
            return any(means > 85)
    return False

def check_cv(df):
    """Check coefficient of variation requirements"""
    if df.iloc[0, 0] == 0 or str(df.iloc[0, 0]).lower() == '0':
        df = df.iloc[1:]
    
    cv_values = []
    for i in range(len(df)):
        row = df.iloc[i, 1:]
        mean = row.mean()
        std = row.std()
        cv = (std / mean) * 100 if mean > 0 else 0
        cv_values.append(cv)
    
    if len(cv_values) > 0:
        first_cv = cv_values[0]
        other_cv = cv_values[1:] if len(cv_values) > 1 else []
        return first_cv < 20 and all(cv < 10 for cv in other_cv)
    return False

def check_same_time_points(df1, df2):
    """Check if time points are the same for both products"""
    return df1.iloc[:, 0].equals(df2.iloc[:, 0])

def check_sample_units(df):
    """Check minimum sample units requirement"""
    return df.shape[1] - 1 >= 12

# --------------------------
# f2 Calculation Functions
# --------------------------
def row_variance(df):
    """Calculate row-wise variance"""
    return df.iloc[:, 1:].var(axis=1, ddof=1)

def conventional_f2(ref_means, test_means):
    """Calculate conventional f2"""
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    p = len(ref_means)
    res = 50 if p == 0 else 100 - 25 * np.log10(1 + (1/p) * sum_sq_diff)
    return res

def expected_f2(ref_df, test_df):
    """Calculate expected f2"""
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    
    # Conventional f2 component
    diff = test_means - ref_means
    sum_sq_diff = (diff ** 2).sum()
    
    # Variance components
    ref_var = row_variance(ref_df)
    test_var = row_variance(test_df)
    sum_var = (ref_var + test_var).sum()
    
    n = ref_df.shape[1] - 1  # Number of units per time point
    p = len(ref_means)
    
    adjustment = (1/n) * sum_var
    result = 100 - 25 * np.log10(1 + (1/p) * (sum_sq_diff + adjustment))
    return result

def bias_corrected_f2(ref_df, test_df):
    """Calculate bias-corrected f2"""
    try:
        ref_means = ref_df.iloc[:, 1:].mean(axis=1)
        test_means = test_df.iloc[:, 1:].mean(axis=1)

        diff = test_means - ref_means
        sum_sq_diff = (diff ** 2).sum()

        ref_var = row_variance(ref_df)
        test_var = row_variance(test_df)
        sum_var = (ref_var + test_var).sum()

        n = ref_df.shape[1] - 1
        p = len(ref_means)

        adjustment = (1 / n) * sum_var
        right_side = sum_sq_diff + p

        try:
            if adjustment < right_side:
                adjusted_diff = sum_sq_diff - adjustment
                if adjusted_diff > 0:
                    results = 100 - 25 * np.log10(1 + (1 / p) * adjusted_diff)
                    return results
                else:
                    return "Bias-corrected F2 could not be calculated due to high within-batch variability and minimal profile differences in the provided dataset, please proceed with alternative method such as Bootstrap F2 and BCa bootstrap F2"
            else:
                return "Bias-corrected F2 could not be calculated due to high within-batch variability and minimal profile differences in the provided dataset, please proceed with alternative method such as Bootstrap F2 and BCa bootstrap F2"
        except Exception as e:
            print(e,"IN EXCEPTION ERROR")

    except Exception:
        print("IN ELSE 3 RESULTS IN bias_corrected_f2")
        return None

def calculate_all_f2(ref_df, test_df):
    """Calculate all f2 metrics"""
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)

    expected_f2_result = expected_f2(ref_df, test_df)

    bias_corrected_f2_result = bias_corrected_f2(ref_df, test_df)
    
    return {
    # "Conventional": conventional_f2(ref_means, test_means),
        "Expected f2": f"{expected_f2_result:.2f}" if isinstance(expected_f2_result, float) else expected_f2_result,
        "Bias Corrected f2": f"{bias_corrected_f2_result:.2f}" if isinstance(bias_corrected_f2_result, float) else bias_corrected_f2_result
    }


# --------------------------
# Bootstrap Functions
# --------------------------
# import numpy as np
# from scipy.stats import skew, kurtosis
# from scipy import stats
# from joblib import Parallel, delayed
# import plotly.graph_objects as go

# def bootstrap_f2(ref_df, test_df, calc_func, n_iterations=10000, n_jobs=-1):
#     try:
#         n_ref_units = ref_df.shape[1] - 1
#         n_test_units = test_df.shape[1] - 1

#         def single_iteration():
#             ref_sample_idx = np.random.choice(range(1, ref_df.shape[1]), n_ref_units, replace=True)
#             test_sample_idx = np.random.choice(range(1, test_df.shape[1]), n_test_units, replace=True)
#             ref_sample = ref_df.iloc[:, [0] + list(ref_sample_idx)]
#             test_sample = test_df.iloc[:, [0] + list(test_sample_idx)]
#             original_f2 = calc_func(ref_sample, test_sample)
#             if original_f2 is not None and isinstance(original_f2, (int, float)):
#                 return original_f2
#             return None

#         f2_values = Parallel(n_jobs=n_jobs)(delayed(single_iteration)() for _ in range(n_iterations))
#         f2_values = [val for val in f2_values if val is not None]
#         f2_values = np.array(f2_values) if f2_values else calc_func(ref_df, test_df)

#         mean_f2 = np.mean(f2_values)
#         median_f2 = np.median(f2_values)
#         skewness_f2 = skew(f2_values)
#         kurtosis_f2 = kurtosis(f2_values)
#         lower_bound = np.percentile(f2_values, 5)
#         upper_bound = np.percentile(f2_values, 95)

#         return lower_bound, upper_bound, mean_f2, median_f2, skewness_f2, kurtosis_f2, f2_values

#     except Exception as e:
#         return None, None, None, None, None, None, []

# def bootstrap_f2(ref_df, test_df, calc_func, n_iterations=10000):
#     """
#     Bootstrap f2 calculation. Optimized version.

#     Args:
#         ref_df (pd.DataFrame): Reference DataFrame. First column is time, subsequent columns are units.
#         test_df (pd.DataFrame): Test DataFrame. First column is time, subsequent columns are units.
#         calc_func (callable): Function to calculate f2. It must accept two DataFrames
#                               (ref_sample, test_sample) and return a numeric f2 value.
#         n_iterations (int): Number of bootstrap iterations.

#     Returns:
#       original_f2 (float): f2 value calculated from the original DataFrames.
#       lower_bound (float or None): 5th percentile of bootstrapped f2s. None if no valid f2s.
#       upper_bound (float or None): 95th percentile of bootstrapped f2s. None if no valid f2s.
#       mean_f2 (float or None): Mean of bootstrapped f2s. None if no valid f2s.
#       median_f2 (float or None): Median of bootstrapped f2s. None if no valid f2s.
#       skewness_f2 (float or None): Skewness of bootstrapped f2s. None if no valid f2s.
#       kurtosis_f2 (float or None): Kurtosis of bootstrapped f2s. None if no valid f2s.
#       f2_values (np.array): Array of all valid bootstrapped f2 values. Empty if no valid f2s.
#       ref_mean_profiles (np.array): Array of mean profiles for reference samples
#                                     (shape [n_iterations, n_timepoints]).
#       test_mean_profiles (np.array): Array of mean profiles for test samples
#                                      (shape [n_iterations, n_timepoints]).
#     """
#     timepoints = ref_df.shape[0]
#     ref_units = ref_df.shape[1] - 1
#     test_units = test_df.shape[1] - 1

#     # Handle edge cases where there are no units to sample
#     if ref_units <= 0 or test_units <= 0:
#         # The original code returns calc_func(ref_df, test_df) directly.
#         # This implies calc_func should handle cases with empty or single-column DataFrames.
#         return calc_func(ref_df, test_df), None, None, None, None, None, None, np.array([]), np.array([]), np.array([])

#     # Calculate original f2 once
#     original_f2 = calc_func(ref_df, test_df)

#     # Pre-allocate arrays for results
#     f2_values = np.empty(n_iterations)
#     f2_values.fill(np.nan) # Initialize with NaN to easily filter invalid results

#     ref_mean_profiles = np.empty((n_iterations, timepoints))
#     test_mean_profiles = np.empty((n_iterations, timepoints))

#     # Pre-extract data into NumPy arrays and the time column DataFrame
#     # This avoids repeated .iloc and DataFrame slicing inside the loop
#     time_col_df = ref_df.iloc[:, [0]] # Keep as DataFrame for pd.concat later
#     ref_data_np = ref_df.iloc[:, 1:].values # NumPy array of shape (timepoints, ref_units)
#     test_data_np = test_df.iloc[:, 1:].values # NumPy array of shape (timepoints, test_units)

#     # Pre-generate all random choices for column indices
#     # This avoids repeated calls to np.random.choice inside the loop, reducing Python overhead.
#     all_ref_sample_indices = np.random.choice(ref_units, size=(n_iterations, ref_units), replace=True)
#     all_test_sample_indices = np.random.choice(test_units, size=(n_iterations, test_units), replace=True)

#     for i in range(n_iterations):
#         # Get the pre-generated 0-based column indices for the current iteration
#         ref_sample_col_indices = all_ref_sample_indices[i, :]
#         test_sample_col_indices = all_test_sample_indices[i, :]

#         # Select columns from the pre-extracted NumPy arrays (very fast)
#         ref_sample_data_np = ref_data_np[:, ref_sample_col_indices]
#         test_sample_data_np = test_data_np[:, test_sample_col_indices]

#         # Calculate mean profiles directly from NumPy arrays (very fast)
#         ref_mean_profiles[i] = ref_sample_data_np.mean(axis=1)
#         test_mean_profiles[i] = test_sample_data_np.mean(axis=1)

#         # Reconstruct Dataframes for calc_func. This is the most expensive part if calc_func
#         # strictly requires DataFrames. However, creating a DataFrame from a NumPy array
#         # and concatenating with a pre-existing time column is more efficient than
#         # repeated .iloc and .reset_index on the original DataFrames.
#         ref_sample_df = pd.concat([time_col_df, pd.DataFrame(ref_sample_data_np)], axis=1)
#         test_sample_df = pd.concat([time_col_df, pd.DataFrame(test_sample_data_np)], axis=1)

#         # Call the calculation function
#         f2_val = calc_func(ref_sample_df, test_sample_df)

#         # Store valid f2 values
#         if f2_val is not None and isinstance(f2_val, (int, float, np.floating, np.integer)):
#             f2_values[i] = f2_val

#     # Filter out NaN values (from iterations where calc_func might have returned None or non-numeric)
#     valid_f2 = f2_values[~np.isnan(f2_values)]

#     # Handle case where no valid f2 values were computed
#     if valid_f2.size == 0:
#         return original_f2, None, None, None, None, None, None, np.array([]), ref_mean_profiles, test_mean_profiles

#     # Calculate statistics from valid f2 values
#     lower_bound = np.percentile(valid_f2, 5)
#     upper_bound = np.percentile(valid_f2, 95)
#     mean_f2 = np.mean(valid_f2)
#     median_f2 = np.median(valid_f2)
#     skewness_f2 = skew(valid_f2)
#     kurtosis_f2 = kurtosis(valid_f2)

#     print(
#         original_f2,
#         lower_bound,
#         upper_bound,
#         mean_f2,
#         median_f2,
#         skewness_f2,
#         kurtosis_f2,
#         valid_f2,
#         ref_mean_profiles,
#         test_mean_profiles
#     )

#     return (
#         original_f2,
#         lower_bound,
#         upper_bound,
#         mean_f2,
#         median_f2,
#         skewness_f2,
#         kurtosis_f2,
#         valid_f2,
#         ref_mean_profiles,
#         test_mean_profiles
#     )

import numpy as np
import pandas as pd
from joblib import Parallel, delayed
from scipy.stats import skew, kurtosis

def bootstrap_f2(ref_df, test_df, calc_func, n_iterations=10000, n_jobs=-1, track_profiles=False):
    """
    Optimized and parallelized bootstrap f2 calculation.

    Args:
        ref_df (pd.DataFrame): Reference DataFrame. First column is time, rest are units.
        test_df (pd.DataFrame): Test DataFrame. First column is time, rest are units.
        calc_func (callable): Function to calculate f2. Accepts two DataFrames.
        n_iterations (int): Number of bootstrap iterations.
        n_jobs (int): Number of parallel jobs (-1 uses all cores).
        track_profiles (bool): Whether to return mean profiles for each iteration.

    Returns:
        original_f2 (float): f2 from original data.
        lower_bound (float): 5th percentile of bootstrapped f2s.
        upper_bound (float): 95th percentile of bootstrapped f2s.
        mean_f2 (float): Mean of bootstrapped f2s.
        median_f2 (float): Median of bootstrapped f2s.
        skewness_f2 (float): Skewness of bootstrapped f2s.
        kurtosis_f2 (float): Kurtosis of bootstrapped f2s.
        f2_values (np.array): Array of valid bootstrapped f2 values.
        ref_mean_profiles (np.array or None): Mean profiles of reference samples.
        test_mean_profiles (np.array or None): Mean profiles of test samples.
    """
    try:
        time_col = ref_df.iloc[:, [0]]
        ref_data = ref_df.iloc[:, 1:].values
        test_data = test_df.iloc[:, 1:].values
        n_ref_units = ref_data.shape[1]
        n_test_units = test_data.shape[1]
        timepoints = ref_data.shape[0]

        original_f2 = calc_func(ref_df, test_df)

        # Pre-generate indices for reproducibility and speed
        ref_indices = np.random.randint(0, n_ref_units, size=(n_iterations, n_ref_units))
        test_indices = np.random.randint(0, n_test_units, size=(n_iterations, n_test_units))

        def compute_f2(i):
            ref_sample = ref_data[:, ref_indices[i]]
            test_sample = test_data[:, test_indices[i]]

            ref_mean = ref_sample.mean(axis=1) if track_profiles else None
            test_mean = test_sample.mean(axis=1) if track_profiles else None

            ref_df_sample = pd.concat([time_col, pd.DataFrame(ref_sample)], axis=1)
            test_df_sample = pd.concat([time_col, pd.DataFrame(test_sample)], axis=1)

            f2 = calc_func(ref_df_sample, test_df_sample)
            if isinstance(f2, (int, float, np.integer, np.floating)):
                return f2, ref_mean, test_mean
            return None, None, None

        # Run parallel jobs
        results = Parallel(n_jobs=n_jobs)(delayed(compute_f2)(i) for i in range(n_iterations))

        # Collect results
        f2_values = []
        ref_profiles_list = []
        test_profiles_list = []

        for f2, ref_mean, test_mean in results:
            if f2 is not None:
                f2_values.append(f2)
                if track_profiles:
                    ref_profiles_list.append(ref_mean)
                    test_profiles_list.append(test_mean)

        f2_values = np.array(f2_values)
        ref_profiles = np.array(ref_profiles_list) if track_profiles and ref_profiles_list else None
        test_profiles = np.array(test_profiles_list) if track_profiles and test_profiles_list else None

        if f2_values.size == 0:
            return original_f2, None, None, None, None, None, None, np.array([]), ref_profiles, test_profiles

        return (
            original_f2,
            np.percentile(f2_values, 5),
            np.percentile(f2_values, 95),
            np.mean(f2_values),
            np.median(f2_values),
            skew(f2_values),
            kurtosis(f2_values),
            f2_values,
            ref_profiles,
            test_profiles
        )

    except Exception as e:
        print(f"Error during bootstrap: {e}")
        return (
            None, None, None, None, None, None, None, np.array([]), None, None
        )



    
# ----------------------------------------
# Create Bootstrap profile plot (intervals) and save HTML
# ----------------------------------------

# def create_bootstrap_profile_plot(times, ref_profiles, test_profiles):
#     """
#     Create a plot showing median and 5-95 percentile bands from bootstrap profiles,
#     plus mean lines with SD error bars (caps) to match the user's example picture.
#     ref_profiles/test_profiles: np.array shape (n_iterations, n_timepoints)
#     times: array-like length n_timepoints
#     """
#     if ref_profiles.size == 0 or test_profiles.size == 0:
#         return None

#     # compute percentiles per timepoint
#     ref_p05 = np.percentile(ref_profiles, 5, axis=0)
#     ref_p50 = np.percentile(ref_profiles, 50, axis=0)
#     ref_p95 = np.percentile(ref_profiles, 95, axis=0)

#     test_p05 = np.percentile(test_profiles, 5, axis=0)
#     test_p50 = np.percentile(test_profiles, 50, axis=0)
#     test_p95 = np.percentile(test_profiles, 95, axis=0)

#     # compute mean and sd (for error bars like in example)
#     ref_mean = np.mean(ref_profiles, axis=0)
#     ref_sd = np.std(ref_profiles, axis=0, ddof=1)
#     test_mean = np.mean(test_profiles, axis=0)
#     test_sd = np.std(test_profiles, axis=0, ddof=1)

#     fig = go.Figure()

#     # Reference: shaded 5-95 band (behind)
#     fig.add_trace(go.Scatter(
#         x=np.concatenate([times, times[::-1]]),
#         y=np.concatenate([ref_p95, ref_p05[::-1]]),
#         fill='toself', fillcolor='rgba(52,152,219,0.14)', line=dict(color='rgba(255,255,255,0)'), hoverinfo='skip', showlegend=False, name='Ref 5-95%'
#     ))
#     # Reference median
#     fig.add_trace(go.Scatter(x=times, y=ref_p50, mode='lines', name='Ref median', line=dict(color='#3498DB', width=2), hoverinfo='skip'))

#     # Reference mean with error bars (SD) and markers (showcaps)
#     fig.add_trace(go.Scatter(
#         x=times,
#         y=ref_mean,
#         mode='markers+lines',
#         name='Ref mean ¬±SD',
#         marker=dict(color='#2A6FBB', size=8, symbol='circle'),
#         line=dict(color='#2A6FBB', width=1, dash='solid'),
#         error_y=dict(type='data', array=ref_sd, visible=True, thickness=1.5, width=6)
#     ))

#     # Test: shaded 5-95 band (behind)
#     fig.add_trace(go.Scatter(
#         x=np.concatenate([times, times[::-1]]),
#         y=np.concatenate([test_p95, test_p05[::-1]]),
#         fill='toself', fillcolor='rgba(231,76,60,0.12)', line=dict(color='rgba(255,255,255,0)'), hoverinfo='skip', showlegend=False, name='Test 5-95%'
#     ))
#     # Test median
#     fig.add_trace(go.Scatter(x=times, y=test_p50, mode='lines', name='Test median', line=dict(color='#E74C3C', width=2), hoverinfo='skip'))

#     # Test mean with error bars (SD) and square markers
#     fig.add_trace(go.Scatter(
#         x=times,
#         y=test_mean,
#         mode='markers+lines',
#         name='Test mean ¬±SD',
#         marker=dict(color='#C0392B', size=8, symbol='square'),
#         line=dict(color='#C0392B', width=1, dash='solid'),
#         error_y=dict(type='data', array=test_sd, visible=True, thickness=1.5, width=6)
#     ))

#     # Styling to match the image
#     # Determine y-range with margin
#     all_vals = np.concatenate([ref_p95, test_p95, ref_p05, test_p05])
#     ymin = max(0, np.min(all_vals) - 5)
#     ymax = max(120, np.max(all_vals) + 8)  # keep a top margin and minimum top (120)
#     fig.update_layout(
#         xaxis_title="Time (min)",
#         yaxis_title="Fraction dissolved (%)",
#         legend=dict(orientation="v", yanchor="top", y=0.98, xanchor="left", x=0.02),
#         plot_bgcolor='#faf6ef',  # slightly creamy
#         paper_bgcolor='#e6e6e6',
#         hovermode='closest',
#         margin=dict(l=100, r=10, t=10, b=10),
#         width=900,
#         height=500,
#         yaxis=dict(range=[ymin, ymax], tick0=0, dtick=10),
#         xaxis=dict(tickmode='array', tickvals=list(times))
#     )

#     # gridlines, framed border
#     fig.update_xaxes(showgrid=True, gridcolor='lightgray', zeroline=False, linecolor='black', mirror=True)
#     fig.update_yaxes(showgrid=True, gridcolor='lightgray', zeroline=False, linecolor='black', mirror=True)

#     # Add small styling to markers to emulate the example's tiny horizontal caps on error bars
#     # (plotly error bars already show caps controlled by 'width'; we set width=6 above)
#     # Add annotation describing percentiles
#     fig.add_annotation(x=1.02, y=0.98, xref='paper', yref='paper', text="Band: 5‚Äì95% (bootstrap)<br>Markers: bootstrap mean ¬± SD", showarrow=False, align='left', bgcolor='white', bordercolor='black', borderwidth=1)

#     return fig


def create_bootstrap_profile_plot(times, ref_profiles, test_profiles):
    """
    Create a plot showing mean lines with 5-95 percentile error bars (caps)
    to match the user's example picture.
    ref_profiles/test_profiles: np.array shape (n_iterations, n_timepoints)
    times: array-like length n_timepoints
    """
    max_time = max(times)

    if max_time > 60:
        xticks = list(range(0, int(max_time) + 11, 10))  # +11 ensures the last tick covers max_time
    elif max_time > 30: # This covers 30 < max_time <= 60
        xticks = list(range(0, int(max_time) + 6, 5))    # +6 ensures the last tick covers max_time
    elif max_time > 12: # This covers 12 < max_time <= 30 (a reasonable interval for this unspecified range)
        xticks = list(range(0, int(max_time) + 3, 2.5))    # +6 ensures the last tick covers max_time
    
    else:
        xticks = list(range(0, int(max_time) + 1, 1))    # +6 ensures the last tick covers max_time

        if ref_profiles.size == 0 or test_profiles.size == 0:
            return None
        


    # compute percentiles per timepoint (will be used for error bar ranges)
    ref_p05 = np.percentile(ref_profiles, 5, axis=0)
    ref_p95 = np.percentile(ref_profiles, 95, axis=0)

    test_p05 = np.percentile(test_profiles, 5, axis=0)
    test_p95 = np.percentile(test_profiles, 95, axis=0)

    # compute mean (for the main line and center of error bars)
    ref_mean = np.mean(ref_profiles, axis=0)
    test_mean = np.mean(test_profiles, axis=0)

    fig = go.Figure()

    # Reference mean with 5-95 percentile error bars
    fig.add_trace(go.Scatter(
        x=times,
        y=ref_mean,
        mode='lines+markers',
        name='Reference Mean',
        marker=dict(color='blue', symbol='circle'),
        line=dict(color='blue', dash='solid'),
        error_y=dict(
            type='data',
            symmetric=False,
            array=ref_p95 - ref_mean,  # Upper error: 95th percentile - mean
            arrayminus=ref_mean - ref_p05, # Lower error: mean - 5th percentile
            visible=True,
            thickness=1.5,
            width=6, # Caps width
            color='blue'
        )
    ))

    # Test mean with 5-95 percentile error bars
    fig.add_trace(go.Scatter(
        x=times,
        y=test_mean,
        mode='lines+markers',
        name='Test Mean',
        marker=dict(color='red', symbol='circle'),
        line=dict(color='red', dash='solid'),
        error_y=dict(
            type='data',
            symmetric=False,
            array=test_p95 - test_mean,  # Upper error: 95th percentile - mean
            arrayminus=test_mean - test_p05, # Lower error: mean - 5th percentile
            visible=True,
            thickness=1.5,
            width=6, # Caps width
            color='green'
        )
    ))

    # Styling to match the example image
    # Determine y-range with margin
    all_vals = np.concatenate([ref_p95, test_p95, ref_p05, test_p05])
    ymin = max(0, np.min(all_vals) - 5)
    ymax = max(120, np.max(all_vals) + 8) # Ensure a minimum top and margin

    fig.update_layout(
        title='Dissolution Curves with Intervals',
        xaxis=dict(
            title='Time (min)',
            tickmode='array',
            tickvals=xticks,
            showgrid=False,
            gridcolor='lightgray',
            showline=False,  # Removes axis line
            zeroline=False   # Removes zero line
        ),

        yaxis=dict(
            title='Fraction dissolved (%)',
            tickmode='linear', dtick=10,
            zeroline=False, zerolinewidth=1, zerolinecolor='black',
            showgrid=True, gridcolor='lightgray',
            range=[ymin, ymax]
        ),
        showlegend=True,
        legend=dict(
            orientation="h", # Horizontal legend
            yanchor="top", y=-0.15, # Position below the plot area
            xanchor="center", x=0.5
        ),
        plot_bgcolor='white', # Set plot background color to white
        paper_bgcolor='white', # Set paper background color to white
        hovermode='closest',
        margin=dict(b=80), # Adjust margins, especially bottom for legend
    )

    return fig

def trim_by_85_rule(ref_df, test_df, market):
    """
    Trim timepoints according to market rules for >85% truncation.
    """
    times = pd.to_numeric(ref_df.iloc[:, 0], errors='coerce')
    ref_means = ref_df.iloc[:, 1:].mean(axis=1)
    test_means = test_df.iloc[:, 1:].mean(axis=1)
    both_mask = (ref_means >= 85) & (test_means >= 85)
    either_mask = (ref_means >= 85) | (test_means >= 85)
    idx_both = np.where(both_mask)[0]
    idx_either = np.where(either_mask)[0]
    trim_idx = None
    m = (market or "").strip().upper()
    if m in ['US FDA', 'FDA']:
        idx_time15 = np.where(times <= 15)[0]
        last_idx_time15 = idx_time15[-1] if len(idx_time15) > 0 else None
        if len(idx_both) > 0:
            first_both = int(idx_both[0])
            if last_idx_time15 is not None:
                trim_idx = min(first_both, last_idx_time15)
            else:
                trim_idx = first_both
        else:
            trim_idx = last_idx_time15
    elif m == 'ANVISA':
        if len(idx_both) > 0:
            trim_idx = int(idx_both[0])
        else:
            trim_idx = None
    elif m in ['EMA/ICH/Canada/Australia', 'CHINA', 'ASEAN']:
        if len(idx_either) > 0:
            trim_idx = int(idx_either[0])
        else:
            trim_idx = None
    else:
        trim_idx = None
    if trim_idx is None:
        return ref_df.copy().reset_index(drop=True), test_df.copy().reset_index(drop=True)
    else:
        ref_trim = ref_df.iloc[: trim_idx + 1].reset_index(drop=True)
        test_trim = test_df.iloc[: trim_idx + 1].reset_index(drop=True)
        return ref_trim, test_trim

def calc_conventional_f2_for_market(ref_df, test_df, market):
    r_trim, t_trim = trim_by_85_rule(ref_df, test_df, market)
    if r_trim.shape[0] == 0:
        return None
    ref_means = r_trim.iloc[:, 1:].mean(axis=1)
    test_means = t_trim.iloc[:, 1:].mean(axis=1)
    return conventional_f2(ref_means, test_means)

def calc_expected_f2_for_market(ref_df, test_df, market):
    r_trim, t_trim = trim_by_85_rule(ref_df, test_df, market)
    return expected_f2(r_trim, t_trim)

def calc_bias_corrected_f2_for_market(ref_df, test_df, market):
    r_trim, t_trim = trim_by_85_rule(ref_df, test_df, market)
    return bias_corrected_f2(r_trim, t_trim)
# --------------------------
# Streamlit App
# --------------------------
def main_f2(reference_df, test_df, selected_methodology, conv_result, market):
    print("IN MAIN F2")
    print(selected_methodology,"SELECTED METHODOLOGY IN MAIN F2")
    try:
        checks = {
            "Minimum 3 time points (excl. zero)": 
                [check_time_points(reference_df), check_time_points(test_df)],
            "First two points ‚â§85% dissolution": 
                [two_time_points(reference_df), two_time_points(test_df)],
            "No >85% dissolution in first 15 min": 
                [not min15_check(reference_df), not min15_check(test_df)],
            "CV requirements met": 
                [check_cv(reference_df), check_cv(test_df)],
            "‚â•12 individual units": 
                [check_sample_units(reference_df), check_sample_units(test_df)],
            "Same time points for both products": 
                [check_same_time_points(reference_df, test_df)]
        }
        
        # Display check results
        check_df = pd.DataFrame(
            checks.values(),
            index=checks.keys(),
            columns=["Reference", "Test"]
        )
        
        # Show dissolution profiles
        # diss_plot_f2 = dissolution_curve(reference_df, test_df)
        # diss_int_plot_f2 = dissolution_curve_interval(reference_df, test_df)
        

        
        # Agency recommendations
        # recommendations = {
        #     "US FDA": ["Conventional f2", "F2 Conventional Bootstrap"],
        #     "EMA/ICH/Canada/Australia": ["Expected f2", "Expected bootstrap f2"],
        #     "China": ["Conventional f2", "Expected f2", "Bias Corrected f2"],
        #     "ASEAN": ["Conventional f2", "Expected f2", "Bias Corrected f2"],
        #     "ANVISA": ["Conventional f2", "Expected f2", "Bias Corrected f2"],
        #     "Other": ["All Methods"]
        # }
        
        
        # Set defaults based on agency
        # if agency == "US FDA":
        #     default = ["F2 Conventional Bootstrap",]
        # elif agency == "EMA/ICH/Canada/Australia":
        #     default = ["Expected f2", "Expected bootstrap f2"]
        # else:
        #     default = ["Conventional f2", "Expected f2", "Bias Corrected f2"]
        
        
        # Prepare data - remove time zero
        ref_clean, test_clean = prepare_data(reference_df, test_df)
        selected_methods = selected_methodology
        results = {}
        
        # Point estimates
        if any(m in selected_methods for m in ["Expected F2", "Bias-Corrected F2", "Expected Bootstrap F2", "Bias-Corrected Bootstrap F2"]):
            f2_estimates = calculate_all_f2(ref_clean, test_clean)
            
            if "Bias-Corrected F2" in selected_methods or "Bias-Corrected Bootstrap F2" in selected_methods:
                print("IN BIAS-CORRECTED F2", selected_methods)
                results["Bias-Corrected F2"] = f2_estimates["Bias Corrected f2"]
                print("OUT BIAS-CORRECTED F2", selected_methods)

                    
                # results["Expected f2 plots"] = {
                #     "Diss_expected_f2": diss_plot_f2,
                #     "Diss_int_expected_f2": diss_int_plot_f2
                # } 
            
            if "Expected F2" in selected_methods or "Expected Bootstrap F2" in selected_methods:
                cv_result = check_cv(reference_df) and check_cv(test_df)

                if cv_result == True:
                    cv_results = "True"
                else:
                    cv_results = "False"
                print("IN EXPECTED F2", selected_methods)
                results["Expected F2"] = f2_estimates["Expected f2"]
                results["cv_result_expected_f2"]=cv_results
                print("OUT EXPECTED F2", selected_methods)

                # results["Bias Corrected f2 plots"] = {
                #     "Diss_bias_corrected_f2": diss_plot_f2,
                #     "Diss_int_bias_corrected_f2": diss_int_plot_f2
                # } 
        
        # Bootstrap calculations
        bootstrap_results = {}
        boostrap_f2_values = {}
        bootstrap_profile_matrices = {}
        n_iterations = 10000  # Reduced for demo, use 10000 for production


        if "Conventional Bootstrap F2" in selected_methods:
            def conv_func(r, t): 
                result_conv = calc_conventional_f2_for_market(r.reset_index(drop=True), t.reset_index(drop=True), market)
                return result_conv
            org, lower, upper, mean, median, skewness, kurt, f2_values, ref_profiles, test_profiles = bootstrap_f2(ref_clean, test_clean, conv_func, n_iterations=10000, n_jobs=-1, track_profiles=True)
            times = ref_clean.iloc[:, 0].to_numpy()
            fig = create_bootstrap_profile_plot(times, ref_profiles, test_profiles)
            bootstrap_results["Conventional Bootstrap F2"] = {
                "Observed F2":conv_result if conv_result else None,
                "Confidence Interval": f"{lower:.2f} - {upper:.2f}" if lower and upper else None,
                "Mean": str(mean),
                "Median": str(median),
                "Skewness": str(skewness),
                "Kurtosis": str(kurt),
                "Hist Plot":fig,
                "Is 5% percentile > 50": "Yes" if float(lower) > 50 else "No",
                "Similarity of R and T": "Accept" if float(lower) > 50 else "Reject",
            }
            boostrap_f2_values['Conventional Bootstrap F2'] = f2_values

            bootstrap_profile_matrices["Conventional Bootstrap F2"] = {
                'ref_profiles': ref_profiles,
                'test_profiles': test_profiles,
                'times': ref_clean.iloc[:, 0].to_numpy()
            }

        if "Expected Bootstrap F2" in selected_methods:
            def exp_func(r, t):
                return_value = calc_expected_f2_for_market(r.reset_index(drop=True), t.reset_index(drop=True), market)
                return return_value
            org, lower, upper, mean, median, skewness, kurt, f2_values, ref_profiles, test_profiles = bootstrap_f2(ref_clean, test_clean, exp_func, n_iterations=10000, n_jobs=-1, track_profiles=True)
            times = ref_clean.iloc[:, 0].to_numpy()
            fig = create_bootstrap_profile_plot(times, ref_profiles, test_profiles)
            bootstrap_results["Expected Bootstrap F2"] = {
                'Observed F2':results.get("Expected F2",None),
                "Confidence Interval": f"{lower:.2f} - {upper:.2f}" if lower and upper else None,
                "Mean": str(mean),
                "Median": str(median),
                "Skewness": str(skewness),
                "Kurtosis": str(kurt),
                "Hist Plot":fig,
                "Is 5% percentile > 50": "Yes" if float(lower) > 50 else "No",
                "Similarity of R and T": "Accept" if float(lower) > 50 else "Reject",
                # "QQ Plot":qq_plot
            }
            boostrap_f2_values['Expected Bootstrap F2'] = f2_values

            bootstrap_profile_matrices["Expected Bootstrap F2"] = {
                'ref_profiles': ref_profiles,
                'test_profiles': test_profiles,
                'times': ref_clean.iloc[:, 0].to_numpy()
            }

        if "Bias-Corrected Bootstrap F2" in selected_methods:
            def bc_func(r, t): 
                return_value = calc_bias_corrected_f2_for_market(r.reset_index(drop=True), t.reset_index(drop=True), market)
                return return_value
                # bc = bias_corrected_f2(r, t)
                # return bc if isinstance(bc, float) else None
            times = ref_clean.iloc[:, 0].to_numpy()
            org, lower, upper, mean, median, skewness, kurt, f2_values, ref_profiles, test_profiles = bootstrap_f2(ref_clean, test_clean, bc_func, n_iterations=10000, n_jobs=-1, track_profiles=True)
            fig = create_bootstrap_profile_plot(times, ref_profiles, test_profiles)
            bootstrap_results["Bias-Corrected Bootstrap F2"] = {
                "Observed F2": results.get("Bias-Corrected F2", "N/A") if results.get("Bias-Corrected F2", "") != "Bias-corrected F2 could not be calculated due to high within-batch variability and minimal profile differences in the provided dataset, please proceed with alternative method such as Bootstrap F2 and BCa bootstrap F2" else "N/A",
                "Confidence Interval": f"{lower:.2f} - {upper:.2f}" if lower and upper else None,
                "Mean": str(mean),
                "Median": str(median),
                "Skewness": str(skewness),
                "Kurtosis": str(kurt),
                "Hist Plot":fig,
                "Is 5% percentile > 50": "Yes" if float(lower) > 50 else "No",
                "Similarity of R and T": "Accept" if float(lower) > 50 else "Reject",
                # "QQ Plot":qq_plot
            }
            boostrap_f2_values['Bias-Corrected Bootstrap F2'] = f2_values

            bootstrap_profile_matrices["Bias-Corrected Bootstrap F2"] = {
                'ref_profiles': ref_profiles,
                'test_profiles': test_profiles,
                'times': times
            }

        if boostrap_f2_values:
            qq_plots = generate_qq_plots(boostrap_f2_values)

        else:
            qq_plots = []
        
        
        # Merge bootstrap results
        # for method, (orig, lower, upper) in bootstrap_results.items():
        #     if method in results:
        #         results[method]["ci"] = (lower, upper)
        #     else:
        #         results[method] = {
        #             "value": orig,
        #             "ci": (lower, upper)
        #         }
        
        # Display results
        # st.subheader("üìä Results")
        
        # for method, data in results.items():
        #     st.markdown(f"##### {method} f2")
        #     col1, col2 = st.columns([1, 3])
            
        #     with col1:
        #         if isinstance(data["value"], str):
        #             st.metric("Value", data["value"])
        #         else:
        #             st.metric("Value", f"{data['value']:.2f}")
                
        #         if data["ci"] and all(x is not None for x in data["ci"]):
        #             st.metric("90% Confidence Interval", 
        #                         f"{data['ci'][0]:.2f} - {data['ci'][1]:.2f}")
        
        
        # Similarity assessment
        # st.subheader("üìù Similarity Assessment")
        # conv_f2 = results.get("Conventional", {}).get("value", 0)
        
        # if isinstance(conv_f2, (int, float)):
        #     if conv_f2 >= 50:
        #         st.success("‚úÖ Profiles are similar (f2 ‚â• 50)")
        #     else:
        #         st.error("‚ùå Profiles are NOT similar (f2 < 50)")
        #     st.info("**Note:** Similarity assessment based on conventional f2 ‚â• 50")
        # else:
        #     st.warning("‚ö†Ô∏è Conventional f2 not available for similarity assessment")

        return results, bootstrap_results, qq_plots
        
    except Exception as e:
        print(e,"ERROR")
   

# if __name__ == "__main__":
#     main()
###################################
#Recommended  time points
# batch_similarity_analyzer_with_bootstrap_interval_profiles_styled.py
import os
import math
# import streamlit as st
import pandas as pd
import numpy as np
from zipfile import ZipFile
# from scipy.stats import skew, kurtosis, norm, probplot
from scipy import stats
import plotly.graph_objects as go
# from plotly.subplots import make_subplots

# ----------------------------------------
# Required Helper Functions (improved)
# ----------------------------------------


        
def compute_cv_per_timepoint(df):
    """Compute CV (%) per timepoint for a dataframe (columns: time, unit1, unit2, ...)."""
    if df.shape[1] <= 1:
        return pd.Series([np.nan]*df.shape[0])
    means = df.iloc[:, 1:].mean(axis=1)
    stds = df.iloc[:, 1:].std(axis=1, ddof=1)
    with np.errstate(divide='ignore', invalid='ignore'):
        cvs = (stds / means) * 100.0
    cvs = cvs.replace([np.inf, -np.inf], np.nan)
    return cvs.fillna(999.0)

def check_cv_criteria(ref_df, test_df, markets):

    if markets == "EMA/ICH/Canada/Australia":
        markets = "EMA"
    
    ref_df_check = combined_check_cv(ref_df, market=markets) 
    test_df_check = combined_check_cv(test_df, market=markets)

    m = (markets or "").strip().upper()
    if ref_df_check and test_df_check:
        if m in ['EMA', 'CHINA', 'ASEAN', 'WHO']:
            return ['Expected F2']
        elif m in ['US FDA', 'FDA', 'ANVISA']:
            return ['Conventional F2']
        else:
            return ['Conventional F2', 'Expected F2']
    else:
        if m in ['EMA', 'CHINA', 'ASEAN','WHO']:
            return ['Expected Bootstrap F2']
        elif m in ['US FDA', 'FDA', 'ANVISA']:
            return ['Conventional Bootstrap F2']
        else:
            return ['Conventional Bootstrap F2', 'Expected Bootstrap F2']



# def check_cv_criteria(ref_df, test_df, market):
#     if market == "EMA/ICH/Canada/Australia":
#         market = "EMA"
#     """
#     Return True if CV criteria PASS for given market.
#     """

#     if ref_df.shape[0] > 0 and (ref_df.iloc[0, 0] == 0 or ref_df.iloc[0, 0] == '0'):
#         ref_df = ref_df.iloc[1:].reset_index(drop=True)
#     if test_df.shape[0] > 0 and (test_df.iloc[0, 0] == 0 or test_df.iloc[0, 0] == '0'):
#         test_df = test_df.iloc[1:].reset_index(drop=True)
#     # return reference_df, test_df
#     m = (market or "").strip().upper()
#     if ref_df.shape[0] == 0:
#         return False
#     times = pd.to_numeric(ref_df.iloc[:, 0], errors='coerce')
#     if times.isnull().all():
#         return False
#     ref_cvs = compute_cv_per_timepoint(ref_df)
#     test_cvs = compute_cv_per_timepoint(test_df)
#     pass_criteria = True
#     if m in ['EMA','CHINA','ASEAN', 'WHO']:
#         early_mask = times <= 10
#         for i in range(len(times)):
#             if early_mask.iloc[i]:
#                 if ref_cvs.iloc[i] > 20 or test_cvs.iloc[i] > 20:
#                     pass_criteria = False
#                     break
#             else:
#                 if ref_cvs.iloc[i] > 10 or test_cvs.iloc[i] > 10:
#                     pass_criteria = False
#                     break
#     elif m in ['US FDA', 'FDA']:
#         early_mask = times <= 15
#         for i in range(len(times)):
#             if early_mask.iloc[i]:
#                 if ref_cvs.iloc[i] > 20 or test_cvs.iloc[i] > 20:
#                     pass_criteria = False
#                     break
#             else:
#                 if ref_cvs.iloc[i] > 10 or test_cvs.iloc[i] > 10:
#                     pass_criteria = False
#                     break
#     elif m == 'ANVISA':
#         p = len(times)
#         num_early = max(1, int(math.ceil(0.4 * p)))
#         for i in range(len(times)):
#             if i < num_early:
#                 if ref_cvs.iloc[i] > 20 or test_cvs.iloc[i] > 20:
#                     pass_criteria = False
#                     break
#             else:
#                 if ref_cvs.iloc[i] > 10 or test_cvs.iloc[i] > 10:
#                     pass_criteria = False
#                     break
#     else:
#         pass_criteria = False


#     if pass_criteria:
#         if m in ['EMA', 'CHINA', 'ASEAN', 'WHO']:
#             return ['Expected F2']
#         elif m in ['US FDA', 'FDA', 'ANVISA']:
#             return ['Conventional F2']
#         else:
#             return ['Conventional F2', 'Expected F2']
#     else:
#         if m in ['EMA', 'CHINA', 'ASEAN','WHO']:
#             return ['Expected Bootstrap F2']
#         elif m in ['US FDA', 'FDA', 'ANVISA']:
#             return ['Conventional Bootstrap F2']
#         else:
#             return ['Conventional Bootstrap F2', 'Expected Bootstrap F2']



def combined_check_cv(df, market=None):
    """
    Combine basic and market-specific CV checking rules.
    Returns True if CV criteria PASS for given market; False otherwise.

    CRITICAL: This function MUST match check_cv() from sa_main.py (line 413-430)
    to ensure consistent pass/fail results for methodology recommendations.
    """
    try:
        # Handle empty or malformed data upfront
        if df.shape[0] == 0 or df.shape[1] <= 1:
            return False

        # CRITICAL FIX: Remove time zero FIRST (matching check_cv behavior)
        if df.iloc[0, 0] == 0 or str(df.iloc[0, 0]).lower() == '0':
            df = df.iloc[1:].reset_index(drop=True)

        # Re-check if we still have data after removing time zero
        if df.shape[0] == 0:
            return False

        # Convert time column to numeric (handles invalid formats or missing values)
        times = pd.to_numeric(df.iloc[:, 0], errors='coerce')
        if times.isnull().all():
            return False

        # Compute CV values per time point (now on data WITHOUT time zero)
        means = df.iloc[:, 1:].mean(axis=1)
        stds = df.iloc[:, 1:].std(axis=1, ddof=1)
        with np.errstate(divide='ignore', invalid='ignore'):
            cv_values = (stds / means) * 100.0
        cv_values = cv_values.replace([np.inf, -np.inf], np.nan).fillna(999.0)

        # Simple CV rules (Fixed 20% for first time-point, 10% for all subsequent time-points)
        # This EXACTLY matches check_cv() logic: first_cv < 20 and all(cv < 10 for cv in other_cv)
        # Note: We check >= 20 (not < 20), so this line FAILS if CV is >= 20
        if cv_values.iloc[0] >= 20 or not (cv_values.iloc[1:] < 10).all():
            return False

        # Add market-specific rules for more advanced checks
        # These rules are ADDITIONAL to the basic check above
        pass_criteria = True
        m = (market or "").strip().upper()
        if m in ['EMA','CHINA','ASEAN', 'WHO']:
            early_mask = times <= 10
            for i in range(len(times)):
                if early_mask.iloc[i]:
                    if cv_values.iloc[i] >= 20:  # FIXED: Changed > to >= for consistency
                        pass_criteria = False
                        break
                else:
                    if cv_values.iloc[i] >= 10:  # FIXED: Changed > to >= for consistency
                        pass_criteria = False
                        break
        elif m in ['US FDA', 'FDA']:
            early_mask = times <= 15
            for i in range(len(times)):
                if early_mask.iloc[i]:
                    if cv_values.iloc[i] >= 20:  # FIXED: Changed > to >= for consistency
                        pass_criteria = False
                        break
                else:
                    if cv_values.iloc[i] >= 10:  # FIXED: Changed > to >= for consistency
                        pass_criteria = False
                        break
        elif m == 'ANVISA':
            p = len(times)
            num_early = max(1, int(math.ceil(0.4 * p)))
            for i in range(len(times)):
                if i < num_early:
                    if cv_values.iloc[i] >= 20:  # FIXED: Changed > to >= for consistency
                        pass_criteria = False
                        break
                else:
                    if cv_values.iloc[i] >= 10:  # FIXED: Changed > to >= for consistency
                        pass_criteria = False
                        break
        else:
            pass_criteria = False  # Unsupported or unspecified market
 
        # Return combined result
        return pass_criteria
 
    except Exception as e:
        print(f"Error in combined_check_cv: {e}")
        return False

############################
#SA_Final_Version
import itertools
import numpy as np
import pandas as pd

from scipy.stats import norm
from sklearn.utils import resample

import matplotlib.pyplot as plt



import seaborn as sns
import scipy.stats as stats
# import rpy2
# import rpy2.robjects as robjects
# from rpy2.robjects.packages import importr, data

import math

import warnings

import plotly.graph_objs as go
import plotly.io as pio
from matplotlib.ticker import MaxNLocator
# utils = importr('utils')
# base = importr('base')
# bootf2 = importr('bootf2')
# read = importr('readxl')
# open = importr('writexl')
warnings.filterwarnings('ignore')

# file_path= "File3.xlsx"
# reference_df =pd.read_excel(file_path, sheet_name=0)
# test_df=pd.read_excel(file_path, sheet_name=1)

from itertools import combinations
from scipy.interpolate import interp1d
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel
arrayboot=[]

np.random.seed(306)


def dissolution_curve_interval(reference_df, test_df):
    try:
        # Calculate means, maxs, and mins for reference data
        ref_data_means = pd.DataFrame(reference_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Reference'])
        ref_data_max = pd.DataFrame(reference_df.iloc[:, 1:].max(axis=1), columns=['Max_Reference'])
        ref_data_min = pd.DataFrame(reference_df.iloc[:, 1:].min(axis=1), columns=['Min_Reference'])

        # Calculate means, maxs, and mins for test data
        test_data_means = pd.DataFrame(test_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Test'])
        test_data_max = pd.DataFrame(test_df.iloc[:, 1:].max(axis=1), columns=['Max_Test'])
        test_data_min = pd.DataFrame(test_df.iloc[:, 1:].min(axis=1), columns=['Min_Test'])

        # Add Time column
        ref_data_means.insert(0, 'Time', reference_df.iloc[:, 0])
        ref_data_max.insert(0, 'Time', reference_df.iloc[:, 0])
        ref_data_min.insert(0, 'Time', reference_df.iloc[:, 0])

        test_data_means.insert(0, 'Time', test_df.iloc[:, 0])
        test_data_max.insert(0, 'Time', test_df.iloc[:, 0])
        test_data_min.insert(0, 'Time', test_df.iloc[:, 0])

        # Create scatter plot traces for reference data
        ref_trace_mean = go.Scatter(
            x=ref_data_means['Time'],
            y=ref_data_means['Mean_Reference'],
            mode='lines+markers',
            name='Reference Mean',
            marker=dict(color='blue'),
            error_y=dict(
                type='data',
                symmetric=False,
                array=ref_data_max['Max_Reference'] - ref_data_means['Mean_Reference'],
                arrayminus=ref_data_means['Mean_Reference'] - ref_data_min['Min_Reference'],
                color='blue'
            )
        )

        # Create scatter plot traces for test data
        test_trace_mean = go.Scatter(
            x=test_data_means['Time'],
            y=test_data_means['Mean_Test'],
            mode='lines+markers',
            name='Test Mean',
            marker=dict(color='green', symbol='diamond'),
            error_y=dict(
                type='data',
                symmetric=False,
                array=test_data_max['Max_Test'] - test_data_means['Mean_Test'],
                arrayminus=test_data_means['Mean_Test'] - test_data_min['Min_Test'],
                color='green'
            )
        )

        # Layout
        layout = go.Layout(
            title='Dissolution Curves with Intervals',
            xaxis=dict(title='Time',zeroline=True, zerolinewidth=1, zerolinecolor='black'),
            yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
            showlegend=True,
            plot_bgcolor='white', # Set plot background color to white 
            paper_bgcolor='white', # Set paper background color to white
            margin=dict(b=80) # Increase bottom margin to give space for the legend
        )

        # Create figure and add traces
        fig = go.Figure(data=[ref_trace_mean, test_trace_mean], layout=layout)

        # Show figure
        # fig.show()

        return fig
    except Exception as e:
        print(e, "dissolution_curve_interval")



# Example usage:
# ref_df = pd.read_csv('reference_data.csv')
# test_df = pd.read_csv('test_data.csv')
# fig = dissolution_curve_interval(ref_df, test_df)
# fig.show()


# Example usage (assuming you have dataframes reference_df and test_df ready)
# fig = dissolution_curve_interval(reference_df, test_df)

    
    #Save the plot as an image file
    # plt.savefig('dissolution_curves_with_intervals.png', format='png', dpi=300)

    #Optionally, display the plot (uncomment if you want to see it as well)
    # plt.show()

    # Close the plot to free up memory
    # plt.close()


def dissolution_curve(reference_df, test_df):
    try:
        # Calculate means for reference and test data
        ref_data_means = pd.DataFrame(reference_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Reference'])
        test_data_means = pd.DataFrame(test_df.iloc[:, 1:].mean(axis=1), columns=['Mean_Test'])

        # Add Time column
        ref_data_means.insert(0, 'Time', reference_df.iloc[:, 0])
        test_data_means.insert(0, 'Time', test_df.iloc[:, 0])

        # Create traces for reference data
        ref_trace = go.Scatter(
            x=ref_data_means['Time'],
            y=ref_data_means['Mean_Reference'],
            mode='lines+markers',
            name='Reference mean',
            marker=dict(symbol='circle', size=8),
            line=dict(color='blue')
        )

        # Create traces for test data
        test_trace = go.Scatter(
            x=test_data_means['Time'],
            y=test_data_means['Mean_Test'],
            mode='lines+markers',
            name='Test mean',
            marker=dict(symbol='circle', size=8),
            line=dict(color='green', dash='dash')
        )

        # Layout
        layout = go.Layout(
            title='Dissolution Curves',
            xaxis=dict(title='Time',zeroline=True, zerolinewidth=1, zerolinecolor='black'),
            yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black',showgrid=True, gridcolor='lightgray'),
            showlegend=True,
            legend=dict(orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1),
            plot_bgcolor='white', # Set plot background color to white 
            paper_bgcolor='white' # Set paper background color to white
        )

        # Create figure and add traces
        fig1 = go.Figure(data=[ref_trace, test_trace], layout=layout)

        # Show figure
        # pio.show(fig)

        return fig1
    except Exception as e:
        print(e, "dissolution_curve")

# Example usage (assuming you have dataframes reference_df and test_df ready)
# fig = dissolution_curve(reference_df, test_df)

    #Save the plot as an image file
    # fig.savefig('dissolution_curves.png', format='png', dpi=300)

    #Optionally, display the plot (uncomment if you want to see it as well)
    # fig.show()

    # Close the plot to free up memory
    # fig.close()
    
# Check 1: A minimum of three-time points (time zero excluded) is considered for both products
def check_time_points(df):
    try:
        if df.iloc[0, 0] == 0 or df.iloc[0, 0]=='0':  # Check if the first time point is zero
            return len(df.iloc[:,0].values)-1 >=3 # Exclude the zero time point 
        else:
            return len(df.iloc[:,0].values) >=3 
    except Exception as e:
        print(e,"check_time_points")
    
# Check 2: 1,2 time points > 85%? If > 85%, No f2 calculations
def two_time_points(df):
    try:
        mean_values = df.iloc[:, 1:].mean(axis=1)  # Exclude the Time Points column
        if mean_values[0]>85 or mean_values[1]>85:
            return False
        else:
            return True
    except Exception as e:
        print(e,"two_time_points")

#Check 4: If in Min check if in 15 min > 85% the no f2 required
def min15__check(df):
    try:
        time_string=df.columns[0]
        mean_values = df.iloc[:, 1:].mean(axis=1) 
        
        if "min" in time_string.lower() or "minutes" in time_string.lower():
            condition = df.iloc[:, 0] <= 15
            indices = df.index[condition]
            filtered_df = mean_values.loc[indices]
            condition_other_columns = filtered_df > 85
            final_indices = filtered_df.index[condition_other_columns].tolist()
            if len(final_indices)>0:
                return True
            else:
                return False
        else:
            return False
    except Exception as e:
        print(e,"min15__check")

          
# The coefficient of variation (CV) of both product should be less than 20% at the first (non-zero) time point and less than 10% at the following time points
def check_cv(df):
    try:
        if df.iloc[0, 0] == 0:  # Check if the first time point is zero
            cv_values = df.iloc[1:, 1:].std(axis=1) / df.iloc[1:, 1:].mean(axis=1) * 100  # Exclude the zero time point and Time Points column
            return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()
        else:
            cv_values = df.iloc[:, 1:].std(axis=1) / df.iloc[:, 1:].mean(axis=1) * 100 # Calculate CV for all time points
            #print(df)
            #print(cv_values)
            return cv_values.iloc[0] < 20 and (cv_values.iloc[1:] < 10).all()
    except Exception as e:
        print(e,"check_cv")



def row_variance(df):
    return df.iloc[:,1:].var(axis=1,ddof=1)

def stand(p,sum_diff_df_sqr):
    f2_v1 = 100 - 25 * np.log10((1 + (1 / p) * sum_diff_df_sqr))  # correct F2_DRL
    return f2_v1

def expected(p,sum_diff_df_sqr,left_side):
    f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) + (left_side)))))
    return f2
    
def BiasCor(p,sum_diff_df_sqr,left_side):
    # f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
    # print("Bias corrected f2", f2)
    Right_side=sum_diff_df_sqr+p
    if left_side >= Right_side:
        f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
        return round(f2,3)
    else:
        f2="Baised Corrected f2 can not be calculated"
        return f2
    return f2



def f2s(ref_data,test_data):
    # breakpoint()
    try:  
        ref_data_means=ref_data.iloc[:,1:].mean(axis=1)
        test_data_means=test_data.iloc[:,1:].mean(axis=1)
        ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
        test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
        diff_df=test_data_df-ref_data_df
        diff_df_sqr=diff_df**2
        sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
        ref_data_var_df=pd.DataFrame(row_variance(ref_data),columns=['Unbiased Variance'])
        test_data_var_df=pd.DataFrame(row_variance(test_data),columns=['Unbiased Variance'])
        addition_df=test_data_var_df+ref_data_var_df
        sum_addition_df=addition_df['Unbiased Variance'].sum()
        n_r,n_c=ref_data.shape
        n=n_c-1
        p=len(ref_data.iloc[:,0])
        left_side=(1/n)*(sum_addition_df)
        f2=stand(p,sum_diff_df_sqr)
        print("Conventional f2 :",round(f2,3))
        expf2=expected(p,sum_diff_df_sqr,left_side)
        print("Expected f2     :", round(expf2,3))
        BiCf2=BiasCor(p,sum_diff_df_sqr,left_side)
        print("BiasCorrected f2:", BiCf2)
        return round(f2,3)
    except Exception as e:
        print(e)



def changed_data_either85_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2_value = f2s(change_reference_df,change_test_df)
    return f2_value



def changed_data_both85_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    # print("mean ref",mean_values_reference)
    # print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    # print(ref_index)
    # print(test_index)

    if ref_index>test_index:
        final_index=ref_index
    elif test_index>ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    # print(final_index)
    # print(final_index+1)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    con_f2 = f2s(change_reference_df,change_test_df)
    return con_f2

def changed_data_either85_FDA_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2_values = (change_reference_df,change_test_df)
    return f2_values




def cal_f2(resampled_test,resampled_reference):
    #print(resampled_reference.iloc[:,1:])
    ref_data_means=resampled_reference.iloc[:,1:].mean(axis=1)
    test_data_means=resampled_test.iloc[:,1:].mean(axis=1)

    #print(ref_data_means)
    #print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    #print(ref_data_df)
    #print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)

    p=len(resampled_reference.iloc[:,0])
    #print('Time points',p)

    f2_v1 = 100 - 25 * np.log10((1 + (1 / p) * sum_diff_df_sqr))  # correct F2_DRL
    #print("f2 score is: ",f2_v1)
    arrayboot.append(f2_v1)
    return f2_v1

def bca(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=cal_f2(test_data,ref_data)
    n_iterations=12
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data.iloc[:,indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data.iloc[:,indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_f2(dft,dfr)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for f2: [{lower_percentile}, {upper_percentile}]")

def bca_exp_f2(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=cal_exp_f2(test_data,ref_data)
    n_iterations=12
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data.iloc[:,indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data.iloc[:,indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_exp_f2(dft,dfr)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for expected f2: [{lower_percentile}, {upper_percentile}]")

def plots(arrayboot,plot):
    fig, axs = plt.figure(figsize=(10, 6))
    sns.histplot(arrayboot, kde=True, bins=10, color='blue')
    axs.xlabel('Value')
    axs.ylabel('Frequency')
    tiltename=f"histogram_plot_{plot}"
    axs.title(tiltename)
    filename=f"histogram_plot_{plot}.png"
    axs.savefig(filename)  # Save the plot as a PNG file
    axs.close()  # Close the figure to avoid displaying it inline
    axs.figure(figsize=(8, 8))
    stats.probplot(arrayboot, dist="norm", plot=axs)
    tiltename=f"qq_plot_{plot}"
    axs.title(tiltename)
    # filename=f"qq_plot_{plot}.png"
    # axs.savefig(filename)  # Save the plot as a PNG file
    # axs.close()  # Close the figure to avoid displaying it inline
    return fig


def changed_data_either85_bca(reference_df,test_df):
    arrayboot.clear()
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index
    
    if final_index==0:
        final_index=change_reference_df.index[-1]
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("f2 Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    #print("Len1",len(arrayboot))
    plot="f2"
    plots(arrayboot,plot)

    print(" ")
    bca(change_test_df,change_reference_df)
    #print("Len2",len(arrayboot))
    plot="f2_bca"
    plots(arrayboot,plot)


    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    #print("Len3",len(arrayboot))
    plot="expected_f2"
    plots(arrayboot,plot)

    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    #print("Len4",len(arrayboot))
    plot="expected_f2_bca"
    plots(arrayboot,plot)

    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Baised Corrected f2 can not be calculated")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        #print("Len5",len(arrayboot))
        plot="baiscorrected_f2"
        plots(arrayboot,plot)

        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)
        #print("Len6",len(arrayboot))
        plot="baiscorrected_f2_bca"
        plots(arrayboot,plot)



def changed_data_both85_bca(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    # print(ref_index)
    # print(test_index)

    if ref_index>test_index:
        final_index=ref_index
    elif test_index>ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    

    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    
    change_test_df=change_test_df.iloc[:final_index+1]
 
    print("Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    print(" ")
    bca(change_test_df,change_reference_df)
    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Cannot caclulate f2")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)


def changed_data_either85_FDA_bca(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    print("Percentile confidence interval:")
    f2_bootstral_tile(change_reference_df,change_test_df)
    print(" ")
    bca(change_test_df,change_reference_df)
    print("Expected f2 Percentile confidence interval:")
    f2_bootstra_expf2_tile(change_reference_df,change_test_df)
    print("Expected f2 Bca confidence interval:")
    bca_exp_f2(change_test_df,change_reference_df)
    res_bias_corrected=biascorrectedf2(change_reference_df,change_test_df)
    if res_bias_corrected==0 or res_bias_corrected=='nan':
        #print(res_bias_corrected)
        print("Cannot caclulate f2")
    else:
        #print(res_bias_corrected)
        print("bias corrected f2 Percentile confidence interval:")
        f2_bootstratp_bias_f2_tile(change_reference_df,change_test_df)
        print("bias corrected f2 bca:")
        bca_bias_f2(change_test_df,change_reference_df)




def f2_bootstral_tile(ref_data,test_data):
    # breakpoint()
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=12
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr = ref_data.iloc[:, indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data.iloc[:,indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_f2(dft,dfr)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

# Check 3: The time points at which the dissolutions are measured are the same for both products
def check_same_time_points(df1, df2):
    return df1.iloc[:,0].equals(df2.iloc[:,0])

# Check 4: At least 12 individual dosage units are used for both products
def check_sample_units(df):
    return df.shape[1] - 1 >= 12  # Exclude the Time Points column


def cal_exp_f2(resampled_test,resampled_reference):
    ref_data_means=resampled_reference.iloc[:,1:].mean(axis=1)
    test_data_means=resampled_test.iloc[:,1:].mean(axis=1)

    #print(ref_data_means)
    #print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    #print(ref_data_df)
    #print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)

    ref_data_var_df=pd.DataFrame(row_variance(resampled_reference),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(resampled_test),columns=['Unbiased Variance'])

    #print(ref_data_var_df)
    #print(test_data_var_df)

    addition_df=test_data_var_df+ref_data_var_df
    #print(addition_df)

    sum_addition_df=addition_df['Unbiased Variance'].sum()
    #print(sum_addition_df)

    n_r,n_c=resampled_test.shape
    n=n_c-1
    #print(n)

    p=len(resampled_test.iloc[:,0])
    #print('Time points',p)

    left_side=(1/n)*(sum_addition_df)
    #print(left_side)

    f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) + (left_side)))))
    #print("Expected f2", f2)
    arrayboot.append(f2)
    return f2

def f2_bootstra_expf2_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=12
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data.iloc[:,indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data.iloc[:,indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=cal_exp_f2(dft,dfr)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

def biascorrectedf2(ref_data,test_data):
    ref_data_means=ref_data.iloc[:,1:].mean(axis=1)
    test_data_means=test_data.iloc[:,1:].mean(axis=1)

    # print(ref_data_means)
    # print(test_data_means)

    ref_data_df=pd.DataFrame(ref_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})
    test_data_df=pd.DataFrame(test_data_means,columns=['Mean']).reset_index().rename(columns={'index':'Column'})

    # print(ref_data_df)
    # print(test_data_df)


    diff_df=test_data_df-ref_data_df
    #print(diff_df)

    diff_df_sqr=diff_df**2
    #print(diff_df_sqr)

    sum_diff_df_sqr=diff_df_sqr['Mean'].sum()
    #print(sum_diff_df_sqr)


    ref_data_var_df=pd.DataFrame(row_variance(ref_data),columns=['Unbiased Variance'])
    test_data_var_df=pd.DataFrame(row_variance(test_data),columns=['Unbiased Variance'])

    # print(ref_data_var_df)
    # print(test_data_var_df)

    addition_df=test_data_var_df+ref_data_var_df
    # print(addition_df)

    sum_addition_df=addition_df['Unbiased Variance'].sum()
    # print(sum_addition_df)

    n_r,n_c=ref_data.shape
    n=n_c-1
    # print(n)

    p=len(ref_data.iloc[:,0])
    # print('Time points',p)

    left_side=(1/n)*(sum_addition_df)
    # print(left_side)

    # f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
    # print("Bias corrected f2", f2)
    Right_side=sum_diff_df_sqr+p

    if left_side >= Right_side:
        with warnings.catch_warnings():
            warnings.simplefilter("error", RuntimeWarning)
            try:
                f2 = 100 - 25 * np.log10((1 +( (1 / p) * ((sum_diff_df_sqr) - (left_side)))))
            except RuntimeWarning as e:
                print(f"Warning occurred: {e}")
                f2=0
                return f2
        #print("Baised Corrected f2", f2)
        return f2
    else:
        f2=0
        #print("No f2",f2)
        return f2

def f2_bootstratp_bias_f2_tile(ref_data,test_data):
    arrayboot.clear()
    ref_data=ref_data
    test_data=test_data
    n_iterations=12
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=biascorrectedf2(dfr,dft)

    alpha=0.1
    lower_bound=np.percentile(f2_values,alpha/2*100)
    upper_bound=np.percentile(f2_values,(1-alpha/2)*100)

    total=sum(arrayboot)
    mea=total/len(arrayboot)
    print('Mean',mea)
    print('Lower',lower_bound)
    print('upper',upper_bound)

def bca_bias_f2(test_data,ref_data):
    #print(len(arrayboot))
    arrayboot.clear()
    #print(len(arrayboot))
    #print(test_data)
    #print(ref_data)
    original_f2=f2_bootstratp_bias_f2_tile(ref_data,test_data)
    n_iterations=12
    n_size=ref_data.shape[1]
    f2_values=np.zeros(n_iterations)

    for i in range(n_iterations):
        indices=np.random.choice(range(1,n_size),n_size-1,replace=True)
        #print(indices)
        time_C=ref_data.iloc[:,0]
        dfr=ref_data[indices]
        indices1=np.random.choice(range(1,n_size),n_size-1,replace=True)
        dft=test_data[indices1]
        #print(dfr)
        #print(dft)                                                                     
        dfr.insert(0,time_C.name,time_C)
        dft.insert(0,time_C.name,time_C)
        #print(dfr)
        #print(dft)
        f2_values[i]=f2_bootstratp_bias_f2_tile(dfr,dft)



    B = len(f2_values)
    count_less_than = np.sum(f2_values < original_f2)
    z0 = norm.ppf(count_less_than / B)


    f2_m = np.mean(f2_values)
    numerator = np.sum((f2_m - f2_values) ** 3)
    denominator = 6 * (np.sum((f2_m - f2_values) ** 2) ** (3 / 2))
    a_hat = numerator / denominator

    alpha = 0.1
    z_alpha = norm.ppf(alpha / 2)
    z_1_alpha = norm.ppf(1 - alpha / 2)
        

    alpha1 = norm.cdf(z0 + (z0 + z_alpha) / (1 - a_hat * (z0 + z_alpha)))
    alpha2 = norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a_hat * (z0 + z_1_alpha)))


    lower_percentile = np.percentile(f2_values, alpha1 * 100)
    upper_percentile = np.percentile(f2_values, alpha2 * 100)

    print(f"BCa Confidence Interval for expected f2: [{lower_percentile}, {upper_percentile}]")



#order sensitive permutations
def generate_time_permutations_fixed_zero(times):
    """
    Generate all order-sensitive permutations of time points for lengths
    between 3 and len(times) (inclusive) with 0 fixed as the first element.
    Assumes that the input 'times' is a list of numeric time points.
    """
    try:
        # Ensure times are integers and sorted so that 0 is first if present
        times = sorted([int(t) for t in times])
        # Ensure 0 is present
        if 0 not in times:
            times.insert(0, 0)
        # Separate 0 from the other time points
        nonzero = [t for t in times if t != 0]
        all_perms = []
        total = len(times)  # total including 0
        # For sequence lengths from 3 up to total
        for r in range(3, total + 1):
            # We choose (r-1) time points from nonzero values in all possible orders
            for perm in itertools.permutations(nonzero, r - 1):
                seq = [0] + list(perm)  # Prepend 0 as fixed starting point
                all_perms.append(seq)
        print(f"Generated {len(all_perms)} permutations with 0 fixed at start")
        return all_perms
    except Exception as e:
        print(f"Permutation generation error: {str(e)}")
        return []

def permutation_compliance_check(perm, regulation, ref_means, test_means):
    """
    Check compliance for a given time sequence (with 0 fixed at the start) 
    based on regulatory rules.
    """
    reasons = []
    compliant = True

    # FDA: If an 85% (or higher) dissolution is observed, it must be at the last point.
    if regulation == "US FDA":
        found_85 = False
        for i, t in enumerate(perm):
            if ref_means.get(t, 0) >= 85 or test_means.get(t, 0) >= 85:
                if i < len(perm) - 1:  # Must be the final measurement
                    reasons.append(f"Early 85% at position {i+1} ({t} min)")
                    compliant = False
                found_85 = True
                break
        if not found_85 and 0 not in perm:
            reasons.append("Missing zero time point")
            compliant = False

    # EMA: All time points must have dissolution <85% and at least 4 time points are required.
    elif regulation == "EMA/ICH/Canada/Australia":
        over_points = [t for t in perm if ref_means.get(t, 0) >= 85 or test_means.get(t, 0) >= 85]
        if over_points:
            reasons.append(f"85%+ dissolution at: {over_points}")
            compliant = False
        if len(perm) < 4:
            reasons.append("Requires ‚â•4 time points")
            compliant = False

    # China: Only the reference product is considered; no time point should have ‚â•85% dissolution.
    elif regulation == "China":
        over_ref = [t for t in perm if ref_means.get(t, 0) >= 85]
        if over_ref:
            reasons.append(f"Reference ‚â•85% at: {over_ref}")
            compliant = False

    # ASEAN: The first 3 time points must be below 85% for both reference and test.
    elif regulation == "ASEAN":
        check_points = perm[:3] if len(perm) >= 3 else perm
        over_ref = [t for t in check_points if ref_means.get(t, 0) >= 85]
        over_test = [t for t in check_points if test_means.get(t, 0) >= 85]
        if over_ref:
            reasons.append(f"Reference ‚â•85% in first 3 points: {over_ref}")
            compliant = False
        if over_test:
            reasons.append(f"Test ‚â•85% in first 3 points: {over_test}")
            compliant = False

    # ANVISA: The 0 time point must be present and must be the first element.
    elif regulation == "ANVISA":
        if 0 not in perm:
            reasons.append("Missing zero time point")
            compliant = False
        elif perm[0] != 0:
            reasons.append("Zero not in first position")
            compliant = False

    return (compliant, " | ".join(reasons) if reasons else "Compliant")

def full_permutation_analysis(ref_df, test_df, regulation):
    """
    Perform a full permutation analysis:
      - Generate all sequences (with 0 fixed at start) for lengths from 3 to n.
      - Calculate f2 for each sequence.
      - Check regulatory compliance.
    """
    try:
        # Extract original time points from the first column of the reference DataFrame.
        times = ref_df.iloc[:, 0].tolist()
        # Ensure 0 is included
        if 0 not in times:
            times.insert(0, 0)
        
        # Calculate the mean dissolution values for each time point (row).
        ref_means = ref_df.set_index(ref_df.columns[0]).iloc[:, 1:].mean(axis=1).to_dict()
        test_means = test_df.set_index(test_df.columns[0]).iloc[:, 1:].mean(axis=1).to_dict()
        # If 0 was artificially inserted, assign its dissolution mean as 0.
        if 0 not in ref_means:
            ref_means[0] = 0
        if 0 not in test_means:
            test_means[0] = 0

        # Generate all order-sensitive permutations with 0 fixed at start.
        perms = generate_time_permutations_fixed_zero(times)
        print(f"Analyzing {len(perms)} permutations for {regulation}...")
        
        results = []
        for perm in perms:
            try:
                p = len(perm)
                # Use the permutation (0 is fixed at the beginning)
                time_order = perm
                
                # Retrieve the mean dissolution values for this sequence.
                ref_values = [ref_means.get(t, 0) for t in time_order]
                test_values = [test_means.get(t, 0) for t in time_order]
                
                # Calculate f2 using the formula.
                diff = np.array(test_values) - np.array(ref_values)
                sum_sq = (diff ** 2).sum()
                f2 = 100 - 25 * np.log10(1 + (sum_sq / p)) if p > 0 else 0
                
                # Check compliance based on the regulatory rule.
                compliant, reason = permutation_compliance_check(time_order, regulation, ref_means, test_means)
                
                results.append({
                    'permutation': time_order,
                    'f2': round(f2, 2),
                    'length': p,
                    'compliant': compliant,
                    'reason': reason
                })
            except Exception as e:
                print(f"Error processing {perm}: {str(e)}")
                continue
                
        return sorted(results, key=lambda x: (-x['f2'], x['length']))
    
    except Exception as e:
        print(f"Analysis failed: {str(e)}")
        return []

def display_permutation_results(results, regulation):
    """
    Display the permutation analysis results.
    """
    if not results:
        print("\nNo valid permutations found")
        return
    
    # Show all results on one page.
    page_size = len(results)
    
    print(f"\n{'='*120}")
    print(f" Complete Permutation Analysis ({regulation})")
    print(f"{'Time Sequence':<40} | {'Length':<6} | {'f2':<8} | {'Status':<8} | Compliance Details")
    print("-"*120)
    
    for res in results[:page_size]:
        status = "‚úÖ" if res['compliant'] else "‚ùå"
        print(f"{str(res['permutation']):<40} | {res['length']:^6} | {res['f2']:^8.2f} | {status:^8} | {res['reason']}")
    
    print("="*120 + "\n")
    
    # Return the optimal (highest f2 among compliant) sequence if available.
    compliant_results = [r for r in results if r['compliant']]
    if compliant_results:
        best = max(compliant_results, key=lambda x: x['f2'])
        print(f"Optimal Compliant Combination:")
        print(f"Sequence: {best['permutation']}")
        print(f"f2 Score: {best['f2']:.2f}")
        f2_score = f"{best['f2']:.2f}"
        print(f"Compliance: {best['reason']}")
        return best['permutation'], f2_score
    else:
        return None, None

def plot_dissolution_curves(ref_df, test_df, optimal_seq):
    """
    Generate two graphs:
      1. Full dissolution curves for the reference and test products, with vertical
         lines marking the optimal time points.
      2. Dissolution curves (points connected by lines) only for the optimal time points.
    """
    # Sort the DataFrames by time.
    ref_df_sorted = ref_df.sort_values(by=ref_df.columns[0])
    test_df_sorted = test_df.sort_values(by=test_df.columns[0])
    
    # Extract full time points and mean dissolution values.
    time_points_full = ref_df_sorted.iloc[:, 0].astype(float).tolist()
    ref_means_full = ref_df_sorted.iloc[:, 1:].mean(axis=1).tolist()
    test_means_full = test_df_sorted.iloc[:, 1:].mean(axis=1).tolist()
    
    # Retrieve mean dissolution values at the optimal time points.
    ref_means_opt = []
    test_means_opt = []
    for t in optimal_seq:
        # Find the corresponding row in the DataFrame.
        ref_row = ref_df[ref_df.iloc[:, 0] == t]
        test_row = test_df[test_df.iloc[:, 0] == t]
        if not ref_row.empty:
            ref_mean = ref_row.iloc[:, 1:].mean(axis=1).values[0]
        else:
            ref_mean = 0
        if not test_row.empty:
            test_mean = test_row.iloc[:, 1:].mean(axis=1).values[0]
        else:
            test_mean = 0
        ref_means_opt.append(ref_mean)
        test_means_opt.append(test_mean)
    
    # --------------------- Graph 1: Full Curves with Optimal Time Points ---------------------
    # plt.figure(figsize=(10, 6))
    # plt.plot(time_points_full, ref_means_full, marker='o', linestyle='-', color='blue', label='Reference Full')
    # plt.plot(time_points_full, test_means_full, marker='o', linestyle='-', color='red', label='Test Full')
    # for t in optimal_seq:
    #     plt.axvline(x=t, color='gray', linestyle='--', alpha=0.5)
    # plt.title('Full Dissolution Curves with Optimal Time Points Indicated')
    # plt.xlabel('Time')
    # plt.ylabel('Dissolution Percentage')
    # plt.legend()
    # plt.grid(True)
    # plt.tight_layout()
    # plt.show()
    
    # --------------------- Graph 2: Optimal Time Points Only ---------------------
    # plt.figure(figsize=(10, 6))
    # plt.plot(optimal_seq, ref_means_opt, marker='o', linestyle='-', color='blue', label='Reference Optimal')
    # plt.plot(optimal_seq, test_means_opt, marker='o', linestyle='-', color='red', label='Test Optimal')
    # plt.title('Dissolution Curves at Optimal Time Points')
    # plt.xlabel('Time')
    # plt.ylabel('Dissolution Percentage')
    # plt.legend()
    # plt.grid(True)
    # plt.tight_layout()
    # plt.show()
    # Create a Plotly figure
    optimal_plotly_fig = go.Figure()

    # Add Reference Optimal trace
    optimal_plotly_fig.add_trace(go.Scatter(
        x=optimal_seq,
        y=ref_means_opt,
        mode='lines+markers',
        name='Reference Optimal',
        marker=dict(color='blue', symbol='circle'),
        line=dict(color='blue', dash='solid')
    ))

    # Add Test Optimal trace
    optimal_plotly_fig.add_trace(go.Scatter(
        x=optimal_seq,
        y=test_means_opt,
        mode='lines+markers',
        name='Test Optimal',
        marker=dict(color='red', symbol='circle'),
        line=dict(color='red', dash='solid')
    ))

    # Customize layout
    optimal_plotly_fig.update_layout(
        title='Dissolution Curves at Optimal Time Points',
        xaxis=dict(title='Time',zeroline=True, zerolinewidth=1, zerolinecolor='black'),
        yaxis=dict(title='Dissolution Percentage',  tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
        legend_title='Legend',
        plot_bgcolor='white',
        paper_bgcolor='white'
    )

    return optimal_plotly_fig


def changed_data_either85_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)



def changed_data_both85_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    # print("mean ref",mean_values_reference)
    # print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    # print(ref_index)
    # print(test_index)

    if ref_index>test_index:
        final_index=ref_index
    elif test_index>ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    # print(final_index)
    # print(final_index+1)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)

def changed_data_either85_FDA_f2s(reference_df,test_df):
    ref_index=0
    test_index=0
    final_index=0
    change_reference_df=reference_df
    change_test_df=test_df
    mean_values_reference = change_reference_df.iloc[:, 1:].mean(axis=1)
    mean_values_test = change_test_df.iloc[:, 1:].mean(axis=1)    
    #print("mean ref",mean_values_reference)
    #print("mean test",mean_values_test)
    for i in range(len(mean_values_reference)):
        if mean_values_reference[i]>=85:
            ref_index=i
            break
    
    for i in range(len(mean_values_test)):
        if mean_values_reference[i]>=85:
            test_index=i
            break

    #print(ref_index)
    #print(test_index)

    if ref_index<test_index:
        final_index=ref_index
    elif test_index<ref_index:
        final_index=test_index
    elif ref_index==test_index:
        final_index=test_index

    if final_index==0:
        final_index=change_reference_df.index[-1]    
    #print(final_index)

    change_reference_df=change_reference_df.iloc[:final_index+1]
    change_test_df=change_test_df.iloc[:final_index+1]
    f2s(change_reference_df,change_test_df)


def calculate_f2_either85(reference_df, test_df):
    """
    1. Keep time 0.
    2. Find the first non-zero timepoint where either mean(reference) or mean(test) > 85%.
    3. Trim both DataFrames to include rows [0 .. that timepoint].
    4. Compute:
    - f2_conv: conventional f2
    - exp_f2 : expected f2 (variance-adjusted)
    Returns:
    f2_conv, exp_f2, trimmed_ref_df, trimmed_test_df
    """

    # 1) Extract times & means
    times     = reference_df.iloc[:, 0].astype(float).values
    ref_mean  = reference_df.iloc[:, 1:].mean(axis=1).values
    test_mean = test_df.iloc[:, 1:].mean(axis=1).values

    # 2) Find cutoff index
    cutoff = next((i for i in range(1, len(times))
                   if ref_mean[i] > 85 or test_mean[i] > 85), len(times) - 1)

    # 3) Trim full DataFrames
    trimmed_ref_df  = reference_df.iloc[:cutoff+1].copy()
    trimmed_test_df = test_df.iloc[:cutoff+1].copy()

    # Force 0% at t=0
    if float(trimmed_ref_df.iloc[0, 0]) == 0:
        trimmed_ref_df.iloc[0, 1:]  = 0.0
        trimmed_test_df.iloc[0, 1:] = 0.0

    # 4a) conventional f2
    diff    = (trimmed_test_df.iloc[:,1:].mean(axis=1).values
               - trimmed_ref_df.iloc[:,1:].mean(axis=1).values)
    p       = len(diff)
    sum_sq  = np.sum(diff**2)
    f2_conv = 100 - 25 * np.log10(1 + sum_sq / p)

    # 4b) expected f2
    var_ref   = trimmed_ref_df.iloc[:,1:].var(axis=1, ddof=1).values
    var_test  = trimmed_test_df.iloc[:,1:].var(axis=1, ddof=1).values
    avg_var   = np.mean(var_ref + var_test)
    exp_f2    = 100 - 25 * np.log10(1 + (sum_sq + p * avg_var) / p)

    return f2_conv, exp_f2, trimmed_ref_df, trimmed_test_df



 
def interpolate_linear(df, new_times):
    """
    Simple linear interpolation (used for determining the candidate window
    and for candidate selection).
    """
    df_sorted = df.sort_values(by=df.columns[0])
    known_times = df_sorted.iloc[:, 0].values.astype(float)
    known_values = df_sorted.iloc[:, 1].values.astype(float)
    f = interp1d(known_times, known_values, kind='linear', 
                 fill_value=(known_values[0], known_values[-1]), bounds_error=False)
    return f(new_times)

def determine_candidate_window(ref_df, test_df, step=1, initial_threshold=10):
    """
    Identify candidate window based on the actual time range in the data.
    """
    max_ref_time = ref_df.iloc[:, 0].max()
    max_test_time = test_df.iloc[:, 0].max()
    fixed_max = max(max_ref_time, max_test_time)
    fixed_min = 0

    times = np.arange(fixed_min, fixed_max + 1, step)
    ref_vals = interpolate_linear(ref_df, times)
    test_vals = interpolate_linear(test_df, times)
    diff = np.abs(ref_vals - test_vals)
    
    valid_times = times[diff <= initial_threshold]
    if len(valid_times) == 0:
        print(f"No time points found within {initial_threshold}% difference; trying threshold=20.")
        valid_times = times[diff <= 20]
        if len(valid_times) == 0:
            print("No candidate window found even with 20% threshold. Using full range.")
            return fixed_min, fixed_max
        else:
            window_max = valid_times[-1]
            print(f"Candidate window determined (threshold 20): {fixed_min} to {window_max}")
            return fixed_min, window_max
    else:
        window_max = valid_times[-1]
        print(f"Candidate window determined (threshold {initial_threshold}): {fixed_min} to {window_max}")
        return fixed_min, window_max

# ======================== STAGE 2: Helper & Predictive Analysis Functions ========================
def generate_all_time_combinations(min_time, max_time, step=1):
    """
    Generate all possible time point sequences within the interval [min_time, max_time],
    ensuring that 0 is fixed at the start and that there are at least 3 points.
    """
    times = list(range(min_time, max_time + 1, step))
    if 0 not in times:
        times.insert(0, 0)
    all_combinations = []
    for r in range(2, len(times)):
        for combo in itertools.combinations(times[1:], r):
            seq = [0] + list(combo)
            all_combinations.append(sorted(seq))
    return list(all_combinations)

def interpolate_dissolution_curve(df, new_times, method='linear'):
    """
    Predict dissolution values at new time points, using linear interpolation.
    """
    df_sorted = df.sort_values(by=df.columns[0])
    known_times = df_sorted.iloc[:, 0].values.astype(float)
    known_values = df_sorted.iloc[:, 1].values.astype(float)
    
    new_times_clamped = np.clip(new_times, known_times.min(), known_times.max())
    f = interp1d(known_times, known_values, kind='linear', 
                 fill_value=(known_values[0], known_values[-1]), bounds_error=False)
    return f(new_times_clamped)
    

def check_regulatory_compliance(seq, regulation, ref_means, test_means):
    """
    Check if a candidate sequence satisfies the selected regulatory rules.
    Revised rules:
      - FDA:
          * For each of reference and test, exactly one time point must have dissolution ‚â•85%.
      - EMA:
          * Across reference or test, at most one time point may be ‚â•85%.
          * At least 4 time points are required.
      - China:
          * For the reference, exactly one time point must be "around 85%" (i.e. between 84 and 86)
            and all other time points must be below 85%.
      - ASEAN:
          * There must be at least 3 time points collected before 15 minutes.
          * At the 15-minute time point (if present), the reference dissolution should be close to 80%
            (here defined as between 75 and 85).
          * Across both reference and test, only one time point in the full profile may be ‚â•85%.
      - ANVISA:
          * The collection must have at least 5 time points.
          * The first time point must be 0.
          * Across reference and test, only one time point may be ‚â•85%.
          * For f2 calculation, use the first 3 time points (excluding the 0 time point);
            these ‚Äúearlier time points‚Äù should be around 40% (here defined as between 35 and 45).
    Returns a tuple (compliant, reasons) where compliant is a boolean and reasons is a list of strings.
    """
    compliant = True
    reasons = []
    
    if regulation == "US FDA":
        # Count time points with ‚â•85% dissolution for both reference and test separately.
        ref_count = sum(1 for t in seq if ref_means[t] >= 85)
        test_count = sum(1 for t in seq if test_means[t] >= 85)
        if ref_count != 1:
            compliant = False
            reasons.append(f"Reference must have exactly one time point ‚â•85% (found {ref_count}).")
        if test_count != 1:
            compliant = False
            reasons.append(f"Test must have exactly one time point ‚â•85% (found {test_count}).")
    
    elif regulation == "EMA":
        # Combined count: only one time point overall may be ‚â•85%
        combined_count = sum(1 for t in seq if ref_means[t] >= 85 or test_means[t] >= 85)
        if combined_count > 1:
            compliant = False
            reasons.append("More than one time point ‚â•85% detected across reference/test.")
        if len(seq) < 4:
            compliant = False
            reasons.append("Insufficient time points (<4).")
    
    elif regulation == "China":
        # For reference: exactly one time point must be around 85% (between 84 and 86) 
        # and all others must be below 85%.
        around_85 = [t for t in seq if 84 <= ref_means[t] <= 86]
        above_85 = [t for t in seq if ref_means[t] > 85]
        if len(around_85) != 1:
            compliant = False
            reasons.append(f"Reference must have exactly one time point around 85% (found {len(around_85)}).")
        # Even if one time point is around 85, any other time point ‚â•85 is not allowed.
        if len(above_85) - len(around_85) > 0:
            compliant = False
            reasons.append("Other reference time points above 85% detected.")
    
    elif regulation == "ASEAN":
        # At least 3 time points must be before 15 minutes.
        before_15 = [t for t in seq if t < 15]
        if len(before_15) < 3:
            compliant = False
            reasons.append("Fewer than 3 time points collected before 15 minutes.")
        # At the 15-minute time point, reference should be close to 80% (between 75 and 85) if present.
        if 15 in seq:
            if not (75 <= ref_means[15] <= 85):
                compliant = False
                reasons.append("At 15 minutes, reference dissolution is not close to 80%.")
        # Combined count for any dissolution ‚â•85% (both reference and test) must be only one.
        combined_count = sum(1 for t in seq if ref_means[t] >= 85 or test_means[t] >= 85)
        if combined_count != 1:
            compliant = False
            reasons.append(f"ASEAN profile must have exactly one time point ‚â•85% (found {combined_count}).")
    
    elif regulation == "ANVISA":
        # Must have at least 5 time points.
        if len(seq) < 5:
            compliant = False
            reasons.append("Less than 5 time points collected.")
        # The first time point must be 0.
        if seq[0] != 0:
            compliant = False
            reasons.append("The first time point must be 0.")
        # Count time points with ‚â•85% dissolution for both reference and test separately.
        ref_count = sum(1 for t in seq if ref_means[t] >= 85)
        test_count = sum(1 for t in seq if test_means[t] >= 85)
        if ref_count != 1:
            compliant = False
            reasons.append(f"Reference must have exactly one time point ‚â•85% (found {ref_count}).")
        if test_count != 1:
            compliant = False
            reasons.append(f"Test must have exactly one time point ‚â•85% (found {test_count}).")
        # For f2 calculation, use the first 3 time points (excluding the 0 time point);
        # these early time points should be around 40% (between 35 and 45).
        nonzero_points = [t for t in seq if t != 0]
        early_points = nonzero_points[:3]
        for t in early_points:
            if not (35 <= ref_means[t] <= 45 and 35 <= test_means[t] <= 45):
                compliant = False
                reasons.append(f"Early time point at t={t} not around 40% for both reference and test.")
    
    return (compliant, reasons)

def predictive_optimal_combinations_advanced(ref_df, test_df, regulation, 
                                             window_min, window_max, diff_threshold=None,
                                             interp_method='linear', points_per_stratum=None):
    """
    Deterministic selection of candidate time points using linear interpolation.
    
    Steps:
      1. Build a valid time grid from the union of 3- and 5-minute increments within [window_min, window_max].
      2. Compute predicted dissolution values (via linear interpolation) for both reference and test.
      3. Fix the starting time (window_min) as the first candidate.
      4. Split the remaining valid times into three strata based on predicted reference values:
           - 0-30: select exactly 2 candidate times (excluding the starting time)
           - 30-60: select exactly 2 candidate times (min and max among those with ref in [30,60))
           - 60-90: select exactly 2 candidate times (min and max among those with ref in [60,90))
         For the 60‚Äì90 stratum, if neither candidate has both ref and test ‚â•80%, replace the later candidate
         with the candidate (from eligible times) that maximizes min(ref, test).
      5. For non‚ÄëFDA mode, the final candidate combination will have 1 + 2 + 2 + 2 = 7 points.
      6. For FDA mode, append one extra candidate‚Äîthe first valid time (after the last candidate) where both
         predictions are ‚â•85%‚Äîyielding 8 points.
      7. Compute f2 using:
             f2 = 50 * log10(100 / (1 + sqrt(mean((test - ref)^2))))
         (The prediction at the starting time is forced to 0.)
         
    Raises an error if any stratum (other than 60‚Äì90, which is adjusted) lacks sufficient eligible times.
    """
   
    
    # 1. Build valid time grid (only 3- and 5-minute increments)
    valid_times = np.sort(np.unique(np.concatenate([
        np.arange(window_min, window_max+1, 3),
        np.arange(window_min, window_max+1, 5)
    ])))
    
    # 2. Compute predicted dissolution values using linear interpolation.
    all_ref_pred = interpolate_linear(ref_df, valid_times)
    all_test_pred = interpolate_linear(test_df, valid_times)
    
    # 3. Fix the starting time.
    fixed_start = window_min  # typically 0
    
    # 4. Define dissolution strata (using predicted reference values).
    strata = {
        "0-30": (0, 30),
        "30-60": (30, 60),
        "60-90": (60, 90)
    }
    required_points = 2  # exactly two candidate times per stratum (excluding the fixed starting time)
    
    # For 0-30, we exclude the fixed starting time.
    def eligible_times_for_stratum(stratum_range, exclude_start=False):
        low, high = stratum_range
        if exclude_start:
            return [t for t, pred in zip(valid_times, all_ref_pred) if t != fixed_start and low <= pred < high]
        else:
            return [t for t, pred in zip(valid_times, all_ref_pred) if low <= pred < high]
    
    candidate = []
    
    # 5. For each stratum, select exactly two candidate times deterministically.
    # For the 0-30 stratum, exclude the fixed starting time.
    for key in ["0-30", "30-60", "60-90"]:
        if key == "0-30":
            eligible = eligible_times_for_stratum(strata[key], exclude_start=True)
        else:
            eligible = eligible_times_for_stratum(strata[key])
        if len(eligible) < required_points:
            raise ValueError(f"Insufficient eligible times in stratum {key}.")
        # Deterministically choose the minimum and maximum eligible time.
        candidate_stratum = [min(eligible), max(eligible)]
        candidate.extend(candidate_stratum)
    
    candidate = sorted(list(set(candidate)))
    # Now, add the fixed starting time at the beginning.
    if fixed_start not in candidate:
        candidate.insert(0, fixed_start)
    
    # For non-FDA mode, we expect candidate count = 1 + 2 + 2 + 2 = 7.
    if regulation != "US FDA" and len(candidate) != 7:
        raise ValueError(f"Final candidate count ({len(candidate)}) is not equal to 7 as required for non-FDA mode.")
    
    # 6. Enforce the 60-90 stratum rule:
    eligible_60_90 = eligible_times_for_stratum(strata["60-90"])
    cand_60_90 = [t for t in candidate if t in eligible_60_90]
    has_80 = False
    for t in cand_60_90:
        idx = int(np.where(valid_times == t)[0][0])
        if all_ref_pred[idx] >= 80 and all_test_pred[idx] >= 80:
            has_80 = True
            break
    if not has_80:
        # Replace the later candidate from the 60-90 group with the candidate that maximizes min(ref, test)
        best_val = -np.inf
        best_t = None
        for t in eligible_60_90:
            idx = int(np.where(valid_times == t)[0][0])
            val = min(all_ref_pred[idx], all_test_pred[idx])
            if val > best_val:
                best_val = val
                best_t = t
        current_60_90 = sorted([t for t in candidate if t in eligible_60_90])
        if len(current_60_90) < 2:
            raise ValueError("Insufficient candidates in 60-90 stratum to adjust.")
        # Replace the later candidate with best_t.
        candidate = [t for t in candidate if t not in current_60_90]
        candidate.extend([min(current_60_90), best_t])
        candidate = sorted(list(set(candidate)))
        if len(candidate) != 7:
            raise ValueError("After adjustment, candidate count is not 7 for non-FDA mode.")
    
    # 7. For FDA mode: Append one extra candidate‚Äîthe first valid time after the last candidate with both predictions ‚â•85%.
    if regulation in ["US FDA", "ANVISA"]:
        expected_count = 8
        last_candidate = candidate[-1]
        post_times = [t for t in valid_times if t > last_candidate]
        extra_point = None
        for t in post_times:
            idx = int(np.where(valid_times == t)[0][0])
            if all_ref_pred[idx] >= 85 and all_test_pred[idx] >= 85:
                extra_point = t
                break
        if extra_point is None:
            raise ValueError("No valid extra candidate found for FDA mode.")
        candidate.append(extra_point)
        candidate = sorted(list(set(candidate)))
        if len(candidate) != expected_count:
            raise ValueError(f"Final candidate count ({len(candidate)}) does not equal expected count ({expected_count}) for FDA mode.")
    else:
        if len(candidate) != 7:
            raise ValueError(f"Non-FDA candidate count ({len(candidate)}) is not equal to 7.")
    
    # 8. Compute f2.
    # Use linear interpolation predictions at the candidate times.
    ref_vals = np.array([float(val) for val in interpolate_linear(ref_df, candidate)])
    test_vals = np.array([float(val) for val in interpolate_linear(test_df, candidate)])
    if candidate[0] == fixed_start:
        ref_vals[0] = 0.0
        test_vals[0] = 0.0
    diff = test_vals - ref_vals
    f2 = 50 * math.log10(100 / (1 + math.sqrt(np.mean(diff**2))))
    
    result = {
        'sequence': candidate,
        'f2': round(f2, 2),
        'compliant': True,
        'reasons': [],
        'ref_vals': ref_vals.tolist(),
        'test_vals': test_vals.tolist()
    }
    return [result], [result]

#new
def calculate_mean_profile(df):
    times = df.iloc[:, 0].astype(float).values
    means = df.iloc[:, 1:].astype(float).mean(axis=1)
    return times, means

def create_monotonic_profile(times, means, step=0.25, window_max=12):
    t = np.asarray(times, float)
    m = np.asarray(means, float)
    grid = np.arange(0, window_max + step, step)
    prof = np.interp(grid, t, m, left=0.0, right=m[-1])
    return np.maximum.accumulate(prof), grid

def find_85_point(ref, test, regulation):
    if regulation in ("US FDA", "ANVISA"):
        for i, (r, t) in enumerate(zip(ref, test)):
            if r >= 85 and t >= 85:
                return i
    elif regulation in ("EMA", "CHINA", "ASEAN"):
        ref_found, test_found = None, None
        for i, (r, t) in enumerate(zip(ref, test)):
            if ref_found is None and r >= 85:
                ref_found = i
            if test_found is None and t >= 85:
                test_found = i
            if ref_found is not None and test_found is not None:
                return min(ref_found, test_found)  # Return the earliest point among the two
        if ref_found is not None:
            return ref_found
        if test_found is not None:
            return test_found
    else:
        for i, (r, t) in enumerate(zip(ref, test)):
            if r >= 85 or t >= 85:
                return i
    return None

def f2_score(ref_vals, test_vals):
    diffs = np.array(test_vals[1:]) - np.array(ref_vals[1:])
    return 50 * math.log10(100.0 / (1.0 + np.sqrt(np.mean(diffs**2)))) if len(diffs) > 0 else 0.0

def optimal_timepoints(reference_df, test_df, regulation,
                          window_max=12, step_hours=0.25,
                          forced_times=(1.0,4.0, 5.0, 7.0)):
    # 1) build monotonic profiles
    rt, rm = calculate_mean_profile(reference_df)
    tt, tm = calculate_mean_profile(test_df)
    ref_prof, grid = create_monotonic_profile(rt, rm, step_hours, window_max)
    test_prof, _   = create_monotonic_profile(tt, tm, step_hours, window_max)

    # 2) regulatory 85% index
    idx85 = find_85_point(ref_prof, test_prof, regulation)

    # 3) map your forced_times (1h,7h) onto the grid
    forced_idxs = [int(np.argmin(np.abs(grid - ft))) for ft in forced_times]

    # 4) collect bracket candidates (with fallback)
    brackets = [(0,30), (30,60), (60,80)]
    bracket_cands = []
    for L, U in brackets:
        idxs = list(np.where((ref_prof >= L) & (ref_prof < U))[0])
        if not idxs:
            fb = list(np.where(ref_prof >= L)[0])
            if fb: idxs = [fb[0]]
        bracket_cands.append(idxs)

    # 5) enumerate up to 2 picks per bracket
    all_designs = []
    for c0 in itertools.combinations(bracket_cands[0], min(2, len(bracket_cands[0]))):
      for c1 in itertools.combinations(bracket_cands[1], min(2, len(bracket_cands[1]))):
        for c2 in itertools.combinations(bracket_cands[2], min(2, len(bracket_cands[2]))):
            base = {0, *c0, *c1, *c2, *forced_idxs}
            if idx85 is not None:
                base.add(idx85)
            all_designs.append(sorted(base))

    # 6) score each design, enforce ‚â•7% jumps
    best, best_f2 = None, -1
    for design in all_designs:
        times_, rvals_, tvals_ = [], [], []
        last_r = -np.inf
        for i in design:
            if abs(ref_prof[i] - last_r) < 7:
                continue
            times_.append(grid[i])
            rvals_.append(ref_prof[i])
            tvals_.append(test_prof[i])
            last_r = ref_prof[i]

        if len(rvals_) < 2:
            continue
        score = f2_score(rvals_, tvals_)
        if score > best_f2:
            best_f2, best = score, (times_, rvals_, tvals_)

    if best is None:
        return [], "No valid sampling design found."

    times_, rvals_, tvals_ = best
    # 7) clean up types and round
    sequence = [
        int(x) if float(x).is_integer() else round(float(x), 2)
        for x in times_
    ]

    return [{
        'sequence': sequence,
        'f2': round(best_f2, 2),
        'compliant': best_f2 >= 50,
        'reasons': [],
        'ref_vals': [round(float(v), 2) for v in rvals_],
        'test_vals':[round(float(v), 2) for v in tvals_]
    }], None



def R_Regulation(file_path):
    dr=read.read_excel(file_path,sheet='Sheet1')
    dt=read.read_excel(file_path,sheet='Sheet2')
    print("Enter Name of Regulation either EMA, FDA, WHO, Canada, ANVISA:")
    reg=input()
    b=bootf2.bootf2(dt,dr,file_out='test',regulation=reg)
    print(b.rx2('boot.summary'))
    print("Please check file created for full report")

def R_all_Regulation(file_path):
    dr=read.read_excel(file_path,sheet='Sheet1')
    dt=read.read_excel(file_path,sheet='Sheet2')
    b=bootf2.bootf2(dt,dr,file_out='EMA_Results',regulation="EMA/ICH/Canada/Australia")
    print("EMA Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='FDA_Results',regulation="US FDA")
    print("FDA Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='WHO_Results',regulation="WHO")
    print("WHO Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='Canada_Results',regulation="Canada")
    print("Canada Summary:")
    print(b.rx2('boot.summary'))
    b=bootf2.bootf2(dt,dr,file_out='ANVISA_Results',regulation="ANVISA")
    print("ANVISA Summary:")
    print(b.rx2('boot.summary'))
    print("Please check files created for full report")

def checks(reference_df,test_df):
    if check_time_points(test_df) and check_time_points(reference_df):
        print("Check 1: A minimum of three-time points - PASSED")
    else:
        print("Check 1: A minimum of three-time points - FAILED, NO f2 Calculated")

    if two_time_points(test_df) and two_time_points(reference_df):
        print("Check 2: 1,2 time points less than 85 percent dissoultion - PASSED")
    else:
        print("Check 2: 1,2 time points less than 85 percent dissoultion- FAILED, NO f2 Calculated")

    if min15__check(test_df) and min15__check(reference_df):
        print("Check 3: In 15 min their is greater than 85 percent dissolution - PASSED, NO f2 Calculated")
    else:
        print("Check 4: In 15 min their is No greater than 85 percent dissolution - FAILED")

    if check_same_time_points(test_df, reference_df):
        print("Check 4: The time points same for both products - PASSED")
    else:
        print("Check 4: The time points same for both products - FAILED")

    if check_sample_units(test_df) and check_sample_units(reference_df):
        print("Check 5: At least 12 individual sample units - PASSED")
    else:
        print("Check 5: At least 12 individual sample units - FAILED")

    print(" ")
    return_dissoltion_curve_fig = dissolution_curve(reference_df,test_df)
    
    return_dissoltion_curve_interval_fig =  dissolution_curve_interval(reference_df, test_df)
    print(return_dissoltion_curve_fig,"FIRST1")
    
    print(return_dissoltion_curve_interval_fig,"FIRST2")
    return return_dissoltion_curve_fig,return_dissoltion_curve_interval_fig
    # print(" ")
    # print("Choose Market:")
    # print("Choose 1 for FDA")
    # print("Choose 2 for EMA/ICH/Canda/AUs")
    # print("Choose 3 for China")
    # print("Choose 4 for ASEAN")
    # print("Choose 5 for ANVISa")

# input1 = int(input("Input number: "))

def check_both_85(reference_df, test_df):
    """Check if both reference and test meet the >= 85% criteria. Returns True if criteria is met, False otherwise."""
    ref_max = reference_df.iloc[:, 1:].mean(axis=1).max()
    test_max = test_df.iloc[:, 1:].mean(axis=1).max()
    return ref_max >= 85 and test_max >= 85

    
def check_either_85(reference_df, test_df):
    """Check if either reference or test meet the >= 85% criteria. Returns True if criteria is met, False otherwise."""
    ref_max = reference_df.iloc[:, 1:].mean(axis=1).max()
    test_max = test_df.iloc[:, 1:].mean(axis=1).max()
    return ref_max >= 85 or test_max >= 85


def f2_factor(ref_means: np.ndarray, test_means: np.ndarray) -> float:
    """
    Traditional similarity factor f2:
    f2 = 100 - 25 * log10( 1 + (1/P) * sum((test_means - ref_means)**2) )
    """
    if ref_means.shape != test_means.shape:
        raise ValueError("ref_means and test_means must have the same shape")
    P = len(ref_means)
    mse = np.mean((test_means - ref_means) ** 2)
    return 100 - 25 * np.log10(1 + mse)

def calculate_f2_fda(reference_df, test_df):
    """
    FDA ‚â• 85% rule:
    - Keep time = 0
    - Find the first timepoint (after t=0) where both reference AND test mean > 85%
    - Trim both DataFrames to include rows [0 .. that timepoint]
    - Compute:
    * f2_conv = conventional f2
    * exp_f2  = expected f2 using the variance‚Äêinclusive formula:
    f2 = 100 - 25 * log10(1
    + (1/P) * Œ£ (ŒºTi - ŒºRi)¬≤
    + (1/n) * Œ£ (S¬≤Ti + S¬≤Ri)
    )
    Returns:
    f2_conv, exp_f2, trimmed_ref_df, trimmed_test_df
    """
    
    # Drop zero time point data
    reference_df = reference_df[reference_df.iloc[:, 0] != 0].reset_index(drop=True)
    test_df = test_df[test_df.iloc[:, 0] != 0].reset_index(drop=True)
    
    # 1) Extract times & means
    times = reference_df.iloc[:, 0].astype(float).values
    ref_mean = reference_df.iloc[:, 1:].mean(axis=1).values
    test_mean = test_df.iloc[:, 1:].mean(axis=1).values
    
    # 2) Find first simultaneous >85% (after t=0)
    cutoff = next(
        (i for i in range(len(times))
         if ref_mean[i] > 85 and test_mean[i] > 85),
        len(times) - 1  # Default to the last timepoint if no simultaneous >85% found
    )
    
    # 3) Trim to [0..cutoff]
    ref_trim = reference_df.iloc[:cutoff+1].copy()
    test_trim = test_df.iloc[:cutoff+1].copy()
    
    # 4) Force zero at t=0
    if float(ref_trim.iloc[0, 0]) == 0.0:
        ref_trim.iloc[0, 1:] = 0.0
        test_trim.iloc[0, 1:] = 0.0
    
    # 5) Prepare arrays
    ref_means_arr = ref_trim.iloc[:, 1:].mean(axis=1).values
    test_means_arr = test_trim.iloc[:, 1:].mean(axis=1).values
    P = len(ref_means_arr)
    n = ref_trim.shape[1] - 1  # number of replicates per timepoint
    
    # 6a) Conventional f2
    f2_conv = f2_factor(ref_means_arr, test_means_arr)
    
    # 6b) Expected f2 with variance term
    #   (1/P) * sum(diff^2)
    mse_term = np.mean((test_means_arr - ref_means_arr) ** 2)
    #   (1/n) * sum(S¬≤Ti + S¬≤Ri)
    var_ref = ref_trim.iloc[:, 1:].var(axis=1, ddof=1).values
    var_test = test_trim.iloc[:, 1:].var(axis=1, ddof=1).values
    variance_term = (np.sum(var_ref + var_test) / n)
    exp_f2 = 100 - 25 * np.log10(1 + mse_term + variance_term)
    
    return f2_conv, exp_f2, ref_trim, test_trim



def main_function(input1,test_df,reference_df):
    input1 = "US FDA" if input1 == "FDA" else input1
    reference_mean = reference_df.iloc[:, 1:].mean(axis=1)
    test_mean = test_df.iloc[:, 1:].mean(axis=1)
    reference_mean_df = pd.DataFrame({
        reference_df.columns[0]: reference_df.iloc[:, 0],
        'Dissolution': reference_mean
    })
    test_mean_df = pd.DataFrame({
        test_df.columns[0]: test_df.iloc[:, 0],
        'Dissolution': test_mean
    })
    # breakpoint()
    try:
        # result_fig1,result_fig2 = checks(reference_df,test_df)
        if input1 == "US FDA":
            print("According to FDA ‚â•¬†85% guidelines\n")

            # 1) Check criteria & CV
            both_85   = check_both_85(reference_df, test_df)
            # either_85 = check_either_85(reference_df, test_df)
            print("Both ‚â•¬†85% criterion met:  ", both_85)
            # print("Either ‚â•¬†85% criterion met:", either_85)

            cv_check = check_cv(test_df) and check_cv(reference_df)
            print("\nCV <20 at first non-zero, <10 thereafter:", cv_check, "\n")

            #(0)Timepoint drop
            reference_df = reference_df[reference_df.iloc[:, 0] != 0].reset_index(drop=True)
            test_df = test_df[test_df.iloc[:, 0] != 0].reset_index(drop=True)

            # 2) Verify time alignment
            if not check_same_time_points(reference_df, test_df):
                print("Error: Time points between test and reference do not match.")
                print("Calculations cannot be performed.")
            else:
                print("IN ELSE")
                try:
                    print("IN TRY")
                    # 3) Compute f2s & trimmed data
                    f2_conv, f2_exp, ref_trim, test_trim = calculate_f2_fda(reference_df, test_df)
                    print(f2_conv,"F2 CONV IN RLSE TRY")
                    if isinstance(f2_conv,str):
                        recom_f2,plotly_fig,ai_df = None, None, None
                        if cv_check == True:
                            cv_checks = "True"
                        else:
                            cv_checks = "False"
                        return f2_conv,f2_exp,ref_trim,test_trim,recom_f2,plotly_fig,cv_checks,ai_df
                    changed_both_f2s = round(f2_conv,2)


                    # # 4) Show trimmed times & means
                    times      = ref_trim.iloc[:, 0].astype(float).tolist()
                    ref_means  = ref_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()
                    test_means = test_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()

                    max_time = max(times)

                    # Set x_dtick based on the conditions
                    if max_time > 60:
                        x_dtick = 10
                    elif max_time > 30: # This covers 30 < max_time <= 60
                        x_dtick = 5
                    elif max_time > 12: # This covers 12 < max_time <= 30 (a reasonable interval for this unspecified range)
                        x_dtick = 2.5
                    else: # This covers 0 <= max_time <= 12
                        x_dtick = 1

                    print("Trimmed times:       ", times)
                    print("Reference means (%): ", ref_means)
                    print("Test means      (%): ", test_means, "\n")

                    # 5) Print f2 results
                    print(f"Conventional f2: {f2_conv:.2f}")
                    print(f"Expected    f2: {f2_exp:.2f}")
                    if not cv_check:
                        print("\nWarning: CV requirements not met; interpret with caution.")

                    # --- Graph 1: Simple dissolution curves ---
                    
                    # Create a Plotly figure
                    result_fig1 = go.Figure()

                    # Add Reference data trace
                    result_fig1.add_trace(go.Scatter(
                    x=times,
                    y=ref_means,
                    mode='lines+markers',
                    name='Reference',
                    marker=dict(color='blue', symbol='circle'),
                    line=dict(dash='solid')
                    ))

                    # Add Test data trace
                    result_fig1.add_trace(go.Scatter(
                    x=times,
                    y=test_means,
                    mode='lines+markers',
                    name='Test',
                    marker=dict(color='red', symbol='circle'),
                    line=dict(dash='dash')
                    ))

                    # Customize layout
                    result_fig1.update_layout(
                    title='Dissolution Curves',
                    xaxis=dict(title='Time', dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                    yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                    legend_title='Legend',
                    plot_bgcolor='white',
                    paper_bgcolor='white'
                    )


                    # --- Graph 2: Curves with min/max intervals ---
                    
                    # Calculate min and max values for reference and test data
                    ref_min = ref_trim.iloc[:, 1:].min(axis=1)
                    ref_max = ref_trim.iloc[:, 1:].max(axis=1)
                    tst_min = test_trim.iloc[:, 1:].min(axis=1)
                    tst_max = test_trim.iloc[:, 1:].max(axis=1)

                    # Create a Plotly figure
                    result_fig2 = go.Figure()

                    # Add Reference Mean data trace with error bars
                    result_fig2.add_trace(go.Scatter(
                        x=times,
                        y=ref_means,
                        mode='lines+markers',
                        name='Reference Mean',
                        error_y=dict(
                        type='data',
                        symmetric=False,
                        array=[ma - rm for ma, rm in zip(ref_max, ref_means)],
                        arrayminus=[rm - mi for rm, mi in zip(ref_means, ref_min)]
                        ),
                        marker=dict(color='blue', symbol='circle'),
                        line=dict(dash='solid')
                    ))

                    # Add Test Mean data trace with error bars
                    result_fig2.add_trace(go.Scatter(
                        x=times,
                        y=test_means,
                        mode='lines+markers',
                        name='Test Mean',
                        error_y=dict(
                        type='data',
                        symmetric=False,
                        array=[ma - tm for ma, tm in zip(tst_max, test_means)],
                        arrayminus=[tm - mi for tm, mi in zip(test_means, tst_min)]
                        ),
                        marker=dict(color='red', symbol='square'),
                        line=dict(dash='dash')
                    ))

                    # Customize layout
                    result_fig2.update_layout(
                        title='Dissolution Curves with Intervals',
                        xaxis=dict(title='Time', zeroline=True, dtick = x_dtick, zerolinewidth=1, zerolinecolor='black'),
                        yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                        legend_title='Legend',
                        plot_bgcolor='white',
                        paper_bgcolor='white'
                    )

                except ValueError as e:
                    print("‚ùå", e)
                    print("Cannot perform FDA‚Äërule f2 calculation.")
                

                
                reference_mean_df = reference_mean_df.apply(pd.to_numeric, errors='coerce')
                test_mean_df = test_mean_df.apply(pd.to_numeric, errors='coerce')
                cross_85 = any((ref > 85) and (test > 85) for ref, test in zip(reference_mean_df.values.flatten(), test_mean_df.values.flatten()))

                print(cross_85,"CROSS 85")

                print(reference_mean_df,"REFERENCE DF")
                print(test_mean_df,"TEST MEAN DF")

                if not cross_85:
                    print(f"Conventional f2 is the best f2.")
                    print("Using conventional time points and dissolution percentages.")
                    print(f"Time Points: {times}")
                    print(f"Reference means (%): {ref_means}")
                    print(f"Test means (%): {test_means}")
                    print(f"Conventional f2 Score: {f2_conv:.2f}")

                    ai_df = pd.DataFrame({
                            "Time Point (h)": times,
                            "Test Dissolution (%)": test_means,
                            "Reference Dissolution (%)": ref_means
                    }).round(2)
                    
                    # Plot conventional curves
                    # Create a Plotly figure
                    plotly_fig = go.Figure()

                    # Add Reference data trace
                    plotly_fig.add_trace(go.Scatter(
                    x=times,
                    y=ref_means,
                    mode='lines+markers',
                    name='Reference',
                    marker=dict(color='blue', symbol='circle'),
                    line=dict(dash='solid')
                    ))

                    # Add Test data trace
                    plotly_fig.add_trace(go.Scatter(
                    x=times,
                    y=test_means,
                    mode='lines+markers',
                    name='Test',
                    marker=dict(color='red', symbol='circle'),
                    line=dict(dash='dash')
                    ))

                    # Customize layout
                    plotly_fig.update_layout(
                    title=f"Optimal Profile (f2 = {f2_conv:.2f})",
                    xaxis=dict(title='Time', zeroline=True,dtick=x_dtick,zerolinewidth=1, zerolinecolor='black'),
                    yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                    legend_title='Legend',
                    plot_bgcolor='white',
                    paper_bgcolor='white'
                    )
                    seq_print = None,
                    recom_f2 = round(f2_conv,2)
                    if cv_check == True:
                        cv_checks = "True"
                    else:
                        cv_checks = "False"
                    return changed_both_f2s,result_fig1,result_fig2,seq_print,recom_f2,plotly_fig,cv_checks,ai_df
                else:
                    try:
                        window_min, window_max = determine_candidate_window(
                            reference_mean_df, test_mean_df, step=5, initial_threshold=10
                        )
                        # first, original
                        results, all_results = predictive_optimal_combinations_advanced(
                            reference_mean_df,
                            test_mean_df,
                            regulation         = input1,
                            window_min         = window_min,
                            window_max         = window_max,
                            diff_threshold     = None,
                            interp_method      = 'linear',
                            points_per_stratum = None
                        )
                        print("‚úÖ Used original integer‚Äêgrid function\n")
                    except ValueError:
                        results, _ = optimal_timepoints(
                            reference_mean_df, test_mean_df,
                            input1, window_max=12,
                            step_hours=0.25
                        )
                    
                
                    for cand in results:
                        cand['sequence'] = [
                            int(t) if float(t).is_integer() else round(t, 2) 
                            for t in cand['sequence']
                        ]

                    best = results[0]
                    print("=== Optimal Predictive Combination ===")
                
                    if best['f2'] < f2_conv:
                        print(f"Conventional f2 is the best f2.")
                        print("Using conventional time points and dissolution percentages.")
                        print(f"Time Points: {times}")
                        print(f"Reference means (%): {ref_means}")
                        print(f"Test means (%): {test_means}")
                        print(f"Conventional f2 Score: {f2_conv:.2f}")

                        ai_df = pd.DataFrame({
                            "Time Point (h)": times,
                            "Test Dissolution (%)": test_means,
                            "Reference Dissolution (%)": ref_means
                        }).round(2)
                    
                        # Create a Plotly figure
                        plotly_fig = go.Figure()

                        # Add Reference data trace
                        plotly_fig.add_trace(go.Scatter(
                        x=times,
                        y=ref_means,
                        mode='lines+markers',
                        name='Reference',
                        marker=dict(color='blue', symbol='circle'),
                        line=dict(dash='solid')
                        ))

                        # Add Test data trace
                        plotly_fig.add_trace(go.Scatter(
                        x=times,
                        y=test_means,
                        mode='lines+markers',
                        name='Test',
                        marker=dict(color='red', symbol='circle'),
                        line=dict(dash='dash')
                        ))

                        # Customize layout
                        plotly_fig.update_layout(
                        title=f"Optimal Profile (f2 = {f2_conv:.2f})",
                        xaxis=dict(title='Time', zeroline=True, dtick=x_dtick, zerolinewidth=1, zerolinecolor='black'),
                        yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                        legend_title='Legend',
                        plot_bgcolor='white',
                        paper_bgcolor='white'
                        )

                        seq_print = None,
                        recom_f2 = round(f2_conv,2)
                    else:
                        sequence = [int(t) if float(t).is_integer() else float(t) for t in best['sequence']]
                        print(f"Time Points: {best['sequence']}")
                        print(f"Predicted f2 Score: {best['f2']}")
                    
                        # Interpolating and plotting predicted curves
                        seq      = best['sequence']
                        ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(seq), method='linear')
                        test_diss= interpolate_dissolution_curve(test_mean_df,      np.array(seq), method='linear')
                        # force 0% at start
                        ref_diss[0] = test_diss[0] = 0.0
            
                        # Create a Plotly figure
                        plotly_fig = go.Figure()

                        # Add Reference data trace
                        plotly_fig.add_trace(go.Scatter(
                            x=seq,
                            y=ref_diss,
                            mode='lines+markers',
                            name='Reference',
                            marker=dict(color='blue', symbol='circle'),
                            line=dict(dash='solid')
                        ))

                        # Add Test data trace
                        plotly_fig.add_trace(go.Scatter(
                            x=seq,
                            y=test_diss,
                            mode='lines+markers',
                            name='Test',
                            marker=dict(color='red', symbol='star'),
                            line=dict(dash='dash')
                        ))

                        # Customize layout
                        plotly_fig.update_layout(
                            title=f"Optimal Profile (f2 = {best['f2']})",
                            xaxis=dict(title='Time (h)', zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                            yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                            legend_title='Legend',
                            plot_bgcolor='white',
                            paper_bgcolor='white'
                        )



                        # Print predicted values
                        print("\nPredicted Reference Dissolution (%):")
                        for t, d in zip(seq, ref_diss):
                            print(f"  {t} h: {d:.2f}%")
                        print("\nPredicted Test Dissolution (%):")
                        for t, d in zip(seq, test_diss):
                            print(f"  {t} h: {d:.2f}%")

                        ai_df = pd.DataFrame({
                            "Time Point (h)": seq,
                            "Test Dissolution (%)": test_diss,
                            "Reference Dissolution (%)": ref_diss
                        }).round(2)

                        # List all candidates
                        print("\n=== All Candidate Combinations ===")
                        for i, cand in enumerate(results, 1):
                            seq_print = cand['sequence']
                            recom_f2 = cand['f2']
                            print(f"{i:2d}. Points: {cand['sequence']} | f2: {cand['f2']} | Compliant: {cand['compliant']}")
                        
                        

                    if cv_check == True:
                        cv_checks = "True"
                    else:
                        cv_checks = "False"
                    return changed_both_f2s,result_fig1,result_fig2,seq_print,recom_f2,plotly_fig,cv_checks,ai_df
  
        elif input1 == "EMA/ICH/Canada/Australia":
            print("According to EMA/ICH/Canada/Australia guidelines")

            # 1) CV check
            cv_check = check_cv(test_df) and check_cv(reference_df)
            print("CV <20 at first non-zero, <10 thereafter:", cv_check)
            
            #(0)Timepoint drop
            reference_df = reference_df[reference_df.iloc[:, 0] != 0].reset_index(drop=True)
            test_df = test_df[test_df.iloc[:, 0] != 0].reset_index(drop=True)

            # 2) Verify time alignment
            if not check_same_time_points(test_df, reference_df):
                print("Error: Time points between test and reference do not match.")
            else:
                print("\nAnalysis based on ‚Äòeither >85%‚Äô dissolution criterion:")
                try:
                    # Compute f2s and get trimmed DataFrames
                    f2_conv, f2_exp, ref_trim, test_trim = calculate_f2_either85(reference_df, test_df)
                    if isinstance(f2_conv,str):
                        recom_f2,plotly_fig,ai_df = None, None, None
                        if cv_check == True:
                            cv_checks = "True"
                        else:
                            cv_checks = "False"
                        return f2_conv,f2_exp,ref_trim,test_trim,recom_f2,plotly_fig,cv_checks,ai_df


                    changed_both_f2s = round(f2_conv,2)
                    # Print results
                    print(f"Conventional f2: {f2_conv:.2f}")
                    print(f"Expected    f2: {f2_exp:.2f}")
                    # 4) Show trimmed times & means
                    # times      = ref_trim.iloc[:, 0].astype(float).tolist()
                    # ref_means  = ref_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()
                    # test_means = test_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()
                    # print("Trimmed times:       ", times)
                    # print("Reference means (%): ", ref_means)
                    # print("Test means      (%): ", test_means, "\n")

                    if not cv_check:
                        print("\nWarning: CV requirements not met; interpret with caution.")

                    # --- Graph 1: Simple dissolution curves ---
                    times      = ref_trim.iloc[:, 0].astype(float)
                    ref_means  = ref_trim.iloc[:, 1:].mean(axis=1)
                    test_means = test_trim.iloc[:, 1:].mean(axis=1)

                    max_time = max(times)

                    # Set x_dtick based on the conditions
                    if max_time > 60:
                        x_dtick = 10
                    elif max_time > 30: # This covers 30 < max_time <= 60
                        x_dtick = 5
                    elif max_time > 12: # This covers 12 < max_time <= 30 (a reasonable interval for this unspecified range)
                        x_dtick = 2.5
                    else: # This covers 0 <= max_time <= 12
                        x_dtick = 1

                    # Create a Plotly figure
                    result_fig1 = go.Figure()

                    # Add Reference data trace
                    result_fig1.add_trace(go.Scatter(
                    x=times,
                    y=ref_means,
                    mode='lines+markers',
                    name='Reference',
                    marker=dict(color='blue', symbol='circle'),
                    line=dict(dash='solid')
                    ))

                    # Add Test data trace
                    result_fig1.add_trace(go.Scatter(
                    x=times,
                    y=test_means,
                    mode='lines+markers',
                    name='Test',
                    marker=dict(color='red', symbol='circle'),
                    line=dict(dash='dash')
                    ))

                    # Customize layout
                    result_fig1.update_layout(
                    title='Dissolution Curves',
                    xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                    yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                    legend_title='Legend',
                    plot_bgcolor='white',
                    paper_bgcolor='white'
                    )

                    # --- Graph 2: Curves with min/max intervals ---
                    ref_min  = ref_trim.iloc[:, 1:].min(axis=1)
                    ref_max  = ref_trim.iloc[:, 1:].max(axis=1)
                    tst_min = test_trim.iloc[:, 1:].min(axis=1)
                    tst_max = test_trim.iloc[:, 1:].max(axis=1)

                    # Create a Plotly figure
                    result_fig2 = go.Figure()

                    # Add Reference Mean data trace with error bars
                    result_fig2.add_trace(go.Scatter(
                        x=times,
                        y=ref_means,
                        mode='lines+markers',
                        name='Reference Mean',
                        error_y=dict(
                        type='data',
                        symmetric=False,
                        array=[ma - rm for ma, rm in zip(ref_max, ref_means)],
                        arrayminus=[rm - mi for rm, mi in zip(ref_means, ref_min)]
                        ),
                        marker=dict(color='blue', symbol='circle'),
                        line=dict(dash='solid')
                    ))

                    # Add Test Mean data trace with error bars
                    result_fig2.add_trace(go.Scatter(
                        x=times,
                        y=test_means,
                        mode='lines+markers',
                        name='Test Mean',
                        error_y=dict(
                        type='data',
                        symmetric=False,
                        array=[ma - tm for ma, tm in zip(tst_max, test_means)],
                        arrayminus=[tm - mi for tm, mi in zip(test_means, tst_min)]
                        ),
                        marker=dict(color='red', symbol='square'),
                        line=dict(dash='dash')
                    ))

                    # Customize layout
                    result_fig2.update_layout(
                        title='Dissolution Curves with Intervals',
                        xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                        yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                        legend_title='Legend',
                        plot_bgcolor='white',
                        paper_bgcolor='white'
                    )

                except ValueError as e:
                    print("‚ùå", e)
                    print("Cannot perform ‚Äòeither >85%‚Äô‚Äìbased f2 calculation.")

                reference_mean_df = reference_mean_df.apply(pd.to_numeric, errors='coerce')
                test_mean_df = test_mean_df.apply(pd.to_numeric, errors='coerce')
                cross_85 = any((ref > 85) and (test > 85) for ref, test in zip(reference_mean_df.values.flatten(), test_mean_df.values.flatten()))
            

                if not cross_85:
                    print(f"Conventional f2 is the best f2.")
                    print("Using conventional time points and dissolution percentages.")
                    print(f"Time Points: {times}")
                    print(f"Reference means (%): {ref_means}")
                    print(f"Test means (%): {test_means}")
                    print(f"Conventional f2 Score: {f2_conv:.2f}")

                    ai_df = pd.DataFrame({
                            "Time Point (h)": times,
                            "Test Dissolution (%)": test_means,
                            "Reference Dissolution (%)": ref_means
                    }).round(2)
                    
                    # Plot conventional curves
                    # Create a Plotly figure
                    plotly_fig = go.Figure()

                    # Add Reference data trace
                    plotly_fig.add_trace(go.Scatter(
                    x=times,
                    y=ref_means,
                    mode='lines+markers',
                    name='Reference',
                    marker=dict(color='blue', symbol='circle'),
                    line=dict(dash='solid')
                    ))

                    # Add Test data trace
                    plotly_fig.add_trace(go.Scatter(
                    x=times,
                    y=test_means,
                    mode='lines+markers',
                    name='Test',
                    marker=dict(color='red', symbol='circle'),
                    line=dict(dash='dash')
                    ))

                    # Customize layout
                    plotly_fig.update_layout(
                    title=f"Optimal Profile (f2 = {f2_conv:.2f})",
                    xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                    yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                    legend_title='Legend',
                    plot_bgcolor='white',
                    paper_bgcolor='white'
                    )
                    seq_print = None,
                    recom_f2 = round(f2_conv,2)
                    if cv_check == True:
                        cv_checks = "True"
                    else:
                        cv_checks = "False"
                    return changed_both_f2s,result_fig1,result_fig2,seq_print,recom_f2,plotly_fig,cv_checks,ai_df
                else:
                    try:
                        window_min, window_max = determine_candidate_window(
                            reference_mean_df, test_mean_df, step=5, initial_threshold=10
                        )
                        # first, original
                        results, all_results = predictive_optimal_combinations_advanced(
                            reference_mean_df,
                            test_mean_df,
                            regulation         = input1,
                            window_min         = window_min,
                            window_max         = window_max,
                            diff_threshold     = None,
                            interp_method      = 'linear',
                            points_per_stratum = None
                        )
                        print("‚úÖ Used original integer‚Äêgrid function\n")
                    except ValueError:
                        results, _ = optimal_timepoints(
                            reference_mean_df, test_mean_df,
                            input1, window_max=12,
                            step_hours=0.25
                        )
                    for cand in results:
                        cand['sequence'] = [
                            int(t) if float(t).is_integer() else round(t, 2) 
                            for t in cand['sequence']
                        ]

                    best = results[0]
                    print("=== Optimal Predictive Combination ===")
                    if best['f2'] < f2_conv:
                        print(f"Conventional f2 is the best f2.")
                        print("Using conventional time points and dissolution percentages.")
                        print(f"Time Points: {times}")
                        print(f"Reference means (%): {ref_means}")
                        print(f"Test means (%): {test_means}")
                        print(f"Conventional f2 Score: {f2_conv:.2f}")

                        ai_df = pd.DataFrame({
                            "Time Point (h)": times,
                            "Test Dissolution (%)": test_means,
                            "Reference Dissolution (%)": ref_means
                        }).round(2)
                    
                        # Create a Plotly figure
                        plotly_fig = go.Figure()

                        # Add Reference data trace
                        plotly_fig.add_trace(go.Scatter(
                        x=times,
                        y=ref_means,
                        mode='lines+markers',
                        name='Reference',
                        marker=dict(color='blue', symbol='circle'),
                        line=dict(dash='solid')
                        ))

                        # Add Test data trace
                        plotly_fig.add_trace(go.Scatter(
                        x=times,
                        y=test_means,
                        mode='lines+markers',
                        name='Test',
                        marker=dict(color='red', symbol='circle'),
                        line=dict(dash='dash')
                        ))

                        # Customize layout
                        plotly_fig.update_layout(
                        title=f"Optimal Profile (f2 = {f2_conv:.2f})",
                        xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                        yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                        legend_title='Legend',
                        plot_bgcolor='white',
                        paper_bgcolor='white'
                        )

                        seq_print = None,
                        recom_f2 = round(f2_conv,2)
                    else:
                        print(f"Time Points: {best['sequence']}")
                        print(f"Predicted f2 Score: {best['f2']}")

                        # Plot predicted curves
                        import matplotlib.pyplot as plt
                        from matplotlib.ticker import MaxNLocator

                        seq      = best['sequence']
                        ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(seq), method='linear')
                        test_diss= interpolate_dissolution_curve(test_mean_df, np.array(seq), method='linear')
                        # force 0% at start
                        ref_diss[0] = test_diss[0] = 0.0

                        # Create a Plotly figure
                        plotly_fig = go.Figure()

                        # Add Reference data trace
                        plotly_fig.add_trace(go.Scatter(
                            x=seq,
                            y=ref_diss,
                            mode='lines+markers',
                            name='Reference',
                            marker=dict(color='blue', symbol='circle'),
                            line=dict(dash='solid')
                        ))

                        # Add Test data trace
                        plotly_fig.add_trace(go.Scatter(
                            x=seq,
                            y=test_diss,
                            mode='lines+markers',
                            name='Test',
                            marker=dict(color='red', symbol='star'),
                            line=dict(dash='dash')
                        ))

                        # Customize layout
                        plotly_fig.update_layout(
                            title=f"Optimal Profile (f2 = {best['f2']})",
                            xaxis=dict(title='Time (h)', zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                            yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                            legend_title='Legend',
                            plot_bgcolor='white',
                            paper_bgcolor='white'
                        )

                        # Print predicted values
                        print("\nPredicted Reference Dissolution (%):")
                        for t, d in zip(seq, ref_diss):
                            print(f"  {t} h: {d:.2f}%")
                        print("\nPredicted Test Dissolution (%):")
                        for t, d in zip(seq, test_diss):
                            print(f"  {t} h: {d:.2f}%")

                        ai_df = pd.DataFrame({
                            "Time Point (h)": seq,
                            "Test Dissolution (%)": test_diss,
                            "Reference Dissolution (%)": ref_diss
                        }).round(2)

                        # List all candidates
                        print("\n=== All Candidate Combinations ===")
                        for i, cand in enumerate(results, 1):
                            seq_print = cand['sequence']
                            recom_f2 = cand['f2']
                            print(f"{i:2d}. Points: {cand['sequence']} | f2: {cand['f2']} | Compliant: {cand['compliant']}")

                    if cv_check == True:
                        cv_checks = "True"
                    else:
                        cv_checks = "False"
                    return changed_both_f2s,result_fig1,result_fig2,seq_print,recom_f2,plotly_fig,cv_checks,ai_df
        
        elif input1 == "China":
            print("According to China guidelines")

            # 1) CV check
            cv_check = check_cv(test_df) and check_cv(reference_df)
            print("CV <20 at first non-zero, <10 thereafter:", cv_check)
            
            #(0)Timepoint drop
            reference_df = reference_df[reference_df.iloc[:, 0] != 0].reset_index(drop=True)
            test_df = test_df[test_df.iloc[:, 0] != 0].reset_index(drop=True)

            # 2) Verify time alignment
            if not check_same_time_points(test_df, reference_df):
                print("Error: Time points between test and reference do not match.")
            else:
                print("\nAnalysis based on ‚Äòeither >85%‚Äô dissolution criterion:")
                try:
                    # Compute f2s and get trimmed DataFrames
                    f2_conv, f2_exp, ref_trim, test_trim = calculate_f2_either85(reference_df, test_df)
                    if isinstance(f2_conv,str):
                        recom_f2,plotly_fig,ai_df = None, None, None
                        if cv_check == True:
                            cv_checks = "True"
                        else:
                            cv_checks = "False"
                        return f2_conv,f2_exp,ref_trim,test_trim,recom_f2,plotly_fig,cv_checks,ai_df
                    changed_both_f2s = round(f2_conv,2)
                    # Print results
                    print(f"Conventional f2: {f2_conv:.2f}")
                    print(f"Expected    f2: {f2_exp:.2f}")

                    if not cv_check:
                        print("\nWarning: CV requirements not met; interpret with caution.")

                    # --- Graph 1: Simple dissolution curves ---
                    times      = ref_trim.iloc[:, 0].astype(float)
                    ref_means  = ref_trim.iloc[:, 1:].mean(axis=1)
                    test_means = test_trim.iloc[:, 1:].mean(axis=1)

                    max_time = max(times)

                    # Set x_dtick based on the conditions
                    if max_time > 60:
                        x_dtick = 10
                    elif max_time > 30: # This covers 30 < max_time <= 60
                        x_dtick = 5
                    elif max_time > 12: # This covers 12 < max_time <= 30 (a reasonable interval for this unspecified range)
                        x_dtick = 2.5
                    else: # This covers 0 <= max_time <= 12
                        x_dtick = 1

                    # Create a Plotly figure
                    result_fig1 = go.Figure()

                    # Add Reference data trace
                    result_fig1.add_trace(go.Scatter(
                    x=times,
                    y=ref_means,
                    mode='lines+markers',
                    name='Reference',
                    marker=dict(color='blue', symbol='circle'),
                    line=dict(dash='solid')
                    ))

                    # Add Test data trace
                    result_fig1.add_trace(go.Scatter(
                    x=times,
                    y=test_means,
                    mode='lines+markers',
                    name='Test',
                    marker=dict(color='red', symbol='circle'),
                    line=dict(dash='dash')
                    ))

                    # Customize layout
                    result_fig1.update_layout(
                    title='Dissolution Curves',
                    xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                    yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                    legend_title='Legend',
                    plot_bgcolor='white',
                    paper_bgcolor='white'
                    )

                    # --- Graph 2: Curves with min/max intervals ---
                    ref_min  = ref_trim.iloc[:, 1:].min(axis=1)
                    ref_max  = ref_trim.iloc[:, 1:].max(axis=1)
                    tst_min = test_trim.iloc[:, 1:].min(axis=1)
                    tst_max = test_trim.iloc[:, 1:].max(axis=1)

                    # Create a Plotly figure
                    result_fig2 = go.Figure()

                    # Add Reference Mean data trace with error bars
                    result_fig2.add_trace(go.Scatter(
                        x=times,
                        y=ref_means,
                        mode='lines+markers',
                        name='Reference Mean',
                        error_y=dict(
                        type='data',
                        symmetric=False,
                        array=[ma - rm for ma, rm in zip(ref_max, ref_means)],
                        arrayminus=[rm - mi for rm, mi in zip(ref_means, ref_min)]
                        ),
                        marker=dict(color='blue', symbol='circle'),
                        line=dict(dash='solid')
                    ))

                    # Add Test Mean data trace with error bars
                    result_fig2.add_trace(go.Scatter(
                        x=times,
                        y=test_means,
                        mode='lines+markers',
                        name='Test Mean',
                        error_y=dict(
                        type='data',
                        symmetric=False,
                        array=[ma - tm for ma, tm in zip(tst_max, test_means)],
                        arrayminus=[tm - mi for tm, mi in zip(test_means, tst_min)]
                        ),
                        marker=dict(color='red', symbol='square'),
                        line=dict(dash='dash')
                    ))

                    # Customize layout
                    result_fig2.update_layout(
                        title='Dissolution Curves with Intervals',
                        xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                        yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                        legend_title='Legend',
                        plot_bgcolor='white',
                        paper_bgcolor='white'
                    )

                except ValueError as e:
                    print("‚ùå", e)
                    print("Cannot perform ‚Äòeither >85%‚Äô‚Äìbased f2 calculation.")

                reference_mean_df = reference_mean_df.apply(pd.to_numeric, errors='coerce')
                test_mean_df = test_mean_df.apply(pd.to_numeric, errors='coerce')
                cross_85 = any((ref > 85) and (test > 85) for ref, test in zip(reference_mean_df.values.flatten(), test_mean_df.values.flatten()))
            

                if not cross_85:
                    print(f"Conventional f2 is the best f2.")
                    print("Using conventional time points and dissolution percentages.")
                    print(f"Time Points: {times}")
                    print(f"Reference means (%): {ref_means}")
                    print(f"Test means (%): {test_means}")
                    print(f"Conventional f2 Score: {f2_conv:.2f}")

                    ai_df = pd.DataFrame({
                            "Time Point (h)": times,
                            "Test Dissolution (%)": test_means,
                            "Reference Dissolution (%)": ref_means
                    }).round(2)
                    
                    # Plot conventional curves
                    # Create a Plotly figure
                    plotly_fig = go.Figure()

                    # Add Reference data trace
                    plotly_fig.add_trace(go.Scatter(
                    x=times,
                    y=ref_means,
                    mode='lines+markers',
                    name='Reference',
                    marker=dict(color='blue', symbol='circle'),
                    line=dict(dash='solid')
                    ))

                    # Add Test data trace
                    plotly_fig.add_trace(go.Scatter(
                    x=times,
                    y=test_means,
                    mode='lines+markers',
                    name='Test',
                    marker=dict(color='red', symbol='circle'),
                    line=dict(dash='dash')
                    ))

                    # Customize layout
                    plotly_fig.update_layout(
                    title=f"Optimal Profile (f2 = {f2_conv:.2f})",
                    xaxis=dict(title='Time',x_dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                    yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                    legend_title='Legend',
                    plot_bgcolor='white',
                    paper_bgcolor='white'
                    )
                    seq_print = None,
                    recom_f2 = round(f2_conv,2)
                    if cv_check == True:
                        cv_checks = "True"
                    else:
                        cv_checks = "False"
                    return changed_both_f2s,result_fig1,result_fig2,seq_print,recom_f2,plotly_fig,cv_checks,ai_df
                else:
                    try:
                        window_min, window_max = determine_candidate_window(
                            reference_mean_df, test_mean_df, step=5, initial_threshold=10
                        )
                        # first, original
                        results, all_results = predictive_optimal_combinations_advanced(
                            reference_mean_df,
                            test_mean_df,
                            regulation         = input1,
                            window_min         = window_min,
                            window_max         = window_max,
                            diff_threshold     = None,
                            interp_method      = 'linear',
                            points_per_stratum = None
                        )
                        print("‚úÖ Used original integer‚Äêgrid function\n")
                    except ValueError:
                        results, _ = optimal_timepoints(
                            reference_mean_df, test_mean_df,
                            input1, window_max=12,
                            step_hours=0.25
                        )
                    for cand in results:
                        cand['sequence'] = [
                            int(t) if float(t).is_integer() else round(t, 2) 
                            for t in cand['sequence']
                        ]

                    best = results[0]
                    print("=== Optimal Predictive Combination ===")
                    print(f"Time Points: {best['sequence']}")
                    print(f"Predicted f2 Score: {best['f2']}")

                    if best['f2'] < f2_conv:
                        print(f"Conventional f2 is the best f2.")
                        print("Using conventional time points and dissolution percentages.")
                        print(f"Time Points: {times}")
                        print(f"Reference means (%): {ref_means}")
                        print(f"Test means (%): {test_means}")
                        print(f"Conventional f2 Score: {f2_conv:.2f}")

                        ai_df = pd.DataFrame({
                            "Time Point (h)": times,
                            "Test Dissolution (%)": test_means,
                            "Reference Dissolution (%)": ref_means
                        }).round(2)
                    
                        # Create a Plotly figure
                        plotly_fig = go.Figure()

                        # Add Reference data trace
                        plotly_fig.add_trace(go.Scatter(
                        x=times,
                        y=ref_means,
                        mode='lines+markers',
                        name='Reference',
                        marker=dict(color='blue', symbol='circle'),
                        line=dict(dash='solid')
                        ))

                        # Add Test data trace
                        plotly_fig.add_trace(go.Scatter(
                        x=times,
                        y=test_means,
                        mode='lines+markers',
                        name='Test',
                        marker=dict(color='red', symbol='circle'),
                        line=dict(dash='dash')
                        ))

                        # Customize layout
                        plotly_fig.update_layout(
                        title=f"Optimal Profile (f2 = {f2_conv:.2f})",
                        xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                        yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                        legend_title='Legend',
                        plot_bgcolor='white',
                        paper_bgcolor='white'
                        )

                        seq_print = None,
                        recom_f2 = round(f2_conv,2)
                    else:
                        # Plot predicted curves
                        import matplotlib.pyplot as plt
                        from matplotlib.ticker import MaxNLocator

                        seq      = best['sequence']
                        ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(seq), method='linear')
                        test_diss= interpolate_dissolution_curve(test_mean_df, np.array(seq), method='linear')
                        # force 0% at start
                        ref_diss[0] = test_diss[0] = 0.0

                        # Create a Plotly figure
                        plotly_fig = go.Figure()

                        # Add Reference data trace
                        plotly_fig.add_trace(go.Scatter(
                            x=seq,
                            y=ref_diss,
                            mode='lines+markers',
                            name='Reference',
                            marker=dict(color='blue', symbol='circle'),
                            line=dict(dash='solid')
                        ))

                        # Add Test data trace
                        plotly_fig.add_trace(go.Scatter(
                            x=seq,
                            y=test_diss,
                            mode='lines+markers',
                            name='Test',
                            marker=dict(color='red', symbol='star'),
                            line=dict(dash='dash')
                        ))

                        # Customize layout
                        plotly_fig.update_layout(
                            title=f"Optimal Profile (f2 = {best['f2']})",
                            xaxis=dict(title='Time (h)', zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                            yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                            legend_title='Legend',
                            plot_bgcolor='white',
                            paper_bgcolor='white'
                        )

                        # Print predicted values
                        print("\nPredicted Reference Dissolution (%):")
                        for t, d in zip(seq, ref_diss):
                            print(f"  {t} h: {d:.2f}%")
                        print("\nPredicted Test Dissolution (%):")
                        for t, d in zip(seq, test_diss):
                            print(f"  {t} h: {d:.2f}%")

                        ai_df = pd.DataFrame({
                            "Time Point (h)": seq,
                            "Test Dissolution (%)": test_diss,
                            "Reference Dissolution (%)": ref_diss
                        }).round(2)



                        # List all candidates
                        print("\n=== All Candidate Combinations ===")
                        for i, cand in enumerate(results, 1):
                            seq_print = cand['sequence']
                            recom_f2 = cand['f2']
                            print(f"{i:2d}. Points: {cand['sequence']} | f2: {cand['f2']} | Compliant: {cand['compliant']}")

                    if cv_check == True:
                        cv_checks = "True"
                    else:
                        cv_checks = "False"
                    return changed_both_f2s,result_fig1,result_fig2,seq_print,recom_f2,plotly_fig,cv_checks,ai_df

        elif input1 == "ASEAN":
            print("According to ASEAN guidelines")

            # 1) CV check
            cv_check = check_cv(test_df) and check_cv(reference_df)
            print("CV <20 at first non-zero, <10 thereafter:", cv_check)
            
            #(0)Timepoint drop
            reference_df = reference_df[reference_df.iloc[:, 0] != 0].reset_index(drop=True)
            test_df = test_df[test_df.iloc[:, 0] != 0].reset_index(drop=True)

            # 2) Verify time alignment
            if not check_same_time_points(test_df, reference_df):
                print("Error: Time points between test and reference do not match.")
            else:
                print("\nAnalysis based on ‚Äòeither >85%‚Äô dissolution criterion:")
                try:
                    # Compute f2s and get trimmed DataFrames
                    f2_conv, f2_exp, ref_trim, test_trim = calculate_f2_either85(reference_df, test_df)
                    if isinstance(f2_conv,str):
                        recom_f2,plotly_fig,ai_df = None, None, None
                        if cv_check == True:
                            cv_checks = "True"
                        else:
                            cv_checks = "False"
                        return f2_conv,f2_exp,ref_trim,test_trim,recom_f2,plotly_fig,cv_checks,ai_df
                    changed_both_f2s = round(f2_conv,2)
                    # Print results
                    print(f"Conventional f2: {f2_conv:.2f}")
                    print(f"Expected    f2: {f2_exp:.2f}")
                    # 4) Show trimmed times & means
                    # times      = ref_trim.iloc[:, 0].astype(float).tolist()
                    # ref_means  = ref_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()
                    # test_means = test_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()
                    # print("Trimmed times:       ", times)
                    # print("Reference means (%): ", ref_means)
                    # print("Test means      (%): ", test_means, "\n")

                    if not cv_check:
                        print("\nWarning: CV requirements not met; interpret with caution.")

                    # --- Graph 1: Simple dissolution curves ---
                    times      = ref_trim.iloc[:, 0].astype(float)
                    ref_means  = ref_trim.iloc[:, 1:].mean(axis=1)
                    test_means = test_trim.iloc[:, 1:].mean(axis=1)

                    max_time = max(times)

                    # Set x_dtick based on the conditions
                    if max_time > 60:
                        x_dtick = 10
                    elif max_time > 30: # This covers 30 < max_time <= 60
                        x_dtick = 5
                    elif max_time > 12: # This covers 12 < max_time <= 30 (a reasonable interval for this unspecified range)
                        x_dtick = 2.5
                    else: # This covers 0 <= max_time <= 12
                        x_dtick = 1

                    # Create a Plotly figure
                    result_fig1 = go.Figure()

                    # Add Reference data trace
                    result_fig1.add_trace(go.Scatter(
                    x=times,
                    y=ref_means,
                    mode='lines+markers',
                    name='Reference',
                    marker=dict(color='blue', symbol='circle'),
                    line=dict(dash='solid')
                    ))

                    # Add Test data trace
                    result_fig1.add_trace(go.Scatter(
                    x=times,
                    y=test_means,
                    mode='lines+markers',
                    name='Test',
                    marker=dict(color='red', symbol='circle'),
                    line=dict(dash='dash')
                    ))

                    # Customize layout
                    result_fig1.update_layout(
                    title='Dissolution Curves',
                    xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                    yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                    legend_title='Legend',
                    plot_bgcolor='white',
                    paper_bgcolor='white'
                    )

                    # --- Graph 2: Curves with min/max intervals ---
                    ref_min  = ref_trim.iloc[:, 1:].min(axis=1)
                    ref_max  = ref_trim.iloc[:, 1:].max(axis=1)
                    tst_min = test_trim.iloc[:, 1:].min(axis=1)
                    tst_max = test_trim.iloc[:, 1:].max(axis=1)

                    # Create a Plotly figure
                    result_fig2 = go.Figure()

                    # Add Reference Mean data trace with error bars
                    result_fig2.add_trace(go.Scatter(
                        x=times,
                        y=ref_means,
                        mode='lines+markers',
                        name='Reference Mean',
                        error_y=dict(
                        type='data',
                        symmetric=False,
                        array=[ma - rm for ma, rm in zip(ref_max, ref_means)],
                        arrayminus=[rm - mi for rm, mi in zip(ref_means, ref_min)]
                        ),
                        marker=dict(color='blue', symbol='circle'),
                        line=dict(dash='solid')
                    ))

                    # Add Test Mean data trace with error bars
                    result_fig2.add_trace(go.Scatter(
                        x=times,
                        y=test_means,
                        mode='lines+markers',
                        name='Test Mean',
                        error_y=dict(
                        type='data',
                        symmetric=False,
                        array=[ma - tm for ma, tm in zip(tst_max, test_means)],
                        arrayminus=[tm - mi for tm, mi in zip(test_means, tst_min)]
                        ),
                        marker=dict(color='red', symbol='square'),
                        line=dict(dash='dash')
                    ))

                    # Customize layout
                    result_fig2.update_layout(
                        title='Dissolution Curves with Intervals',
                        xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                        yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                        legend_title='Legend',
                        plot_bgcolor='white',
                        paper_bgcolor='white'
                    )

                except ValueError as e:
                    print("‚ùå", e)
                    print("Cannot perform ‚Äòeither >85%‚Äô‚Äìbased f2 calculation.")

                    
                reference_mean_df = reference_mean_df.apply(pd.to_numeric, errors='coerce')
                test_mean_df = test_mean_df.apply(pd.to_numeric, errors='coerce')
                cross_85 = any((ref > 85) and (test > 85) for ref, test in zip(reference_mean_df.values.flatten(), test_mean_df.values.flatten()))
            

                if not cross_85:
                    print(f"Conventional f2 is the best f2.")
                    print("Using conventional time points and dissolution percentages.")
                    print(f"Time Points: {times}")
                    print(f"Reference means (%): {ref_means}")
                    print(f"Test means (%): {test_means}")
                    print(f"Conventional f2 Score: {f2_conv:.2f}")

                    ai_df = pd.DataFrame({
                            "Time Point (h)": times,
                            "Test Dissolution (%)": test_means,
                            "Reference Dissolution (%)": ref_means
                    }).round(2)
                    
                    # Plot conventional curves
                    # Create a Plotly figure
                    plotly_fig = go.Figure()

                    # Add Reference data trace
                    plotly_fig.add_trace(go.Scatter(
                    x=times,
                    y=ref_means,
                    mode='lines+markers',
                    name='Reference',
                    marker=dict(color='blue', symbol='circle'),
                    line=dict(dash='solid')
                    ))

                    # Add Test data trace
                    plotly_fig.add_trace(go.Scatter(
                    x=times,
                    y=test_means,
                    mode='lines+markers',
                    name='Test',
                    marker=dict(color='red', symbol='circle'),
                    line=dict(dash='dash')
                    ))

                    # Customize layout
                    plotly_fig.update_layout(
                    title=f"Optimal Profile (f2 = {f2_conv:.2f})",
                    xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                    yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                    legend_title='Legend',
                    plot_bgcolor='white',
                    paper_bgcolor='white'
                    )
                    seq_print = None,
                    recom_f2 = round(f2_conv,2)
                    if cv_check == True:
                        cv_checks = "True"
                    else:
                        cv_checks = "False"
                    return changed_both_f2s,result_fig1,result_fig2,seq_print,recom_f2,plotly_fig,cv_checks,ai_df
                else:
                    try:
                        window_min, window_max = determine_candidate_window(
                            reference_mean_df, test_mean_df, step=5, initial_threshold=10
                        )
                        # first, original
                        results, all_results = predictive_optimal_combinations_advanced(
                            reference_mean_df,
                            test_mean_df,
                            regulation         = input1,
                            window_min         = window_min,
                            window_max         = window_max,
                            diff_threshold     = None,
                            interp_method      = 'linear',
                            points_per_stratum = None
                        )
                        print("‚úÖ Used original integer‚Äêgrid function\n")
                    except ValueError:
                        results, _ = optimal_timepoints(
                            reference_mean_df, test_mean_df,
                            input1, window_max=12,
                            step_hours=0.25
                        )
                    for cand in results:
                        cand['sequence'] = [
                            int(t) if float(t).is_integer() else round(t, 2) 
                            for t in cand['sequence']
                        ]

                    best = results[0]
                    print("=== Optimal Predictive Combination ===")
                    print(f"Time Points: {best['sequence']}")
                    print(f"Predicted f2 Score: {best['f2']}")

                    if best['f2'] < f2_conv:
                        print(f"Conventional f2 is the best f2.")
                        print("Using conventional time points and dissolution percentages.")
                        print(f"Time Points: {times}")
                        print(f"Reference means (%): {ref_means}")
                        print(f"Test means (%): {test_means}")
                        print(f"Conventional f2 Score: {f2_conv:.2f}")

                        ai_df = pd.DataFrame({
                            "Time Point (h)": times,
                            "Test Dissolution (%)": test_means,
                            "Reference Dissolution (%)": ref_means
                        }).round(2)
                    
                        # Create a Plotly figure
                        plotly_fig = go.Figure()

                        # Add Reference data trace
                        plotly_fig.add_trace(go.Scatter(
                        x=times,
                        y=ref_means,
                        mode='lines+markers',
                        name='Reference',
                        marker=dict(color='blue', symbol='circle'),
                        line=dict(dash='solid')
                        ))

                        # Add Test data trace
                        plotly_fig.add_trace(go.Scatter(
                        x=times,
                        y=test_means,
                        mode='lines+markers',
                        name='Test',
                        marker=dict(color='red', symbol='circle'),
                        line=dict(dash='dash')
                        ))

                        # Customize layout
                        plotly_fig.update_layout(
                        title=f"Optimal Profile (f2 = {f2_conv:.2f})",
                        xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                        yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                        legend_title='Legend',
                        plot_bgcolor='white',
                        paper_bgcolor='white'
                        )

                        seq_print = None,
                        recom_f2 = round(f2_conv,2)
                    else:
                        # Plot predicted curves
                        import matplotlib.pyplot as plt
                        from matplotlib.ticker import MaxNLocator

                        seq      = best['sequence']
                        ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(seq), method='linear')
                        test_diss= interpolate_dissolution_curve(test_mean_df, np.array(seq), method='linear')
                        # force 0% at start
                        ref_diss[0] = test_diss[0] = 0.0

                        # Create a Plotly figure
                        plotly_fig = go.Figure()

                        # Add Reference data trace
                        plotly_fig.add_trace(go.Scatter(
                            x=seq,
                            y=ref_diss,
                            mode='lines+markers',
                            name='Reference',
                            marker=dict(color='blue', symbol='circle'),
                            line=dict(dash='solid')
                        ))

                        # Add Test data trace
                        plotly_fig.add_trace(go.Scatter(
                            x=seq,
                            y=test_diss,
                            mode='lines+markers',
                            name='Test',
                            marker=dict(color='red', symbol='star'),
                            line=dict(dash='dash')
                        ))

                        # Customize layout
                        plotly_fig.update_layout(
                            title=f"Optimal Profile (f2 = {best['f2']})",
                            xaxis=dict(title='Time (h)', zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                            yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                            legend_title='Legend',
                            plot_bgcolor='white',
                            paper_bgcolor='white'
                        )

                        # Print predicted values
                        print("\nPredicted Reference Dissolution (%):")
                        for t, d in zip(seq, ref_diss):
                            print(f"  {t} h: {d:.2f}%")
                        print("\nPredicted Test Dissolution (%):")
                        for t, d in zip(seq, test_diss):
                            print(f"  {t} h: {d:.2f}%")

                        ai_df = pd.DataFrame({
                            "Time Point (h)": seq,
                            "Test Dissolution (%)": test_diss,
                            "Reference Dissolution (%)": ref_diss
                        }).round(2)

                        # List all candidates
                        print("\n=== All Candidate Combinations ===")
                        for i, cand in enumerate(results, 1):
                            seq_print = cand['sequence']
                            recom_f2 = cand['f2']
                            print(f"{i:2d}. Points: {cand['sequence']} | f2: {cand['f2']} | Compliant: {cand['compliant']}")

                    if cv_check == True:
                        cv_checks = "True"
                    else:
                        cv_checks = "False"
                    return changed_both_f2s,result_fig1,result_fig2,seq_print,recom_f2,plotly_fig,cv_checks,ai_df

        elif input1 == "ANVISA":
            print("According to ANVISA ‚â•¬†85% guidelines\n")

            # 1) Check criteria & CV
            both_85   = check_both_85(reference_df, test_df)
            # either_85 = check_either_85(reference_df, test_df)
            print("Both ‚â•¬†85% criterion met:  ", both_85)
            # print("Either ‚â•¬†85% criterion met:", either_85)

            cv_check = check_cv(test_df) and check_cv(reference_df)
            print("\nCV <20 at first non-zero, <10 thereafter:", cv_check, "\n")

            #(0)Timepoint drop
            reference_df = reference_df[reference_df.iloc[:, 0] != 0].reset_index(drop=True)
            test_df = test_df[test_df.iloc[:, 0] != 0].reset_index(drop=True)

            # 2) Verify time alignment
            if not check_same_time_points(reference_df, test_df):
                print("Error: Time points between test and reference do not match.")
                print("Calculations cannot be performed.")
            else:
                try:
                    # 3) Compute f2s & trimmed data
                    f2_conv, f2_exp, ref_trim, test_trim = calculate_f2_fda(reference_df, test_df)
                    if isinstance(f2_conv,str):
                        recom_f2,plotly_fig,ai_df = None, None, None
                        if cv_check == True:
                            cv_checks = "True"
                        else:
                            cv_checks = "False"
                        return f2_conv,f2_exp,ref_trim,test_trim,recom_f2,plotly_fig,cv_checks,ai_df
                    changed_both_f2s = round(f2_conv,2)
                    # # 4) Show trimmed times & means
                    times      = ref_trim.iloc[:, 0].astype(float).tolist()
                    ref_means  = ref_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()
                    test_means = test_trim.iloc[:, 1:].mean(axis=1).round(2).tolist()

                    print("Trimmed times:       ", times)
                    print("Reference means (%): ", ref_means)
                    print("Test means      (%): ", test_means, "\n")

                    # 5) Print f2 results
                    print(f"Conventional f2: {f2_conv:.2f}")
                    print(f"Expected    f2: {f2_exp:.2f}")
                    if not cv_check:
                        print("\nWarning: CV requirements not met; interpret with caution.")

                    # --- Graph 1: Simple dissolution curves ---

                    max_time = max(times)

                    # Set x_dtick based on the conditions
                    if max_time > 60:
                        x_dtick = 10
                    elif max_time > 30: # This covers 30 < max_time <= 60
                        x_dtick = 5
                    elif max_time > 12: # This covers 12 < max_time <= 30 (a reasonable interval for this unspecified range)
                        x_dtick = 2.5
                    else: # This covers 0 <= max_time <= 12
                        x_dtick = 1
                    
                    # Create a Plotly figure
                    result_fig1 = go.Figure()

                    # Add Reference data trace
                    result_fig1.add_trace(go.Scatter(
                    x=times,
                    y=ref_means,
                    mode='lines+markers',
                    name='Reference',
                    marker=dict(color='blue', symbol='circle'),
                    line=dict(dash='solid')
                    ))

                    # Add Test data trace
                    result_fig1.add_trace(go.Scatter(
                    x=times,
                    y=test_means,
                    mode='lines+markers',
                    name='Test',
                    marker=dict(color='red', symbol='circle'),
                    line=dict(dash='dash')
                    ))

                    # Customize layout
                    result_fig1.update_layout(
                    title='Dissolution Curves',
                    xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                    yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                    legend_title='Legend',
                    plot_bgcolor='white',
                    paper_bgcolor='white'
                    )


                    # --- Graph 2: Curves with min/max intervals ---
                    
                    # Calculate min and max values for reference and test data
                    ref_min = ref_trim.iloc[:, 1:].min(axis=1)
                    ref_max = ref_trim.iloc[:, 1:].max(axis=1)
                    tst_min = test_trim.iloc[:, 1:].min(axis=1)
                    tst_max = test_trim.iloc[:, 1:].max(axis=1)

                    # Create a Plotly figure
                    result_fig2 = go.Figure()

                    # Add Reference Mean data trace with error bars
                    result_fig2.add_trace(go.Scatter(
                        x=times,
                        y=ref_means,
                        mode='lines+markers',
                        name='Reference Mean',
                        error_y=dict(
                        type='data',
                        symmetric=False,
                        array=[ma - rm for ma, rm in zip(ref_max, ref_means)],
                        arrayminus=[rm - mi for rm, mi in zip(ref_means, ref_min)]
                        ),
                        marker=dict(color='blue', symbol='circle'),
                        line=dict(dash='solid')
                    ))

                    # Add Test Mean data trace with error bars
                    result_fig2.add_trace(go.Scatter(
                        x=times,
                        y=test_means,
                        mode='lines+markers',
                        name='Test Mean',
                        error_y=dict(
                        type='data',
                        symmetric=False,
                        array=[ma - tm for ma, tm in zip(tst_max, test_means)],
                        arrayminus=[tm - mi for tm, mi in zip(test_means, tst_min)]
                        ),
                        marker=dict(color='red', symbol='square'),
                        line=dict(dash='dash')
                    ))

                    # Customize layout
                    result_fig2.update_layout(
                        title='Dissolution Curves with Intervals',
                        xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                        yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                        legend_title='Legend',
                        plot_bgcolor='white',
                        paper_bgcolor='white'
                    )

                except ValueError as e:
                    print("‚ùå", e)
                    print("Cannot perform FDA‚Äërule f2 calculation.")
                

                
                reference_mean_df = reference_mean_df.apply(pd.to_numeric, errors='coerce')
                test_mean_df = test_mean_df.apply(pd.to_numeric, errors='coerce')
                cross_85 = any((ref > 85) and (test > 85) for ref, test in zip(reference_mean_df.values.flatten(), test_mean_df.values.flatten()))
            

                if not cross_85:
                    print(f"Conventional f2 is the best f2.")
                    print("Using conventional time points and dissolution percentages.")
                    print(f"Time Points: {times}")
                    print(f"Reference means (%): {ref_means}")
                    print(f"Test means (%): {test_means}")
                    print(f"Conventional f2 Score: {f2_conv:.2f}")

                    ai_df = pd.DataFrame({
                            "Time Point (h)": times,
                            "Test Dissolution (%)": test_means,
                            "Reference Dissolution (%)": ref_means
                    }).round(2)
                    
                    # Plot conventional curves
                    # Create a Plotly figure
                    plotly_fig = go.Figure()

                    # Add Reference data trace
                    plotly_fig.add_trace(go.Scatter(
                    x=times,
                    y=ref_means,
                    mode='lines+markers',
                    name='Reference',
                    marker=dict(color='blue', symbol='circle'),
                    line=dict(dash='solid')
                    ))

                    # Add Test data trace
                    plotly_fig.add_trace(go.Scatter(
                    x=times,
                    y=test_means,
                    mode='lines+markers',
                    name='Test',
                    marker=dict(color='red', symbol='circle'),
                    line=dict(dash='dash')
                    ))

                    # Customize layout
                    plotly_fig.update_layout(
                    title=f"Optimal Profile (f2 = {f2_conv:.2f})",
                    xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                    yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                    legend_title='Legend',
                    plot_bgcolor='white',
                    paper_bgcolor='white'
                    )
                    seq_print = None,
                    recom_f2 = round(f2_conv,2)
                    if cv_check == True:
                        cv_checks = "True"
                    else:
                        cv_checks = "False"
                    return changed_both_f2s,result_fig1,result_fig2,seq_print,recom_f2,plotly_fig,cv_checks,ai_df
                else:
                    try:
                        window_min, window_max = determine_candidate_window(
                            reference_mean_df, test_mean_df, step=5, initial_threshold=10
                        )
                        # first, original
                        results, all_results = predictive_optimal_combinations_advanced(
                            reference_mean_df,
                            test_mean_df,
                            regulation         = input1,
                            window_min         = window_min,
                            window_max         = window_max,
                            diff_threshold     = None,
                            interp_method      = 'linear',
                            points_per_stratum = None
                        )
                        print("‚úÖ Used original integer‚Äêgrid function\n")
                    except ValueError:
                        results, _ = optimal_timepoints(
                            reference_mean_df, test_mean_df,
                            input1, window_max=12,
                            step_hours=0.25
                        )
                    
                
                    for cand in results:
                        cand['sequence'] = [
                            int(t) if float(t).is_integer() else round(t, 2) 
                            for t in cand['sequence']
                        ]

                    best = results[0]
                    print("=== Optimal Predictive Combination ===")
                
                    if best['f2'] < f2_conv:
                        print(f"Conventional f2 is the best f2.")
                        print("Using conventional time points and dissolution percentages.")
                        print(f"Time Points: {times}")
                        print(f"Reference means (%): {ref_means}")
                        print(f"Test means (%): {test_means}")
                        print(f"Conventional f2 Score: {f2_conv:.2f}")

                        ai_df = pd.DataFrame({
                            "Time Point (h)": times,
                            "Test Dissolution (%)": test_means,
                            "Reference Dissolution (%)": ref_means
                        }).round(2)
                    
                        # Create a Plotly figure
                        plotly_fig = go.Figure()

                        # Add Reference data trace
                        plotly_fig.add_trace(go.Scatter(
                        x=times,
                        y=ref_means,
                        mode='lines+markers',
                        name='Reference',
                        marker=dict(color='blue', symbol='circle'),
                        line=dict(dash='solid')
                        ))

                        # Add Test data trace
                        plotly_fig.add_trace(go.Scatter(
                        x=times,
                        y=test_means,
                        mode='lines+markers',
                        name='Test',
                        marker=dict(color='red', symbol='circle'),
                        line=dict(dash='dash')
                        ))

                        # Customize layout
                        plotly_fig.update_layout(
                        title=f"Optimal Profile (f2 = {f2_conv:.2f})",
                        xaxis=dict(title='Time',dtick=x_dtick, zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                        yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                        legend_title='Legend',
                        plot_bgcolor='white',
                        paper_bgcolor='white'
                        )

                        seq_print = None,
                        recom_f2 = round(f2_conv,2)
                    else:
                        sequence = [int(t) if float(t).is_integer() else float(t) for t in best['sequence']]
                        print(f"Time Points: {best['sequence']}")
                        print(f"Predicted f2 Score: {best['f2']}")
                    
                        # Interpolating and plotting predicted curves
                        seq      = best['sequence']
                        ref_diss = interpolate_dissolution_curve(reference_mean_df, np.array(seq), method='linear')
                        test_diss= interpolate_dissolution_curve(test_mean_df,      np.array(seq), method='linear')
                        # force 0% at start
                        ref_diss[0] = test_diss[0] = 0.0
            
                        # Create a Plotly figure
                        plotly_fig = go.Figure()

                        # Add Reference data trace
                        plotly_fig.add_trace(go.Scatter(
                            x=seq,
                            y=ref_diss,
                            mode='lines+markers',
                            name='Reference',
                            marker=dict(color='blue', symbol='circle'),
                            line=dict(dash='solid')
                        ))

                        # Add Test data trace
                        plotly_fig.add_trace(go.Scatter(
                            x=seq,
                            y=test_diss,
                            mode='lines+markers',
                            name='Test',
                            marker=dict(color='red', symbol='star'),
                            line=dict(dash='dash')
                        ))

                        # Customize layout
                        plotly_fig.update_layout(
                            title=f"Optimal Profile (f2 = {best['f2']})",
                            xaxis=dict(title='Time (h)', zeroline=True, zerolinewidth=1, zerolinecolor='black'),
                            yaxis=dict(title='Dissolution (%)', tickmode='linear', dtick=10, zeroline=True, zerolinewidth=1, zerolinecolor='black', showgrid=True, gridcolor='lightgray'),
                            legend_title='Legend',
                            plot_bgcolor='white',
                            paper_bgcolor='white'
                        )



                        # Print predicted values
                        print("\nPredicted Reference Dissolution (%):")
                        for t, d in zip(seq, ref_diss):
                            print(f"  {t} h: {d:.2f}%")
                        print("\nPredicted Test Dissolution (%):")
                        for t, d in zip(seq, test_diss):
                            print(f"  {t} h: {d:.2f}%")

                        ai_df = pd.DataFrame({
                            "Time Point (h)": seq,
                            "Test Dissolution (%)": test_diss,
                            "Reference Dissolution (%)": ref_diss
                        }).round(2)

                        # List all candidates
                        print("\n=== All Candidate Combinations ===")
                        for i, cand in enumerate(results, 1):
                            seq_print = cand['sequence']
                            recom_f2 = cand['f2']
                            print(f"{i:2d}. Points: {cand['sequence']} | f2: {cand['f2']} | Compliant: {cand['compliant']}")
                           

                    if cv_check == True:
                        cv_checks = "True"
                    else:
                        cv_checks = "False"
                    return changed_both_f2s,result_fig1,result_fig2,seq_print,recom_f2,plotly_fig,cv_checks,ai_df
 
    except Exception as e:
        print(e,"ERROR")
