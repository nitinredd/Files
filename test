#RDB
import os
import re
import glob
import base64
import io
import json
from typing import Union
from jose import jwt
import threading
from typing import List, Optional, Dict, Any
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Depends
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBasic
from datetime import datetime, timedelta
from pydantic import BaseModel
import fitz  # PyMuPDF
import pandas as pd
from PIL import Image
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA, LLMChain
from langchain.prompts import PromptTemplate
import config
import mimetypes
from fastapi import HTTPException
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List
from difflib import SequenceMatcher
from typing import Tuple
import traceback



# ---------------------------
# Configuration (as provided by you â€” DO NOT CHANGE unless you need to)
# ---------------------------
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Azure Configuration (kept exactly as requested)
base_url=""
api_version="2025-01-01-preview"

api_key=""
deployment_name="api-ai4o"
model_name="gpt-4o"

# Initialize Azure services
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-3-large",
    api_version="2025-01-01-preview",
    azure_endpoint="",
    api_key="",
    azure_deployment=""
)
chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)
# ---------------------------

# ðŸ” JWT Config
SECRET_KEY = "myFAVsecretKEY"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

# Reaction types and filesystem layout (kept from your code)
REACTION_TYPES = [
    "C-C_Bond_Formation", "C-N_Bond_Formation", "Salt_Formation", "Hydrolysis",
    "Amidation", "Reduction", "Oxidation", "Cyclization", "Purification",
    "Metal_mediated_catalyzed", "C-halogen Bond Formation", "Miscellaneous"
]

BASE_DIR = r"C:\Users\Desktop\WORK\API\Reaction_Database\Datasets_O\Reaction_Database"
PRODUCTS_DIR = os.path.join(BASE_DIR, "Products")
SCHEMES_DIR = os.path.join(BASE_DIR, "Synthetic_Schemes")

# Ensure dirs exist (idempotent)
os.makedirs(PRODUCTS_DIR, exist_ok=True)
os.makedirs(SCHEMES_DIR, exist_ok=True)

# Prompt template (kept largely the same)
EXTRACTION_PROMPT_TEMPLATE = """
You are a pharmaceutical chemistry expert specializing in reaction chemistry. Extract the following information from the document in a structured format Mandatorily:
1. **API Name**: The active pharmaceutical ingredient
2. **Reaction Chemistry**: Type and description
3. **Yield**: Exact yield percentages or values mentioned in the source
4. **Procedure**: Summarize the complete procedure into clear, concise numbered bullet points, preserving the key steps and important details. Do NOT omit any steps or essential content.
5. **Tabular Data**: Provide COMPLETE tabular data in markdown table format. Do NOT omit, summarize, or transform any content.

Structure your response as follows (literal headers must appear exactly like below):

### API Name
[API name here]
### Reaction Chemistry
[Reaction chemistry description here]
### Yield
[Yield value here]
### Procedure
[Complete procedure here]
### Tabular Data
[Markdown table here]

Document Content:
{context}
Question: {question}
Answer:
"""
EXTRACTION_PROMPT = PromptTemplate(template=EXTRACTION_PROMPT_TEMPLATE, input_variables=["context", "question"])


QA_PROMPT_TEMPLATE = """
You are a concise pharmaceutical chemistry expert. Use the provided document context to answer the user's question directly and concisely.

Rules (follow exactly):
- Use only the information present in the context. Do NOT hallucinate.
- Answer in one short paragraph (2-6 sentences) unless the user explicitly asks for step-by-step procedure.
- Do NOT reproduce the full document content. Do NOT provide unrequested long verbatim passages.
- If multiple documents were used, identify only those documents that were actually the sources of the answer.
- If the answer is not present in the context, say "I could not find an answer in the provided documents." (do not guess).

Context:
{context}

Question: {question}

Answer:
"""
QA_PROMPT = PromptTemplate(template=QA_PROMPT_TEMPLATE, input_variables=["context", "question"])

# ------------------------------
# Helper functions
# ------------------------------
def find_scheme_image(reaction_type: str, product_name: str) -> Optional[str]:
    for ext in ['.jpeg', '.jpg', '.png', '.gif']:
        scheme_path = os.path.join(SCHEMES_DIR, reaction_type, f"{product_name}{ext}")
        if os.path.exists(scheme_path):
            return scheme_path
    return None

def list_products() -> List[Dict[str, Any]]:
    products = []
    for reaction_type in REACTION_TYPES:
        reaction_dir = os.path.join(PRODUCTS_DIR, reaction_type)
        if not os.path.exists(reaction_dir):
            continue
        pdf_files = glob.glob(os.path.join(reaction_dir, "*.pdf"))
        for pdf_path in pdf_files:
            filename = os.path.basename(pdf_path)
            product_name = os.path.splitext(filename)[0]
            scheme_image = find_scheme_image(reaction_type, product_name)
            scheme_cdx = os.path.join(SCHEMES_DIR, reaction_type, f"{product_name}.cdx")
            product_id = f"{reaction_type}_{product_name}"
            products.append({
                "id": product_id,
                "name": product_name,
                "reaction_type": reaction_type,
                "pdf_path": pdf_path,
                "scheme_image": scheme_image if scheme_image else None,
                "scheme_cdx": scheme_cdx if os.path.exists(scheme_cdx) else None
            })
    return products

def extract_pdf_text(pdf_path: str) -> str:
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text() + "\n"
        return text
    except Exception as e:
        raise RuntimeError(f"Error reading PDF {pdf_path}: {e}")

# Cache for vectorstores and product details
_vectorstore_cache: Dict[str, FAISS] = {}
_product_details_cache: Dict[str, Dict[str, Any]] = {}
_vectorstore_lock = threading.Lock()

def build_product_vector_store(product: Dict[str, Any]) -> Optional[FAISS]:
    pid = product["id"]
    with _vectorstore_lock:
        if pid in _vectorstore_cache:
            return _vectorstore_cache[pid]
        text = extract_pdf_text(product["pdf_path"])
        if not text or len(text.strip()) < 50:
            return None
        doc = Document(page_content=text, metadata={
            "product_id": pid,
            "product_name": product["name"],
            "reaction_type": product["reaction_type"],
            "source": product["pdf_path"]
        })
        vs = FAISS.from_documents([doc], cached_embeddings)
        _vectorstore_cache[pid] = vs
        return vs

def parse_structured_response(text: str) -> Dict[str, Any]:
    """
    Parse the LLM output (which we instructed to use literal '### API Name', etc.)
    Return JSON with keys: api_name, reaction_chemistry, yield, procedure, tables (list)
    Each table is {headers: [], rows: [[], [], ...]}
    """
    result = {"raw": text, "api_name": None, "reaction_chemistry": None, "yield": None, "procedure": None, "tables": []}
    def grab(section):
        m = re.search(rf"###\s*{re.escape(section)}\s*(.*?)\s*(?=###\s*\w+|\Z)", text, re.DOTALL | re.IGNORECASE)
        return m.group(1).strip() if m else None

    result["api_name"] = grab("API Name")
    result["reaction_chemistry"] = grab("Reaction Chemistry")
    result["yield"] = grab("Yield")
    result["procedure"] = grab("Procedure")
    tab_raw = grab("Tabular Data")
    if tab_raw:
        # find markdown tables starting with |...|
        table_patterns = re.findall(r"(\|[^\n]*\|\s*\n\|[-:\s|]*\|\s*\n(?:\|[^\n]*\|\s*\n?)*)", tab_raw, re.DOTALL)
        if table_patterns:
            for tbl_md in table_patterns:
                # normalize lines
                lines = [ln.strip().strip("|").strip() for ln in tbl_md.splitlines() if ln.strip()]
                if len(lines) >= 2:
                    # header line and separator
                    header = [h.strip() for h in lines[0].split("|")]
                    rows = []
                    for rowline in lines[2:]:
                        cols = [c.strip() for c in rowline.split("|")]
                        rows.append(cols)
                    result["tables"].append({"headers": header, "rows": rows, "raw_md": tbl_md})
        else:
            # fallback: provide the raw block
            result["tables"].append({"headers": [], "rows": [], "raw_md": tab_raw})
    return result

# ------------------------------
# FastAPI app
# ------------------------------
app = FastAPI(title="Reaction Database AI (FastAPI)")

# Allow the React dev server origin (adjust if you host differently)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # add your host/origin if different
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ------------------------------
# Pydantic models
# ------------------------------
class ProductOut(BaseModel):
    id: str
    name: str
    reaction_type: str
    has_scheme_image: bool
    has_scheme_cdx: bool

class QARequest(BaseModel):
    product_id: Optional[str] = None
    question: str = "Extract API Name, Reaction Chemistry, Yield, Procedure, and Tabular Data"

class QueryRequest(BaseModel):
    product_ids: List[str]
    question: str


# ------------------------------
# Endpoints
# ------------------------------
@app.get("/reactions", response_model=List[str])
def get_reactions():
    return REACTION_TYPES

@app.get("/products", response_model=List[ProductOut])
def get_products(reaction_type: Optional[str] = None):
    allp = list_products()
    if reaction_type:
        allp = [p for p in allp if p["reaction_type"] == reaction_type]
    out = []
    for p in allp:
        out.append(ProductOut(
            id=p["id"],
            name=p["name"],
            reaction_type=p["reaction_type"],
            has_scheme_image=bool(p["scheme_image"]),
            has_scheme_cdx=bool(p["scheme_cdx"])
        ))
    return out

@app.get("/product/{product_id}/meta")
def product_meta(product_id: str):
    products = list_products()
    for p in products:
        if p["id"] == product_id:
            return {
                "id": p["id"],
                "name": p["name"],
                "reaction_type": p["reaction_type"],
                "pdf_path": p["pdf_path"],
                "scheme_image": p["scheme_image"],
                "scheme_cdx": p["scheme_cdx"]
            }
    raise HTTPException(status_code=404, detail="Product not found")

@app.get("/product/{product_id}/pdf")
def product_pdf(product_id: str):
    meta = product_meta(product_id)
    pdf_path = meta["pdf_path"]
    if os.path.exists(pdf_path):
        return FileResponse(pdf_path, media_type="application/pdf", filename=os.path.basename(pdf_path))
    raise HTTPException(status_code=404, detail="PDF not found")

@app.get("/product/{product_id}/scheme-image")
def product_scheme_image(product_id: str):
    meta = product_meta(product_id)
    path = meta.get("scheme_image")
    if path and os.path.exists(path):
        return FileResponse(path, media_type="image/png", filename=os.path.basename(path))
    raise HTTPException(status_code=404, detail="Scheme image not found")

@app.post("/product/details")
def product_details(req: QARequest):
    """
    Robust product/details endpoint with strong product-name detection and detailed server-side debugging prints.

    Behavior:
      - If req.product_id provided:
          * If question == canonical extraction -> run extraction prompt and return parsed structured data (cached).
          * Else -> run retrieval QA on that product and return {"answer":..., "sources":[...]}.
      - If req.product_id not provided:
          * Attempt to detect product name inside question by:
              1) normalized alnum substring match (prefer long names),
              2) token overlap check,
              3) similarity ratio fallback (SequenceMatcher).
          * If a product is detected -> build retriever on that product PDF and run QA (returns answer + sources).
          * Otherwise fallback to generative model response (no retriever).
    NOTE: This function prints detailed information to the backend logs for debugging detection issues.
    """
    try:
        q_text = (req.question or "").strip()
        if not q_text:
            return JSONResponse(status_code=400, content={"error": "question is required"})

        # canonical extraction text (case-insensitive)
        CANONICAL_EXTRACTION = "extract api name, reaction chemistry, yield, procedure, and tabular data"

        # helper: normalize to alphanumeric lowercase
        def _normalize_alnum(s: str) -> str:
            return re.sub(r"[^a-z0-9]", "", (s or "").lower())

        # helper: tokenized set (alnum tokens)
        def _tokens(s: str):
            return [t for t in re.split(r'[^a-z0-9]+', (s or "").lower()) if t]

        # helper: simple normalization (keep spaces)
        def _normalize_simple(s: str) -> str:
            return re.sub(r'[^a-z0-9\s]', ' ', (s or "").lower())

        # BEGIN DEBUG LOG
        print("=== /product/details called ===")
        print("Question:", q_text)
        # END DEBUG LOG

        # Detect if the question mentions any reaction-type keyword (so we must run detection/retrieval)
        q_norm_simple = _normalize_simple(q_text)
        contains_reaction_keyword = False
        for rt in REACTION_TYPES:
            rt_norm = _normalize_simple(rt)
            if rt_norm.strip() and rt_norm.strip() in q_norm_simple:
                contains_reaction_keyword = True
                break
        print(f"DEBUG: contains_reaction_keyword={contains_reaction_keyword}")

        # helper to detect product by multiple strategies
        def _detect_product_by_name(question: str) -> Optional[Dict[str, Any]]:
            products = list_products()
            if not products:
                print("DEBUG: No products available to match.")
                return None

            q_norm_alnum = _normalize_alnum(question)
            q_tokens = set(_tokens(question))

            # 1) Exact normalized substring match (prefer longer product names)
            sorted_products = sorted(products, key=lambda p: len(p["name"]), reverse=True)
            exact_matches = []
            for p in sorted_products:
                name_norm = _normalize_alnum(p["name"])
                if not name_norm:
                    continue
                if name_norm in q_norm_alnum:
                    exact_matches.append((p, name_norm))
            if exact_matches:
                chosen = exact_matches[0][0]
                print(f"DEBUG: Exact normalized substring match -> '{chosen['name']}' (id: {chosen['id']})")
                return chosen

            # 2) Token overlap heuristic: count how many name tokens appear in question tokens
            token_matches = []
            for p in sorted_products:
                pname_tokens = set(_tokens(p["name"]))
                if not pname_tokens:
                    continue
                overlap = pname_tokens.intersection(q_tokens)
                if overlap:
                    token_matches.append((p, len(overlap), len(pname_tokens), overlap))
            if token_matches:
                # prefer highest overlap ratio (over name token count), then more tokens matched
                token_matches.sort(key=lambda x: (-(x[1] / x[2]), -x[1]))
                best = token_matches[0]
                p_best, match_count, token_count, overlap = best
                ratio = match_count / token_count
                print(f"DEBUG: Token-overlap candidate -> '{p_best['name']}' (id: {p_best['id']}) overlap {match_count}/{token_count} tokens {overlap} ratio={ratio:.2f}")
                # require at least 50% token overlap to be confident
                if ratio >= 0.5:
                    return p_best

            # 3) Similarity fallback: pick best SequenceMatcher ratio over product.name vs question
            best = None
            best_ratio = 0.0
            for p in products:
                pname = (p["name"] or "").lower()
                if not pname.strip():
                    continue
                # Compute two similarity scores and average them:
                # a) between full names (lower) and question
                r1 = SequenceMatcher(None, pname, question.lower()).ratio()
                # b) between normalized alnum strings (robust to spacing/punctuation)
                r2 = SequenceMatcher(None, _normalize_alnum(pname), q_norm_alnum).ratio()
                ratio = (r1 + r2) / 2.0
                if ratio > best_ratio:
                    best_ratio = ratio
                    best = (p, ratio, r1, r2)
            # log top candidate
            if best:
                p_best, ratio, r1, r2 = best
                print(f"DEBUG: Best similarity candidate -> '{p_best['name']}' (id: {p_best['id']}) ratio_avg={ratio:.3f} r1={r1:.3f} r2={r2:.3f}")
                # Apply a conservative threshold to avoid false positives
                if ratio >= 0.60:
                    return p_best
                else:
                    print(f"DEBUG: Best similarity below threshold (0.60): {ratio:.3f} -> will NOT auto-select")
            return None

        # helper: run retrieval QA over a given product and return {answer, sources}
        def _run_retrieval_for_product(product: Dict[str, Any], question: str, k: int = 3) -> Dict[str, Any]:
            print(f"DEBUG: Running retrieval for product: {product['name']} (id: {product['id']})")
            pdf_path = product.get("pdf_path")
            if not pdf_path or not os.path.exists(pdf_path):
                raise HTTPException(status_code=404, detail=f"PDF not found for product {product.get('id')}: {pdf_path}")

            # extract text and verify
            try:
                text = extract_pdf_text(pdf_path)
            except Exception as e:
                print(f"ERROR: extract_pdf_text failed for {pdf_path}: {e}")
                raise HTTPException(status_code=500, detail=f"Failed to extract text: {e}")

            if not text or len(text.strip()) < 20:
                raise HTTPException(status_code=500, detail="Document content empty or unreadable")

            doc = Document(page_content=text, metadata={
                "product_id": product["id"],
                "product_name": product["name"],
                "reaction_type": product["reaction_type"],
                "source": pdf_path
            })

            try:
                vs = FAISS.from_documents([doc], cached_embeddings)
            except Exception as e:
                print(f"ERROR: FAISS.from_documents failed: {e}")
                raise HTTPException(status_code=500, detail=f"Failed building FAISS index: {e}")

            retriever = vs.as_retriever(search_kwargs={"k": k})

            # choose QA prompt (prefer QA_PROMPT)
            try:
                prompt_to_use = QA_PROMPT
            except NameError:
                # fallback to PROMPT or EXTRACTION_PROMPT
                prompt_to_use = globals().get("PROMPT") or globals().get("EXTRACTION_PROMPT")
                print("DEBUG: QA_PROMPT not found, fallback to available prompt.")

            qa_chain = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt_to_use},
                return_source_documents=True,
            )

            out = qa_chain({"query": question})
            answer_text = out.get("result") or out.get("output_text") or ""
            source_docs = out.get("source_documents", []) or []

            # build unique ordered sources
            seen = set()
            sources = []
            for sd in source_docs:
                pid = sd.metadata.get("product_id")
                pname = sd.metadata.get("product_name")
                if pid and pid not in seen:
                    seen.add(pid)
                    sources.append({"product_id": pid, "product_name": pname})
            print(f"DEBUG: Retrieval produced answer length={len(answer_text)} and {len(sources)} source(s)")
            return {"answer": answer_text, "sources": sources}

        # -------------------------
        # Main branching logic
        # -------------------------

        # 1) explicit product_id path (keep prior behavior)
        if req.product_id:
            products = list_products()
            product = next((p for p in products if p["id"] == req.product_id), None)
            if not product:
                return JSONResponse(status_code=404, content={"error": "Product not found"})

            is_extraction = q_text.strip().lower() == CANONICAL_EXTRACTION

            if not is_extraction:
                # question about this product -> run retrieval QA
                return _run_retrieval_for_product(product, q_text, k=3)

            # structured extraction -> cached or run
            if req.product_id in _product_details_cache:
                print("DEBUG: returning cached parsed details")
                return _product_details_cache[req.product_id]

            vs = build_product_vector_store(product)
            if not vs:
                return JSONResponse(status_code=500, content={"error": "Failed to build vector store (empty/invalid PDF)"})

            retriever = vs.as_retriever(search_kwargs={"k": 1})
            # extraction prompt chooser
            try:
                prompt_for_extraction = EXTRACTION_PROMPT
            except NameError:
                prompt_for_extraction = globals().get("PROMPT")
            if not prompt_for_extraction:
                return JSONResponse(status_code=500, content={"error": "No extraction prompt configured on the server."})

            qa_chain = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt_for_extraction},
                return_source_documents=False,
            )

            raw_response = qa_chain.run(q_text)
            parsed = parse_structured_response(raw_response)
            _product_details_cache[req.product_id] = parsed
            print("DEBUG: structured extraction completed and cached")
            return parsed

        # If the question is very short or looks like a greeting, skip detection and go straight to generative LLM,
        # BUT DO NOT skip if the question contains any reaction-type keyword (in that case detection/retrieval must run).
        if (not contains_reaction_keyword) and (len(q_text) < 4 or (len(_tokens(q_text)) <= 1 and not any(c.isdigit() for c in q_text))):
            print("DEBUG: Short/greeting query detected and no reaction keyword present -> using generative model.")
            prompt_for_gen = QA_PROMPT
            llm_chain = LLMChain(llm=chat_model, prompt=prompt_for_gen)
            try:
                raw_response = llm_chain.predict(context="", question=q_text)
            except Exception as e:
                print(f"ERROR: Generative LLMChain failed: {e}")
                raise HTTPException(status_code=500, detail=f"Generative model call failed: {e}")
            return {"response": raw_response}

        # 2) no explicit product_id: attempt detection from free-text (this will run if contains_reaction_keyword True OR question isn't a short greeting)
        detected = _detect_product_by_name(q_text)
        if detected:
            # IMPORTANT: run retrieval on detected product
            print(f"DEBUG: Detected product by name: {detected['name']}' (id: {detected['id']}) -> running retrieval")
            return _run_retrieval_for_product(detected, q_text, k=3)

        # 3) fallback: generative response (concise QA prompt)
        print("DEBUG: No product detected in question -> falling back to generative QA (no retriever)")
        prompt_for_gen = QA_PROMPT
        llm_chain = LLMChain(llm=chat_model, prompt=prompt_for_gen)
        try:
            raw_response = llm_chain.predict(context="", question=q_text)
        except Exception as e:
            print(f"ERROR: Generative LLMChain failed: {e}")
            raise HTTPException(status_code=500, detail=f"Generative model call failed: {e}")
        return {"response": raw_response}

    except HTTPException as he:
        # raise HTTP exceptions normally
        raise he
    except Exception as e:
        tb = traceback.format_exc()
        print("=== /product/details ERROR ===")
        print(tb)
        trace_lines = tb.splitlines()[-30:]
        return JSONResponse(status_code=500, content={
            "error": "Internal server error in /product/details",
            "message": str(e),
            "trace": trace_lines
        })

@app.post("/query")
def query_documents(req: QueryRequest):
    """
    Query multiple product documents (product_ids) and return an answer + sources.
    """
    if not req.product_ids:
        raise HTTPException(status_code=400, detail="product_ids must be a non-empty list")
    if not req.question or not req.question.strip():
        raise HTTPException(status_code=400, detail="question required")

    # Collect documents
    docs = []
    for pid in req.product_ids:
        try:
            meta = product_meta(pid)
        except HTTPException:
            continue
        pdf_path = meta.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            continue
        text = extract_pdf_text(pdf_path)
        if not text or len(text.strip()) < 50:
            continue
        doc = Document(
            page_content=text,
            metadata={
                "product_id": pid,
                "product_name": meta.get("name", ""),
                "reaction_type": meta.get("reaction_type", "")
            }
        )
        docs.append(doc)

    if len(docs) == 0:
        raise HTTPException(status_code=404, detail="No documents available for the provided product_ids")

    # Build FAISS on-the-fly (safe for moderate numbers of docs)
    try:
        vector_store = FAISS.from_documents(docs, cached_embeddings)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed building vector index: {e}")

    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    qa_chain = RetrievalQA.from_chain_type(
        llm=chat_model,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": QA_PROMPT},
        return_source_documents=True
    )

    try:
        out = qa_chain({"query": req.question})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"QA execution failed: {e}")

    answer_text = out.get("result") or out.get("output_text") or str(out)
    source_docs = out.get("source_documents", [])

    # Build sources: unique product ids preserving order
    seen = set()
    sources = []
    for sd in source_docs:
        pid = sd.metadata.get("product_id")
        pname = sd.metadata.get("product_name")
        if pid and pid not in seen:
            seen.add(pid)
            sources.append({"product_id": pid, "product_name": pname})

    return {"answer": answer_text, "sources": sources}

@app.get("/products/search")
def search_products(q: str = "", limit: int = 10):
    """
    Return up to `limit` products that match the query `q`.
    Matching strategy:
      1. prefix matches on normalized name (best)
      2. substring matches on normalized name (fallback)
    Normalization: lowercased, non-alphanumeric removed (so 'Prod-1' matches 'prod1', 'Prod 1', etc).
    """
    if q is None:
        q = ""
    q_norm = re.sub(r'[^a-z0-9]', '', q.lower())
    if q_norm == "":
        # return first N products as a fallback (stable order)
        prods = list_products()[:limit]
        return [{"id": p["id"], "name": p["name"], "reaction_type": p["reaction_type"]} for p in prods]

    products = list_products()
    # collect prefix matches first (longer names first)
    prefix_matches = []
    substring_matches = []
    for p in products:
        name_norm = re.sub(r'[^a-z0-9]', '', p["name"].lower())
        if name_norm.startswith(q_norm):
            prefix_matches.append(p)
        elif q_norm in name_norm:
            substring_matches.append(p)

    # sort prefix matches by name length (prefer longer exact names)
    prefix_matches = sorted(prefix_matches, key=lambda x: -len(x["name"]))
    substring_matches = sorted(substring_matches, key=lambda x: -len(x["name"]))

    combined = prefix_matches + substring_matches
    combined = combined[:limit]

    return [{"id": p["id"], "name": p["name"], "reaction_type": p["reaction_type"]} for p in combined]

# ------------------------------
# Transcription endpoint (optional Google Cloud)
# ------------------------------
# If you want server-side Google STT you can paste your credentials JSON string into
# GOOGLE_CREDENTIALS_JSON below (NOT recommended for public repos). For demos, prefer browser Web Speech API.
GOOGLE_CREDENTIALS_JSON = ""  # <<< If you want to enable server-side Google STT, paste credentials JSON string here.

@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...), use_google: Optional[bool] = Form(False)):
    """
    Accept an audio file (wav/webm/mp3) and return a transcript.
    By default this endpoint will only provide a simple error unless GOOGLE_CREDENTIALS_JSON is provided.
    The recommended demo approach is to use the browser Web Speech API for instant transcripts.
    """
    content = await file.read()
    if not use_google:
        # We recommend using browser speech API â€” but if user uploaded audio anyway, we can attempt a naive fallback:
        # Try to return an error telling front-end to use browser API.
        return {"error": "Server-side transcription disabled. Use browser Web Speech API for demo, or set use_google=True and provide GOOGLE_CREDENTIALS_JSON in main.py."}

    # If user wants Google transcription server-side:
    if not GOOGLE_CREDENTIALS_JSON:
        return {"error": "GOOGLE_CREDENTIALS_JSON not configured in main.py. Paste credentials JSON string into file to enable."}

    # Server-side Google Cloud Speech-to-Text usage (if enabled)
    try:
        from google.cloud import speech_v1p1beta1 as speech
        from google.oauth2 import service_account
    except Exception as e:
        return {"error": f"google-cloud-speech library not installed: {e}"}

    # Create credentials from JSON string (in-memory)
    creds_info = json.loads(GOOGLE_CREDENTIALS_JSON)
    credentials = service_account.Credentials.from_service_account_info(creds_info)
    client = speech.SpeechClient(credentials=credentials)

    audio = speech.RecognitionAudio(content=content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="en-US",
        enable_automatic_punctuation=True,
    )

    response = client.recognize(config=config, audio=audio)
    transcripts = [r.alternatives[0].transcript for r in response.results]
    return {"transcript": " ".join(transcripts)}

# ------------------------------
# Health
# ------------------------------
@app.get("/health")
def health():
    return {"status": "ok"}


# ðŸ” JWT Token Creation Function
def create_access_token(data: dict, expires_delta: Union[timedelta , None] = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

# ðŸ”“ Login Endpoint
@app.post("/login")
async def login(user_info: dict = Depends(config.check_ldap_auth)):
    if not user_info:
        raise HTTPException(status_code=400, detail="Invalid Credentials")

    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user_info["first_name"]},
        expires_delta=access_token_expires
    )

    return {
        "access_token": access_token,
        "token_type": "Bearer",
        "firstname": user_info["first_name"],
        "lastname": user_info["last_name"],
    }
