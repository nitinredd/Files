@app.post("/product/details")
def product_details(req: QARequest):
    """
    Robust product/details endpoint.

    Behavior summary:
      - If req.product_id is provided:
          * If question == canonical extraction -> run structured extraction (parsing and caching).
          * Else -> run retrieval QA on that specific product; return {"answer":..., "sources":[...]}.
      - If req.product_id not provided:
          * Try to detect a product name inside the question (exact normalized substring -> token-overlap -> similarity).
          * If detected -> run retrieval QA on that product and return {"answer":..., "sources":[...]}.
          * If NOT detected -> call the LLM directly (no document retrieval) and return {"response": ...}.
    """
    try:
        q_text = (req.question or "").strip()
        if not q_text:
            return JSONResponse(status_code=400, content={"error": "question is required"})

        # canonical extraction text (case-insensitive)
        CANONICAL_EXTRACTION = "extract api name, reaction chemistry, yield, procedure, and tabular data"

        # DEBUG
        print("=== /product/details called ===")
        print("Question:", q_text)
        print("product_id:", getattr(req, "product_id", None))

        # Helper: run retrieval QA on a product and return answer + sources
        def _run_retrieval_for_product(product: Dict[str, Any], question: str, k: int = 3) -> Dict[str, Any]:
            print(f"DEBUG: Running retrieval for product: {product['name']} (id: {product['id']})")
            pdf_path = product.get("pdf_path")
            if not pdf_path or not os.path.exists(pdf_path):
                raise HTTPException(status_code=404, detail=f"PDF not found for product {product.get('id')}: {pdf_path}")

            # Build/reuse vectorstore for product
            with _vectorstore_lock:
                vs = _vectorstore_cache.get(product["id"])
            if not vs:
                vs = build_product_vector_store(product)
                if not vs:
                    raise HTTPException(status_code=500, detail="Failed to build vector store for product")

            retriever = vs.as_retriever(search_kwargs={"k": k})

            # Use QA_PROMPT as configured in module (must exist)
            prompt_to_use = globals().get("QA_PROMPT") or globals().get("PROMPT") or EXTRACTION_PROMPT

            qa_chain = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt_to_use},
                return_source_documents=True,
            )

            out = qa_chain({"query": question})
            # langchain sometimes returns different keys; support common variants
            answer_text = out.get("result") or out.get("output_text") or out.get("answer") or ""
            source_docs = out.get("source_documents", []) or []

            # Build unique ordered sources list
            seen = set()
            sources = []
            for sd in source_docs:
                pid = sd.metadata.get("product_id")
                pname = sd.metadata.get("product_name")
                if pid and pid not in seen:
                    seen.add(pid)
                    sources.append({"product_id": pid, "product_name": pname})
            print(f"DEBUG: Retrieval produced answer length={len(answer_text)} and {len(sources)} source(s)")
            return {"answer": answer_text, "sources": sources}

        # Helper: detect product name inside question (reuse your detection logic)
        def _detect_product_by_name(question: str) -> Optional[Dict[str, Any]]:
            products = list_products()
            if not products:
                print("DEBUG: No products to match.")
                return None

            def _normalize_alnum(s: str) -> str:
                return re.sub(r"[^a-z0-9]", "", (s or "").lower())

            def _tokens(s: str):
                return [t for t in re.split(r'[^a-z0-9]+', (s or "").lower()) if t]

            q_norm_alnum = _normalize_alnum(question)
            q_tokens = set(_tokens(question))

            # 1) Exact normalized substring match (prefer longer product names)
            sorted_products = sorted(products, key=lambda p: len(p["name"] or ""), reverse=True)
            for p in sorted_products:
                name_norm = _normalize_alnum(p["name"])
                if name_norm and name_norm in q_norm_alnum:
                    print(f"DEBUG: Exact normalized substring match -> '{p['name']}' (id: {p['id']})")
                    return p

            # 2) Token overlap heuristic
            token_matches = []
            for p in sorted_products:
                pname_tokens = set(_tokens(p["name"]))
                if not pname_tokens:
                    continue
                overlap = pname_tokens.intersection(q_tokens)
                if overlap:
                    token_matches.append((p, len(overlap), len(pname_tokens), overlap))
            if token_matches:
                token_matches.sort(key=lambda x: (-(x[1] / x[2]), -x[1]))
                p_best, match_count, token_count, overlap = token_matches[0]
                ratio = match_count / token_count
                print(f"DEBUG: Token-overlap candidate -> '{p_best['name']}' overlap {match_count}/{token_count} ratio={ratio:.2f}")
                if ratio >= 0.5:
                    return p_best

            # 3) Similarity fallback
            best = None
            best_ratio = 0.0
            for p in products:
                pname = (p["name"] or "").lower()
                if not pname.strip():
                    continue
                r1 = SequenceMatcher(None, pname, question.lower()).ratio()
                r2 = SequenceMatcher(None, _normalize_alnum(pname), _normalize_alnum(question)).ratio()
                ratio = (r1 + r2) / 2.0
                if ratio > best_ratio:
                    best_ratio = ratio
                    best = (p, ratio, r1, r2)
            if best:
                p_best, ratio, r1, r2 = best
                print(f"DEBUG: Best similarity candidate -> '{p_best['name']}' ratio_avg={ratio:.3f}")
                if ratio >= 0.60:
                    return p_best

            return None

        # -------------------------
        # Main branching logic
        # -------------------------
        # 1) If explicit product_id provided -> handle product-specific retrieval or extraction
        if getattr(req, "product_id", None):
            products = list_products()
            product = next((p for p in products if p["id"] == req.product_id), None)
            if not product:
                return JSONResponse(status_code=404, content={"error": "Product not found"})

            is_extraction = q_text.strip().lower() == CANONICAL_EXTRACTION

            if not is_extraction:
                # retrieval QA for this product
                try:
                    return _run_retrieval_for_product(product, q_text, k=3)
                except HTTPException as he:
                    raise he
                except Exception as e:
                    print("ERROR in product retrieval:", e)
                    tb = traceback.format_exc()
                    print(tb)
                    raise HTTPException(status_code=500, detail=str(e))

            # structured extraction path (cached)
            if req.product_id in _product_details_cache:
                print("DEBUG: returning cached parsed details for", req.product_id)
                return _product_details_cache[req.product_id]

            # build vector store and run extraction prompt
            vs = build_product_vector_store(product)
            if not vs:
                return JSONResponse(status_code=500, content={"error": "Failed to build vector store (empty/invalid PDF)"})

            retriever = vs.as_retriever(search_kwargs={"k": 1})
            prompt_for_extraction = globals().get("EXTRACTION_PROMPT") or globals().get("PROMPT")
            if not prompt_for_extraction:
                return JSONResponse(status_code=500, content={"error": "No extraction prompt configured."})

            qa_chain = RetrievalQA.from_chain_type(
                llm=chat_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt_for_extraction},
                return_source_documents=False,
            )

            try:
                raw_response = qa_chain.run(q_text)
            except Exception as e:
                print("ERROR running extraction chain:", e)
                raise HTTPException(status_code=500, detail=f"LLM extraction error: {e}")

            parsed = parse_structured_response(raw_response)
            _product_details_cache[req.product_id] = parsed
            print("DEBUG: structured extraction completed and cached")
            return parsed

        # 2) No explicit product_id -> attempt to detect a product name in the question
        detected = _detect_product_by_name(q_text)
        if detected:
            print(f"DEBUG: Detected product by name: {detected['name']} (id: {detected['id']}) -> running retrieval")
            try:
                return _run_retrieval_for_product(detected, q_text, k=3)
            except HTTPException as he:
                raise he
            except Exception as e:
                print("ERROR in detected-product retrieval:", e)
                tb = traceback.format_exc()
                print(tb)
                raise HTTPException(status_code=500, detail=str(e))

        # 3) No product detected -> direct LLM call (fallback to generative)
        print("DEBUG: No product detected -> calling LLM directly (generative path)")

        # Use the QA_PROMPT to get a concise answer (this will call LLM directly)
        prompt_for_gen = globals().get("QA_PROMPT") or globals().get("PROMPT") or EXTRACTION_PROMPT

        # Build a simple chain that uses the prompt template and the question
        qa_chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=None,  # no retriever -> direct LLM
            chain_type_kwargs={"prompt": prompt_for_gen},
            return_source_documents=False,
        )

        try:
            raw_response = qa_chain.run(q_text)
        except Exception as e:
            print("ERROR running direct LLM chain:", e)
            tb = traceback.format_exc()
            print(tb)
            raise HTTPException(status_code=500, detail=f"LLM error: {e}")

        return {"response": raw_response}

    except HTTPException as he:
        # re-raise HTTP exceptions so FastAPI handles them
        raise he
    except Exception as e:
        tb = traceback.format_exc()
        print("=== /product/details ERROR ===")
        print(tb)
        trace_lines = tb.splitlines()[-30:]
        return JSONResponse(status_code=500, content={
            "error": "Internal server error in /product/details",
            "message": str(e),
            "trace": trace_lines
        })
