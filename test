# backend/main.py

import os
import io
import json
import uuid
import pandas as pd
import logging
import speech_recognition as sr
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel

from gtts import gTTS
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA

# ─── CONFIG ──────────────────────────────────────────────────────

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure GPT configuration (fill these in)
BASE_URL        = ""  # e.g. "https://YOUR_RESOURCE.openai.azure.com/"
API_VERSION     = "2024-02-15-preview"
API_KEY         = ""
DEPLOYMENT_NAME = "GPT4o"
MODEL_NAME      = "GPT4o"

# JSON data files
EMBEDDED_FILES = {
    "data_1": r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Test_1.json",
    "data_2": None,
    "data_3": r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Formulas.json",
}

# Sample‑prompt tiles (for frontend to fetch)
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X":["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# ─── SETUP LLM + EMBEDDINGS ──────────────────────────────────────

# Embedding store & cache
file_store = LocalFileStore("langchain-embeddings")
base_embed = AzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint=BASE_URL,
    api_key=API_KEY,
    azure_deployment=DEPLOYMENT_NAME
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    base_embed, file_store, namespace=base_embed.model
)

# Chat LLM
chat_model = AzureChatOpenAI(
    azure_deployment=DEPLOYMENT_NAME,
    model=MODEL_NAME,
    api_version=API_VERSION,
    api_key=API_KEY,
    azure_endpoint=BASE_URL
)

# ─── AGENT CLASSES ───────────────────────────────────────────────

class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=False
        )
    def ask(self, query: str) -> str:
        resp = self.chain.invoke({"query": query})
        return resp.get("result", "")

class CoordinatorAgent:
    def __init__(self, children: list[ChildAgent]):
        self.children = children
    def coordinate(self, query: str) -> str | None:
        for child in self.children:
            try:
                ans = child.ask(query)
                if ans and "not found" not in ans.lower():
                    logger.info(f"[Coordinator] '{child.name}' answered")
                    return ans
            except Exception as e:
                logger.warning(f"[Coordinator] child {child.name} error: {e}")
        return None

class OversightAgent:
    def validate(self, answer: str) -> str:
        return answer

class LearningAgent:
    def __init__(self):
        self.logs: list[dict] = []
    def log(self, q: str, a: str):
        self.logs.append({"query": q, "response": a})

class AgentManager:
    def __init__(self, agents: list[ChildAgent]):
        self.coordinator = CoordinatorAgent(agents)
        self.oversight  = OversightAgent()
        self.learning   = LearningAgent()
    def handle_query(self, query: str) -> str:
        raw = self.coordinator.coordinate(query)
        answer = raw if raw else "Oops! No relevant information found."
        validated = self.oversight.validate(answer)
        self.learning.log(query, validated)
        return validated

# ─── DATA LOADING & VECTORSTORE BUILD ────────────────────────────

def load_json_data(paths: dict[str, str]) -> dict[str, pd.DataFrame]:
    dfs: dict[str, pd.DataFrame] = {}
    for name, path in paths.items():
        if not path:
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            records = []
            if isinstance(data, list):
                for item in data:
                    records.append({"content": json.dumps(item, sort_keys=True)})
            else:
                records.append({"content": json.dumps(data, sort_keys=True)})
            df = pd.DataFrame(records)
            dfs[name] = df
            logger.info(f"[Data] Loaded {len(df)} records for '{name}'")
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    return dfs

def build_vectorstores(dfs: dict[str, pd.DataFrame]) -> list[ChildAgent]:
    agents: list[ChildAgent] = []
    for key, df in dfs.items():
        docs = [
            Document(page_content=row["content"], metadata={"source": key})
            for _, row in df.iterrows()
        ]
        if not docs:
            continue
        store = FAISS.from_documents(docs, cached_embeddings)
        retriever = store.as_retriever(search_kwargs={"k": 5})
        agents.append(ChildAgent(name=key, retriever=retriever))
        logger.info(f"[Vectorstore] Built store for '{key}' ({len(docs)} docs)")
    return agents

# Initialize on startup
DATAFRAMES = load_json_data(EMBEDDED_FILES)
AGENTS     = build_vectorstores(DATAFRAMES)
MANAGER    = AgentManager(AGENTS)
recognizer = sr.Recognizer()

# ─── FASTAPI APP ─────────────────────────────────────────────────

app = FastAPI(title="Multi-Agent JSON Chatbot")

# CORS for your React frontend on :5173
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request models
class ChatRequest(BaseModel):
    message: str

# ─── ENDPOINTS ────────────────────────────────────────────────────

@app.get("/sample-tiles")
def sample_tiles():
    """Return the sample-prompt tiles and questions."""
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
def chat(req: ChatRequest):
    q = req.message.strip()
    if not q:
        raise HTTPException(400, "Empty query")
    logger.info(f"[API] /chat query='{q}'")
    ans = MANAGER.handle_query(q)
    return {"response": ans}

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    """
    Accepts an uploaded audio file (wav/mp3) and returns the transcribed text.
    """
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"[API] STT error: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    """
    Returns an MP3 audio stream of the given text.
    """
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"[API] TTS error: {e}")
        raise HTTPException(500, str(e))

@app.get("/health")
def health():
    return {"status": "ok", "agents": [a.name for a in AGENTS]}
