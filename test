# multi_agent_json_chatbot.py
# Complete updated script with ContextualCompressionRetriever + proper LLMChainExtractor usage
# Drop-in replacement for your previous file. Fill Azure credentials and file paths as needed.

import os
import io
import json
import uuid
import logging
from typing import Union, List

import pandas as pd
import speech_recognition as sr
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel

from gtts import gTTS

# LangChain / Azure LLM & embeddings imports
from langchain_openai import AzureChatOpenAI
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA

# Compressor/extractor imports
from langchain.chains.llm import LLMChain
from langchain.prompts import PromptTemplate
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain.schema import Document as LC_Document

# ─── CONFIG ──────────────────────────────────────────────────────

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure GPT configuration (fill these in)
base_url = ""            # your azure endpoint
api_version = "2024-02-15-preview"

api_key = ""             # your azure api key
deployment_name = "GPT4o"
model_name = "GPT4o"

# Paths to embedded JSON files
EMBEDDED_FILES = {
    'data_1': r"C:\Users\Desktop\WORK\Formulations\Scale_up_Predictor_chatbot\Dataset\Prime_M+F_JSON.json",
    # add other files if needed
}

# Sample-prompt tiles (for frontend to fetch)
TILE_QUESTIONS = {
    "Product A": ["What is the API used?", "What is the batch size?", "Who is the manufacturer?"],
    "Line B":    ["What is the speed range?", "What equipment is used?", "What is the pressure limit?"],
    "Facility X":["Who owns this facility?", "What lines are operational?"],
    "Formulation Z": ["List excipients used", "Describe dissolution method"],
    "Process Y": ["Steps in granulation?", "Drying temperature?"],
    "Machine Q": ["Model number details?", "Maintenance interval?"],
    "Raw Material P": ["What are specs?", "Approved vendors?"]
}

# Retriever configuration
RETRIEVER_K = 2  # set initially to 2, lower to 1 if necessary

# Token estimation and safe limits
TOKEN_CHAR_RATIO = 4          # heuristic: ~4 characters per token
MODEL_CONTEXT_LIMIT = 128000  # tokens (from your model error)
SAFETY_MARGIN_TOKENS = 2000   # headroom

# ─── EMBEDDINGS & LLM SETUP ──────────────────────────────────────

# Embedding store & cache
file_store = LocalFileStore('langchain-embeddings')
base = AzureOpenAIEmbeddings(
    model="text-embedding-ada-002",
    api_version="2023-07-01-preview",
    azure_endpoint="",   # fill if needed
    api_key="",          # fill if needed
    azure_deployment="Def_data_qa"
)

chat_model = AzureChatOpenAI(
    azure_deployment=deployment_name,
    model=model_name,
    api_version=api_version,
    api_key=api_key,
    azure_endpoint=base_url
)

cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# ─── EXTRACTION CHAIN (used by ContextualCompressionRetriever) ───

# Prompt that the extractor chain expects: requires `text` and `query` keys
extract_prompt = PromptTemplate(
    input_variables=["text", "query"],
    template=(
        "You are an information extractor.\n\n"
        "User question:\n{query}\n\n"
        "From the following DOCUMENT TEXT, extract only the minimal snippets that are directly "
        "relevant to answering the question. Return only the snippets, separated by a blank line. "
        "Do not add commentary or explanations.\n\nDOCUMENT:\n{text}\n\n"
        "If nothing relevant is found, return an empty string."
    )
)

# Create an LLMChain for extraction (you can replace chat_model with a cheaper LLM if desired)
EXTRACTOR_CHAIN = LLMChain(llm=chat_model, prompt=extract_prompt)

# Wrap as LLMChainExtractor (this is the object ContextualCompressionRetriever expects)
EXTRACTOR = LLMChainExtractor(llm_chain=EXTRACTOR_CHAIN)

# ─── AGENT CLASSES ───────────────────────────────────────────────

class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        self.retriever = retriever  # keep retriever handy for fallback / logging
        # Use map_reduce to reduce prompt-stuffing risk
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="map_reduce",
            retriever=retriever,
            return_source_documents=False
        )

    def _estimate_tokens(self, text: str) -> int:
        return max(1, int(len(text) / TOKEN_CHAR_RATIO))

    def ask(self, query: str) -> str:
        # Normal fast path
        try:
            return self.chain.run(query)
        except Exception as e:
            logger.warning(f"[{self.name}] Chain run error: {e} — attempting fallback summarization")
            # Fallback: get raw retrieved docs, run extractor chain to get concise snippets,
            # then ask chat_model directly with the extracted context.
            try:
                # Use the retriever we saved on this agent
                docs = []
                try:
                    docs = self.retriever.get_relevant_documents(query)
                except Exception as dd:
                    logger.warning(f"[{self.name}] retriever.get_relevant_documents failed: {dd}")
                    # also try chain's retriever if available
                    try:
                        docs = self.chain.retriever.get_relevant_documents(query)
                    except Exception as dd2:
                        logger.error(f"[{self.name}] both retriever attempts failed: {dd2}")
                        docs = []

                if not docs:
                    logger.info(f"[{self.name}] No docs retrieved for fallback query.")
                    return ""

                joined = "\n\n".join(d.page_content for d in docs)

                # Estimate tokens, if too big we'll still let extractor try to compress
                est_tokens = self._estimate_tokens(joined)
                logger.info(f"[{self.name}] Fallback joined length {len(joined)} chars ~ {est_tokens} tokens")

                # Call extractor chain properly with named inputs
                try:
                    extracted_snippets = EXTRACTOR_CHAIN.predict(text=joined, query=query)
                except TypeError:
                    # some langchain versions may expect .run with a dict
                    extracted_snippets = EXTRACTOR_CHAIN.run({"text": joined, "query": query})

                if not extracted_snippets or not extracted_snippets.strip():
                    logger.info(f"[{self.name}] Extractor returned empty for query: {query}")
                    return ""

                # Final prompt to the model using only extracted snippets as context
                prompt_for_chain = f"Context:\n{extracted_snippets}\n\nQuestion: {query}"

                # Use chat_model to get the final answer. The method below assumes chat_model.generate exists.
                # If your LangChain wrapper differs, replace with the appropriate call (e.g., chat_model.predict).
                resp = chat_model.generate([{"role": "user", "content": prompt_for_chain}])
                try:
                    return resp.generations[0][0].text
                except Exception:
                    # If the structure is different, fall back to stringifying
                    return str(resp)
            except Exception as ex:
                logger.error(f"[{self.name}] Fallback failed: {ex}")
                return ""

class CoordinatorAgent:
    def __init__(self, children: List[ChildAgent]):
        self.children = children

    def coordinate(self, query: str) -> Union[str, None]:
        for child in self.children:
            try:
                # Optional debugging: log retrieved document sizes
                try:
                    docs = child.retriever.get_relevant_documents(query)
                    sizes = [(len(d.page_content), d.metadata.get("source")) for d in docs]
                    logger.info(f"[Retriever sizes for {child.name}] {sizes}")
                except Exception:
                    pass

                ans = child.ask(query)
                if ans and "not found" not in ans.lower() and "error" not in ans.lower():
                    logger.info(f"Coordinator selected {child.name}")
                    return ans
            except Exception as e:
                logger.warning(f"Child agent {child.name} error: {str(e)}")
        return None

class OversightAgent:
    def validate(self, answer: str) -> str:
        # Placeholder - you can add safety checks, formatting, etc.
        return answer

class LearningAgent:
    def __init__(self):
        self.logs: List[dict] = []
    def log(self, q: str, a: str):
        self.logs.append({"query": q, "response": a})

class AgentManager:
    def __init__(self, agents: List[ChildAgent]):
        self.coordinator = CoordinatorAgent(agents)
        self.oversight  = OversightAgent()
        self.learning   = LearningAgent()
    def handle_query(self, query: str) -> str:
        raw = self.coordinator.coordinate(query)
        answer = raw if raw else "Oops! No relevant information found."
        validated = self.oversight.validate(answer)
        self.learning.log(query, validated)
        return validated

# ─── DATA LOADING & VECTORSTORE BUILD ───────────────────────────

def load_json_data(paths: dict[str, str]) -> dict[str, pd.DataFrame]:
    dfs: dict[str, pd.DataFrame] = {}
    for name, path in paths.items():
        if not path:
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            records = []
            if isinstance(data, list):
                for item in data:
                    records.append({"content": json.dumps(item, sort_keys=True)})
            else:
                records.append({"content": json.dumps(data, sort_keys=True)})
            df = pd.DataFrame(records)
            dfs[name] = df
            logger.info(f"[Data] Loaded {len(df)} records for '{name}'")
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    return dfs

def build_vectorstores(dfs: dict[str, pd.DataFrame]) -> List[ChildAgent]:
    agents: List[ChildAgent] = []

    # reuse the EXTRACTOR (LLMChainExtractor) created globally
    for key, df in dfs.items():
        docs = [
            Document(page_content=row["content"], metadata={"source": key})
            for _, row in df.iterrows()
        ]
        if not docs:
            continue

        store = FAISS.from_documents(docs, cached_embeddings)
        base_retriever = store.as_retriever(search_kwargs={"k": RETRIEVER_K})

        compressed_retriever = ContextualCompressionRetriever(
            base_retriever=base_retriever,
            base_compressor=EXTRACTOR
        )

        agents.append(ChildAgent(name=key, retriever=compressed_retriever))
        logger.info(f"[Vectorstore] Built store for '{key}' ({len(docs)} docs) with k={RETRIEVER_K}")
    return agents

# Initialize on startup
DATAFRAMES = load_json_data(EMBEDDED_FILES)
AGENTS     = build_vectorstores(DATAFRAMES)
MANAGER    = AgentManager(AGENTS)
recognizer = sr.Recognizer()

# ─── FASTAPI APP ─────────────────────────────────────────────────

app = FastAPI(title="Multi-Agent JSON Chatbot")

# CORS for your React frontend on :5173
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request models
class ChatRequest(BaseModel):
    message: str

# ─── ENDPOINTS ────────────────────────────────────────────────────

@app.get("/sample-tiles")
def sample_tiles():
    """Return the sample-prompt tiles and questions."""
    return JSONResponse(TILE_QUESTIONS)

@app.post("/chat")
def chat(req: ChatRequest):
    q = req.message.strip()
    if not q:
        raise HTTPException(400, "Empty query")
    logger.info(f"[API] /chat query='{q}'")
    ans = MANAGER.handle_query(q)
    return {"response": ans}

@app.post("/rebuild-agents")
def rebuild_agents():
    global DATAFRAMES, AGENTS, MANAGER
    DATAFRAMES = load_json_data(EMBEDDED_FILES)
    AGENTS = build_vectorstores(DATAFRAMES)
    MANAGER = AgentManager(AGENTS)
    return {"status": "rebuilt", "agents": [a.name for a in AGENTS], "k": RETRIEVER_K}

@app.post("/speech-to-text")
async def speech_to_text(file: UploadFile = File(...)):
    """
    Accepts an uploaded audio file (wav/mp3) and returns the transcribed text.
    """
    try:
        data = await file.read()
        audio = sr.AudioFile(io.BytesIO(data))
        with audio as src:
            audio_data = recognizer.record(src)
        text = recognizer.recognize_google(audio_data)
        return {"text": text}
    except Exception as e:
        logger.error(f"[API] STT error: {e}")
        raise HTTPException(500, str(e))

@app.get("/text-to-speech")
def text_to_speech(text: str):
    """
    Returns an MP3 audio stream of the given text.
    """
    try:
        buf = io.BytesIO()
        gTTS(text).write_to_fp(buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="audio/mp3")
    except Exception as e:
        logger.error(f"[API] TTS error: {e}")
        raise HTTPException(500, str(e))

@app.get("/health")
def health():
    return {"status": "ok", "agents": [a.name for a in AGENTS]}

# ─── QUICK TEST SNIPPET (OPTIONAL) ───────────────────────────────
# You can enable the snippet below temporarily to test EXTRACTOR_CHAIN behavior on startup.
if __name__ == "__main__":
    # Quick extractor test
    try:
        sample_doc = "API: AzureOpenAI Embeddings. Deployment: Def_data_qa. This doc contains details about usage and deployments."
        test_out = EXTRACTOR_CHAIN.predict(text=sample_doc, query="What deployment is used?")
        logger.info(f"[EXTRACTOR test] -> {test_out}")
    except Exception as e:
        logger.error(f"[EXTRACTOR test] failed: {e}")
