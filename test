# add this import near the top with your other imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
# -------------------------
# Improved data loader that keeps JSON but stores as text fields
# -------------------------
def load_json_data(paths: dict[str, str]) -> dict[str, pd.DataFrame]:
    dfs: dict[str, pd.DataFrame] = {}
    for name, path in paths.items():
        if not path:
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            records = []
            # convert each logical item into a JSON string (one "record" per item)
            if isinstance(data, list):
                for idx, item in enumerate(data):
                    content = json.dumps(item, sort_keys=True, ensure_ascii=False)
                    records.append({"id": f"{name}__{idx}", "content": content})
            else:
                content = json.dumps(data, sort_keys=True, ensure_ascii=False)
                records.append({"id": f"{name}__0", "content": content})
            df = pd.DataFrame(records)
            dfs[name] = df
            logger.info(f"[Data] Loaded {len(df)} records for '{name}'")
        except Exception as e:
            logger.error(f"[Data] Failed to load '{name}': {e}")
    return dfs
$$$$$$$$$$$
# -------------------------
# Build vectorstores with chunking & metadata
# -------------------------
def build_vectorstores(dfs: dict[str, pd.DataFrame]) -> list[ChildAgent]:
    agents: list[ChildAgent] = []

    # configure splitter: tune chunk_size & overlap as needed
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,    # characters; adjust down if needed (e.g. 800)
        chunk_overlap=200
    )

    for key, df in dfs.items():
        docs = []
        total_chunks = 0
        for _, row in df.iterrows():
            content = row["content"]
            doc_id = row.get("id", key)
            # split content into smaller passages
            chunks = splitter.split_text(content)
            for i, chunk in enumerate(chunks):
                metadata = {"source": key, "orig_id": doc_id, "chunk": i, "chunk_len": len(chunk)}
                docs.append(Document(page_content=chunk, metadata=metadata))
            total_chunks += len(chunks)

        if not docs:
            continue

        # Create FAISS store from documents
        store = FAISS.from_documents(docs, cached_embeddings)
        # reduce k so we only return a small number of short chunks to the combiner
        retriever = store.as_retriever(search_kwargs={"k": 3})
        agents.append(ChildAgent(name=key, retriever=retriever))
        logger.info(f"[Vectorstore] Built store for '{key}' ({len(docs)} chunks total)")
    return agents
$$$$$$$$$$$$$$$$$$$$$
# -------------------------
# ChildAgent: use map_reduce combiner but DO NOT pass unsupported chain_type_kwargs
# -------------------------
class ChildAgent:
    def __init__(self, name: str, retriever):
        self.name = name
        # Use map_reduce but avoid passing extra/unsupported kwargs that LangChain/Pydantic may reject.
        # return_source_documents=True helps debugging / provenance (you can show which chunks were used).
        self.chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="map_reduce",   # safe to use; don't pass unknown kwargs
            retriever=retriever,
            return_source_documents=True,
        )

    def ask(self, query: str) -> str:
        try:
            # .run returns a single-string answer for most RetrievalQA chains
            return self.chain.run(query)
        except Exception as e:
            logger.warning(f"Child agent {self.name} error in ask(): {e}")
            raise
